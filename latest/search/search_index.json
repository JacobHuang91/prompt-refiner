{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prompt Refiner","text":"<p>A lightweight Python library for optimizing and cleaning LLM inputs. Reduce token usage, improve prompt quality, and lower API costs.</p>"},{"location":"#overview","title":"Overview","text":"<p>Prompt Refiner helps you clean and optimize prompts before sending them to LLM APIs. By removing unnecessary whitespace, duplicate characters, and other inefficiencies, you can:</p> <ul> <li>Reduce token usage and API costs - Remove unnecessary characters and content</li> <li>Improve prompt quality - Clean HTML, fix Unicode issues, normalize whitespace</li> <li>Enhance security - Redact PII automatically before sending to APIs</li> <li>Track optimization value - Measure token savings and cost reductions</li> </ul> <p>Proven Effectiveness</p> <p>Benchmarked on 30 real-world test cases, Prompt Refiner achieves 4-15% token reduction while maintaining 96-99% quality. Aggressive optimization can save up to ~$54/month on GPT-4 at scale (1M tokens/month).</p> <p>Processing overhead is &lt; 0.5ms per 1k tokens - negligible compared to network and LLM latency.</p> <p>See benchmark results \u2192</p>"},{"location":"#status","title":"Status","text":"<p>Early Development</p> <p>This project is in early development. Features are being added iteratively.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Build custom cleaning pipelines with the pipe operator:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\n# Define a cleaning pipeline\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000, strategy=\"middle_out\")\n)\n\nraw_input = \"&lt;div&gt;  User input with &lt;b&gt;lots&lt;/b&gt; of   spaces... &lt;/div&gt;\"\nclean_prompt = pipeline.run(raw_input)\n# Output: \"User input with lots of spaces...\"\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"#5-core-modules","title":"5 Core Modules","text":"<p>Prompt Refiner is organized into 5 specialized modules:</p>"},{"location":"#basic-operations","title":"Basic Operations","text":"<p>The first 4 modules provide core text processing operations:</p>"},{"location":"#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<ul> <li>StripHTML() - Remove HTML tags, convert to Markdown</li> <li>NormalizeWhitespace() - Collapse excessive whitespace</li> <li>FixUnicode() - Remove zero-width spaces and problematic Unicode</li> </ul> <p>Learn more about Cleaner \u2192</p>"},{"location":"#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<ul> <li>TruncateTokens() - Smart truncation with sentence boundaries<ul> <li>Strategies: <code>\"head\"</code>, <code>\"tail\"</code>, <code>\"middle_out\"</code></li> </ul> </li> <li>Deduplicate() - Remove similar content (great for RAG)</li> </ul> <p>Learn more about Compressor \u2192</p>"},{"location":"#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<ul> <li>RedactPII() - Automatically redact emails, phones, IPs, credit cards, URLs, SSNs</li> </ul> <p>Learn more about Scrubber \u2192</p>"},{"location":"#4-analyzer-show-value","title":"4. Analyzer - Show Value","text":"<ul> <li>CountTokens() - Track token savings and optimization impact</li> </ul> <p>Learn more about Analyzer \u2192</p>"},{"location":"#advanced-context-budget-management","title":"Advanced: Context Budget Management","text":""},{"location":"#5-packer-intelligent-context-packing-v013","title":"5. Packer - Intelligent Context Packing (v0.1.3+)","text":"<p>For RAG applications and chatbots, the Packer module manages context budgets with priority-based selection:</p> <ul> <li>MessagesPacker() - For chat completion APIs (OpenAI, Anthropic)</li> <li>TextPacker() - For text completion APIs (Llama Base, GPT-3)</li> </ul> <p>Key Features: - Priority-based greedy packing algorithm - JIT refinement with <code>refine_with</code> parameter - Automatic format overhead calculation - Perfect for RAG with conversation history</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_HIGH, StripHTML\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"You are helpful.\", role=\"system\", priority=PRIORITY_SYSTEM)\n\n# Clean RAG documents on-the-fly\npacker.add(\n    \"&lt;div&gt;RAG doc...&lt;/div&gt;\",\n    role=\"system\",\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n\nmessages = packer.pack()  # Returns List[Dict] ready for chat APIs\n</code></pre> <p>Learn more about Packer \u2192</p>"},{"location":"#complete-example","title":"Complete Example","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\noriginal_text = \"\"\"Your messy input here...\"\"\"\n\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    # Clean\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n    | FixUnicode()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\"})\n    # Analyze\n    | counter\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())  # Shows token savings\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Get Started</p> <p>Install Prompt Refiner and build your first pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting Started</p> </li> <li> <p>API Reference</p> <p>Complete API documentation for all operations and modules</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>Examples</p> <p>Browse practical examples for each module</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>Contributing</p> <p>Learn how to contribute to the project</p> <p>:octicons-arrow-right-24: Contributing Guide</p> </li> </ul>"},{"location":"benchmark/","title":"Benchmark Results","text":"<p>Prompt Refiner's effectiveness has been validated through comprehensive testing covering both quality &amp; cost savings and performance &amp; latency.</p>"},{"location":"benchmark/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"benchmark/#quality-cost-benchmark","title":"\ud83c\udfaf Quality &amp; Cost Benchmark","text":"<p>Comprehensive A/B testing on 30 real-world test cases measuring token reduction and response quality.</p> <p>Jump to Quality Benchmark \u2192</p>"},{"location":"benchmark/#latency-benchmark","title":"\u26a1 Latency Benchmark","text":"<p>Performance testing measuring processing overhead of refining operations.</p> <p>Jump to Latency Benchmark \u2192</p>"},{"location":"benchmark/#quality-cost-results","title":"Quality &amp; Cost Results","text":"<p>The benchmark measures two critical factors:</p> <ul> <li>Token Reduction - How much we can reduce prompt size (cost savings)</li> <li>Response Quality - Whether responses remain semantically equivalent</li> </ul> <p>Quality is evaluated using two methods: 1. Cosine Similarity - Semantic similarity of response embeddings (0-1 scale) 2. LLM Judge - GPT-4 evaluation of response equivalence</p>"},{"location":"benchmark/#results-summary","title":"Results Summary","text":"<p>We tested 3 optimization strategies on 30 test cases (15 SQuAD Q&amp;A pairs + 15 RAG scenarios):</p> Strategy Token Reduction Quality (Cosine) Judge Approval Overall Equivalent Minimal 4.3% 0.987 86.7% 86.7% Standard 4.8% 0.984 90.0% 86.7% Aggressive 15.0% 0.964 80.0% 66.7%"},{"location":"benchmark/#strategy-definitions","title":"Strategy Definitions","text":"<p>Minimal (Conservative cleaning): <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p> <p>Standard (Recommended for most use cases): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p> <p>Aggressive (Maximum savings): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150, strategy=\"head\")\n)\n</code></pre></p>"},{"location":"benchmark/#key-findings","title":"Key Findings","text":""},{"location":"benchmark/#standard-strategy-best-balance","title":"\ud83c\udfaf Standard Strategy: Best Balance","text":"<p>The Standard strategy offers the best balance: - 4.8% token reduction with minimal quality impact - 90% judge approval - highest among all strategies - 0.984 cosine similarity - nearly perfect semantic preservation</p>"},{"location":"benchmark/#cost-savings","title":"\ud83d\udcb0 Cost Savings","text":"<p>Real-world cost savings for production applications:</p> GPT-4 TurboGPT-4 <p>Input cost: $0.01 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $4.30 $4.80 $15.00 1M tokens/month $43 $48 $150 10M tokens/month $430 $480 $1,500 <p>Input cost: $0.03 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $13 $14 $45 1M tokens/month $129 $144 $450 10M tokens/month $1,290 $1,440 $4,500"},{"location":"benchmark/#performance-by-scenario","title":"\ud83d\udcca Performance by Scenario","text":"<p>RAG Scenarios (with duplicates and HTML): - Minimal: 17% reduction on average - Standard: 31% reduction on average - Aggressive: 49% reduction on complex documents</p> <p>SQuAD Q&amp;A (clean academic text): - All strategies: 2-5% reduction (less messy data = less to clean)</p> <p>Key Insight</p> <p>Token savings scale with input messiness. RAG contexts with HTML, duplicates, and whitespace see 3-10x more reduction than clean text.</p>"},{"location":"benchmark/#visualizations","title":"Visualizations","text":""},{"location":"benchmark/#token-reduction-vs-quality","title":"Token Reduction vs Quality","text":"<p>The scatter plot shows each strategy's position in the cost-quality tradeoff space. Standard strategy achieves near-optimal quality while maintaining solid savings.</p>"},{"location":"benchmark/#test-dataset","title":"Test Dataset","text":"<p>The benchmark uses 30 carefully curated test cases:</p>"},{"location":"benchmark/#squad-samples-15-cases","title":"SQuAD Samples (15 cases)","text":"<p>Question-answer pairs with context covering: - History (\"When did Beyonce start becoming popular?\") - Science (\"What is DNA?\") - Geography, literature, technology</p>"},{"location":"benchmark/#rag-scenarios-15-cases","title":"RAG Scenarios (15 cases)","text":"<p>Realistic retrieval-augmented generation use cases: - E-commerce product catalogs with HTML - Documentation with excessive whitespace - Customer support tickets with duplicates - Code search results - Recipe collections</p>"},{"location":"benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>Want to validate these results yourself?</p>"},{"location":"benchmark/#prerequisites","title":"Prerequisites","text":"<pre><code># Install benchmark dependencies\nuv pip install -e \".[benchmark]\"\n\n# Set up OpenAI API key\ncd benchmark/custom\ncp .env.example .env\n# Edit .env and add your OPENAI_API_KEY\n</code></pre>"},{"location":"benchmark/#run-the-benchmark","title":"Run the Benchmark","text":"<pre><code>cd benchmark/custom\npython benchmark.py\n</code></pre> <p>This will: 1. Test 30 cases with 3 strategies (90 total comparisons) 2. Generate detailed report with visualizations 3. Save results to <code>./results/</code> directory</p> <p>Estimated cost: ~$2-5 per full run (using gpt-4o-mini)</p>"},{"location":"benchmark/#advanced-options","title":"Advanced Options","text":"<pre><code># Use a different model\npython benchmark.py --model gpt-4o\n\n# Test specific strategies only\npython benchmark.py --strategies minimal standard\n\n# Use fewer test cases (faster, cheaper)\npython benchmark.py --limit 10\n</code></pre>"},{"location":"benchmark/#recommendations","title":"Recommendations","text":"<p>Based on benchmark results:</p>"},{"location":"benchmark/#for-production-rag-applications","title":"For Production RAG Applications","text":"<p>Use Standard strategy - Best balance of savings and quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p>"},{"location":"benchmark/#for-high-volume-cost-sensitive-applications","title":"For High-Volume, Cost-Sensitive Applications","text":"<p>Consider Aggressive strategy if 15% cost reduction outweighs slightly lower quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150)\n)\n</code></pre></p>"},{"location":"benchmark/#for-quality-critical-applications","title":"For Quality-Critical Applications","text":"<p>Use Minimal strategy for maximum quality preservation <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p>"},{"location":"benchmark/#latency-performance","title":"Latency &amp; Performance","text":"<p>\"What's the latency overhead?\" - Negligible. Prompt Refiner adds &lt; 0.5ms per 1k tokens of overhead.</p>"},{"location":"benchmark/#performance-results","title":"Performance Results","text":"Strategy @ 1k tokens @ 10k tokens @ 50k tokens Overhead per 1k tokens Minimal (HTML + Whitespace) 0.05ms 0.48ms 2.39ms 0.05ms Standard (+ Deduplicate) 0.26ms 2.47ms 12.27ms 0.25ms Aggressive (+ Truncate) 0.26ms 2.46ms 12.38ms 0.25ms"},{"location":"benchmark/#key-performance-insights","title":"Key Performance Insights","text":"<ul> <li>\u26a1 Minimal strategy: Only 0.05ms per 1k tokens (faster than a network packet)</li> <li>\ud83c\udfaf Standard strategy: 0.25ms per 1k tokens - adds ~2.5ms to a 10k token prompt</li> <li>\ud83d\udcca Context: Network + LLM TTFT is typically 600ms+, refining adds &lt; 0.5% overhead</li> <li>\ud83d\ude80 Individual operations (HTML, whitespace) are &lt; 0.5ms per 1k tokens</li> </ul>"},{"location":"benchmark/#real-world-impact","title":"Real-World Impact","text":"<pre><code>10k token RAG context refining: ~2.5ms overhead\nNetwork latency: ~100ms\nLLM Processing (TTFT): ~500ms+\nTotal overhead: &lt; 0.5% of request time\n</code></pre> <p>Performance Takeaway</p> <p>Refining overhead is negligible compared to network + LLM latency (600ms+). Standard refining adds ~2.5ms overhead - less than 0.5% of total request time.</p>"},{"location":"benchmark/#running-the-latency-benchmark","title":"Running the Latency Benchmark","text":"<p>The latency benchmark requires no API keys and runs completely offline:</p> <pre><code>cd benchmark/latency\npython benchmark.py\n</code></pre> <p>This will: 1. Test individual operations at multiple scales (1k, 10k, 50k tokens) 2. Test complete strategies (Minimal, Standard, Aggressive) 3. Report average, median, and P95 latency metrics 4. Show per-1k-token normalized overhead</p> <p>Cost: $0 (runs locally, no API calls)</p> <p>Duration: ~30-60 seconds</p>"},{"location":"benchmark/#learn-more","title":"Learn More","text":"<ul> <li>View Quality Benchmark Documentation</li> <li>View Latency Benchmark Documentation</li> <li>Browse Test Cases</li> <li>Examine Raw Results</li> </ul>"},{"location":"benchmark/#contributing","title":"Contributing","text":"<p>Have ideas to improve the benchmark? We welcome: - New test cases (especially domain-specific scenarios) - Additional evaluation metrics - Alternative refining strategies - Multi-model comparisons</p> <p>Open an issue or submit a PR!</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Prompt Refiner!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>uv package manager</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run tests\nmake test\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>prompt-refiner/\n\u251c\u2500\u2500 src/prompt_refiner/     # Source code\n\u2502   \u251c\u2500\u2500 cleaner/           # Cleaner module\n\u2502   \u251c\u2500\u2500 compressor/        # Compressor module\n\u2502   \u251c\u2500\u2500 scrubber/          # Scrubber module\n\u2502   \u2514\u2500\u2500 analyzer/          # Analyzer module\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 examples/              # Example scripts\n\u2514\u2500\u2500 docs/                  # Documentation\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following existing patterns</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_cleaner.py -v\n</code></pre>"},{"location":"contributing/#4-format-code","title":"4. Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"contributing/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write clear docstrings (Google style)</li> <li>Keep functions small and focused</li> </ul>"},{"location":"contributing/#example","title":"Example","text":"<pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text\n    \"\"\"\n    return text.strip()\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Aim for high test coverage</li> <li>Test edge cases</li> </ul> <pre><code>def test_strip_html():\n    operation = StripHTML()\n    result = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n    assert result == \"Hello\"\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update relevant documentation files</li> <li>Add examples for new features</li> <li>Keep API reference up to date (auto-generated from docstrings)</li> </ul>"},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code>make docs-serve\n</code></pre> <p>Then visit http://127.0.0.1:8000</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with Prompt Refiner in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"Default (Lightweight)With Precise Token Counting <p>Zero dependencies - perfect for most use cases:</p> <pre><code>pip install llm-prompt-refiner\n</code></pre> <p>Install with optional <code>tiktoken</code> for precise token counting:</p> <pre><code>pip install llm-prompt-refiner[token]\n</code></pre> <p>Then opt-in by passing a <code>model</code> parameter:</p> <pre><code>from prompt_refiner import CountTokens, MessagesPacker\n\ncounter = CountTokens(model=\"gpt-4\")\npacker = MessagesPacker(max_tokens=1000, model=\"gpt-4\")\n</code></pre>"},{"location":"getting-started/#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's create a simple pipeline to clean HTML and normalize whitespace:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Process some text\nraw_input = \"\"\"\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Welcome&lt;/h1&gt;\n        &lt;p&gt;This  has    excessive   spaces.&lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nclean_output = pipeline.run(raw_input)\nprint(clean_output)\n# Output: \"Welcome This has excessive spaces.\"\n</code></pre>"},{"location":"getting-started/#understanding-the-pipeline-pattern","title":"Understanding the Pipeline Pattern","text":"<p>Prompt Refiner uses a pipeline pattern where you chain operations together:</p> <ol> <li>Create operations - Initialize the operations you need</li> <li>Chain with <code>|</code> operator - Combine operations in order</li> <li>Run with <code>.run()</code> - Execute the pipeline on your text</li> </ol> <pre><code>pipeline = (\n    Operation1()            # 1. Create operations\n    | Operation2()          # 2. Chain with | operator\n    | Operation3()\n)\n\nresult = pipeline.run(text)  # 3. Run\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use the traditional fluent API with <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(Operation3())\n)\n</code></pre></p> <p>Order Matters</p> <p>Operations run in the order you add them. For example, you should typically clean HTML before normalizing whitespace.</p>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#pattern-1-web-content-cleaning","title":"Pattern 1: Web Content Cleaning","text":"<p>Clean content scraped from the web:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)  # Convert to Markdown\n    | FixUnicode()               # Fix Unicode issues\n    | NormalizeWhitespace()      # Normalize spaces\n)\n</code></pre>"},{"location":"getting-started/#pattern-2-rag-context-optimization","title":"Pattern 2: RAG Context Optimization","text":"<p>Optimize retrieved context for RAG applications:</p> <pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate(similarity_threshold=0.85)  # Remove duplicates\n    | TruncateTokens(max_tokens=2000)       # Fit in context window\n)\n</code></pre>"},{"location":"getting-started/#pattern-3-secure-pii-handling","title":"Pattern 3: Secure PII Handling","text":"<p>Redact sensitive information before sending to APIs:</p> <pre><code>from prompt_refiner import RedactPII\n\nsecure_pipeline = RedactPII(redact_types={\"email\", \"phone\", \"ssn\"})\n</code></pre>"},{"location":"getting-started/#pattern-4-full-optimization-with-tracking","title":"Pattern 4: Full Optimization with Tracking","text":"<p>Complete optimization with metrics:</p> <pre><code>from prompt_refiner import (\n    StripHTML, NormalizeWhitespace,\n    TruncateTokens, RedactPII, CountTokens\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\nfull_pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n    | RedactPII()\n    | counter\n)\n\nresult = full_pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"getting-started/#pattern-5-advanced-rag-with-context-budget-v013","title":"Pattern 5: Advanced - RAG with Context Budget (v0.1.3+)","text":"<p>For RAG applications, manage context budgets with priority-based packing:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM, PRIORITY_USER, PRIORITY_HIGH,\n    StripHTML, NormalizeWhitespace\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on provided context.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with JIT cleaning\npacker.add(\n    \"&lt;div&gt;Document 1...&lt;/div&gt;\",\n    role=\"system\",\n    priority=PRIORITY_HIGH,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n\n# Current user query (must include)\npacker.add(\n    \"What is the answer?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()  # Ready for chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"getting-started/#proven-results","title":"Proven Results","text":"<p>Curious about the real-world effectiveness? Check out our comprehensive benchmark results:</p> <p>Benchmark Highlights</p> <ul> <li>4-15% token reduction across 30 test cases</li> <li>96-99% quality preservation (cosine similarity + LLM judge)</li> <li>Real cost savings: $48-$150/month per 1M tokens</li> </ul> <p>View Full Benchmark \u2192</p>"},{"location":"getting-started/#exploring-modules","title":"Exploring Modules","text":"<p>Prompt Refiner has 5 specialized modules:</p>"},{"location":"getting-started/#basic-operations","title":"Basic Operations","text":"<ul> <li>Cleaner - Clean dirty data (HTML, whitespace, Unicode)</li> <li>Compressor - Reduce size (truncation, deduplication)</li> <li>Scrubber - Security and privacy (PII redaction)</li> <li>Analyzer - Metrics and analysis (token counting)</li> </ul>"},{"location":"getting-started/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Packer - Context budget management for RAG and chatbots (v0.1.3+)</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn the Modules</p> <p>Deep dive into each of the 4 core modules</p> <p>:octicons-arrow-right-24: Modules Overview</p> </li> <li> <p>Browse Examples</p> <p>See practical examples for each operation</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for all Prompt Refiner classes and operations.</p> <p>This section contains auto-generated documentation from the codebase docstrings. All operations inherit from the base <code>Operation</code> class and implement a <code>process(text: str) -&gt; str</code> method.</p>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":"<ul> <li> <p>:material-pipe:{ .lg .middle } Refiner</p> <p>Pipeline builder for chaining operations</p> <p>:octicons-arrow-right-24: Refiner API</p> </li> <li> <p>:material-broom:{ .lg .middle } Cleaner</p> <p>Operations for cleaning dirty data</p> <p>:octicons-arrow-right-24: Cleaner API</p> </li> <li> <p>:material-compress:{ .lg .middle } Compressor</p> <p>Operations for reducing size</p> <p>:octicons-arrow-right-24: Compressor API</p> </li> <li> <p>:material-shield-lock:{ .lg .middle } Scrubber</p> <p>Operations for security and privacy</p> <p>:octicons-arrow-right-24: Scrubber API</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Analyzer</p> <p>Operations for metrics and analysis</p> <p>:octicons-arrow-right-24: Analyzer API</p> </li> <li> <p>:material-package-variant:{ .lg .middle } Packer</p> <p>Context budget management with priorities</p> <p>:octicons-arrow-right-24: Packer API</p> </li> </ul>"},{"location":"api-reference/#operation-base-class","title":"Operation Base Class","text":"<p>All operations in Prompt Refiner inherit from the <code>Operation</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Operation(ABC):\n    @abstractmethod\n    def process(self, text: str) -&gt; str:\n        \"\"\"Process the input text and return the result.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#usage-pattern","title":"Usage Pattern","text":"<p>Operations are used within a <code>Refiner</code> pipeline:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = refiner.run(\"Your text here...\")\n</code></pre>"},{"location":"api-reference/#module-organization","title":"Module Organization","text":"<ul> <li>Refiner - Core pipeline builder class</li> <li>Cleaner - <code>StripHTML</code>, <code>NormalizeWhitespace</code>, <code>FixUnicode</code></li> <li>Compressor - <code>TruncateTokens</code>, <code>Deduplicate</code></li> <li>Scrubber - <code>RedactPII</code></li> <li>Analyzer - <code>CountTokens</code></li> <li>Packer - <code>ContextPacker</code></li> </ul>"},{"location":"api-reference/analyzer/","title":"Analyzer Module","text":"<p>The Analyzer module provides operations for measuring optimization impact and tracking metrics.</p>"},{"location":"api-reference/analyzer/#counttokens","title":"CountTokens","text":"<p>Count tokens and provide before/after statistics to demonstrate optimization value.</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens","title":"prompt_refiner.analyzer.CountTokens","text":"<pre><code>CountTokens(original_text=None, model=None)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Count tokens and provide statistics before/after processing.</p> <p>Supports two modes: - Precise mode: Uses tiktoken if installed (pip install llm-prompt-refiner[token]) - Estimation mode: Uses character-based approximation (1 token \u2248 4 characters)</p> <p>Initialize the token counter.</p> <p>Parameters:</p> Name Type Description Default <code>original_text</code> <code>Optional[str]</code> <p>Optional original text to compare against</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name for tiktoken encoding. If None, uses character-based    estimation. If specified, attempts to use tiktoken for precise counting.</p> <code>None</code> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def __init__(self, original_text: Optional[str] = None, model: Optional[str] = None):\n    \"\"\"\n    Initialize the token counter.\n\n    Args:\n        original_text: Optional original text to compare against\n        model: Model name for tiktoken encoding. If None, uses character-based\n               estimation. If specified, attempts to use tiktoken for precise counting.\n    \"\"\"\n    self.original_text = original_text\n    self.model = model\n    self._stats: Optional[dict] = None\n    self.is_precise = False\n    self._encoding = None\n\n    # Only try tiktoken if user explicitly requests it by passing a model\n    if model is not None:\n        try:\n            import tiktoken\n\n            try:\n                self._encoding = tiktoken.encoding_for_model(model)\n            except KeyError:\n                # Fall back to cl100k_base if model not found\n                self._encoding = tiktoken.get_encoding(\"cl100k_base\")\n            self.is_precise = True\n            logger.debug(f\"Using tiktoken for precise token counting (model: {model})\")\n        except ImportError:\n            # User requested precise mode but tiktoken not installed\n            self.is_precise = False\n            logger.warning(\n                f\"Model '{model}' specified but tiktoken not installed. \"\n                \"Falling back to character-based estimation. \"\n                \"Install with: pip install llm-prompt-refiner[token]\"\n            )\n    else:\n        # User didn't specify model - use estimation mode directly\n        self.is_precise = False\n        logger.debug(\"Using character-based token estimation (model not specified)\")\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens-functions","title":"Functions","text":""},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Count tokens in the text and store statistics.</p> <p>This operation doesn't modify the text, it just analyzes it.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>The same text (unchanged)</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Count tokens in the text and store statistics.\n\n    This operation doesn't modify the text, it just analyzes it.\n\n    Args:\n        text: The input text\n\n    Returns:\n        The same text (unchanged)\n    \"\"\"\n    current_tokens = self._estimate_tokens(text)\n\n    if self.original_text is not None:\n        original_tokens = self._estimate_tokens(self.original_text)\n        saved_tokens = original_tokens - current_tokens\n        saving_percent = (saved_tokens / original_tokens * 100) if original_tokens &gt; 0 else 0\n\n        self._stats = {\n            \"original\": original_tokens,\n            \"cleaned\": current_tokens,\n            \"saved\": saved_tokens,\n            \"saving_percent\": f\"{saving_percent:.1f}%\",\n        }\n    else:\n        self._stats = {\n            \"tokens\": current_tokens,\n        }\n\n    return text\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.get_stats","title":"get_stats","text":"<pre><code>get_stats()\n</code></pre> <p>Get token statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing token statistics</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get token statistics.\n\n    Returns:\n        Dictionary containing token statistics\n    \"\"\"\n    return self._stats or {}\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.format_stats","title":"format_stats","text":"<pre><code>format_stats()\n</code></pre> <p>Format statistics as a human-readable string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted statistics string</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def format_stats(self) -&gt; str:\n    \"\"\"\n    Format statistics as a human-readable string.\n\n    Returns:\n        Formatted statistics string\n    \"\"\"\n    if not self._stats:\n        return \"No statistics available\"\n\n    if \"original\" in self._stats:\n        return (\n            f\"Original: {self._stats['original']} tokens\\n\"\n            f\"Cleaned: {self._stats['cleaned']} tokens\\n\"\n            f\"Saved: {self._stats['saved']} tokens ({self._stats['saving_percent']})\"\n        )\n    else:\n        return f\"Tokens: {self._stats['tokens']}\"\n</code></pre>"},{"location":"api-reference/analyzer/#token-counting-modes","title":"Token Counting Modes","text":"<p>Two Counting Modes</p> <p>CountTokens supports two modes:</p> <p>Estimation Mode (Default) - Zero dependencies, uses character-based approximation: ~1 token \u2248 4 characters - Fast and lightweight, good for most use cases - Applies 10% safety buffer in ContextPacker to prevent overflow</p> <pre><code>counter = CountTokens()  # Estimation mode\n</code></pre> <p>Precise Mode (Optional) - Requires <code>tiktoken</code>: <code>pip install llm-prompt-refiner[token]</code> - Exact token counting using OpenAI's tokenizer - No safety buffer needed, 100% capacity utilization - Opt-in by passing a <code>model</code> parameter</p> <pre><code>counter = CountTokens(model=\"gpt-4\")  # Precise mode\n</code></pre>"},{"location":"api-reference/analyzer/#examples","title":"Examples","text":""},{"location":"api-reference/analyzer/#basic-token-counting","title":"Basic Token Counting","text":"<pre><code>from prompt_refiner import CountTokens\n\ncounter = CountTokens()\ncounter.process(\"Hello World\")\n\nstats = counter.get_stats()\nprint(stats)\n# {'tokens': 2}\n</code></pre>"},{"location":"api-reference/analyzer/#beforeafter-comparison","title":"Before/After Comparison","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\n# Build pipeline with counter at the end\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\n\n# Get statistics\nstats = counter.get_stats()\nprint(stats)\n# {\n#   'original': 6,\n#   'cleaned': 2,\n#   'saved': 4,\n#   'saving_percent': '66.7%'\n# }\n\n# Formatted output\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"api-reference/analyzer/#cost-calculation-example","title":"Cost Calculation Example","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"\"\"Your long text here...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nstats = counter.get_stats()\n\n# Calculate cost savings\n# Example: GPT-4 pricing - $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings = original_cost - cleaned_cost\n\nprint(f\"Original cost: ${original_cost:.4f}\")\nprint(f\"Cleaned cost: ${cleaned_cost:.4f}\")\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre>"},{"location":"api-reference/analyzer/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/analyzer/#roi-demonstration","title":"ROI Demonstration","text":"<pre><code>from prompt_refiner import (\n    Refiner, StripHTML, NormalizeWhitespace,\n    Deduplicate, TruncateTokens, CountTokens\n)\n\noriginal_text = \"\"\"Your messy input...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nfull_optimization = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=1000))\n    .pipe(counter)\n)\n\nresult = full_optimization.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#ab-testing-different-strategies","title":"A/B Testing Different Strategies","text":"<pre><code>from prompt_refiner import Refiner, TruncateTokens, Deduplicate, CountTokens\n\noriginal_text = \"\"\"Your text...\"\"\"\n\n# Strategy A: Just truncate\ncounter_a = CountTokens(original_text=original_text)\nstrategy_a = (\n    Refiner()\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_a)\n)\nstrategy_a.run(original_text)\n\n# Strategy B: Deduplicate then truncate\ncounter_b = CountTokens(original_text=original_text)\nstrategy_b = (\n    Refiner()\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_b)\n)\nstrategy_b.run(original_text)\n\nprint(\"Strategy A:\", counter_a.format_stats())\nprint(\"Strategy B:\", counter_b.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nfrom prompt_refiner import Refiner, StripHTML, CountTokens\n\nlogger = logging.getLogger(__name__)\n\ndef process_user_input(text):\n    counter = CountTokens(original_text=text)\n\n    refiner = (\n        Refiner()\n        .pipe(StripHTML())\n        .pipe(counter)\n    )\n\n    result = refiner.run(text)\n    stats = counter.get_stats()\n\n    # Log optimization impact\n    logger.info(\n        f\"Processed input: \"\n        f\"original={stats['original']} tokens, \"\n        f\"cleaned={stats['cleaned']} tokens, \"\n        f\"saved={stats['saved']} tokens ({stats['saving_percent']})\"\n    )\n\n    return result\n</code></pre>"},{"location":"api-reference/analyzer/#tips","title":"Tips","text":"<p>Always Use with Original Text</p> <p>To see before/after comparisons, always initialize <code>CountTokens</code> with the original text:</p> <pre><code>counter = CountTokens(original_text=original_text)\n</code></pre> <p>Otherwise, you'll only get the final token count.</p> <p>Place at End of Pipeline</p> <p>For accurate \"after\" measurements, place <code>CountTokens</code> as the last operation in your pipeline:</p> <pre><code>refiner = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(CountTokens(original_text=text))  # Last!\n)\n</code></pre>"},{"location":"api-reference/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data, including HTML removal, whitespace normalization, and Unicode fixing.</p>"},{"location":"api-reference/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML","title":"prompt_refiner.cleaner.StripHTML","text":"<pre><code>StripHTML(preserve_tags=None, to_markdown=False)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p> <p>Initialize the HTML stripper.</p> <p>Parameters:</p> Name Type Description Default <code>preserve_tags</code> <code>Optional[Set[str]]</code> <p>Set of tag names to preserve (e.g., {'p', 'li', 'table'})</p> <code>None</code> <code>to_markdown</code> <code>bool</code> <p>Convert common HTML tags to Markdown syntax</p> <code>False</code> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def __init__(\n    self,\n    preserve_tags: Optional[Set[str]] = None,\n    to_markdown: bool = False,\n):\n    \"\"\"\n    Initialize the HTML stripper.\n\n    Args:\n        preserve_tags: Set of tag names to preserve (e.g., {'p', 'li', 'table'})\n        to_markdown: Convert common HTML tags to Markdown syntax\n    \"\"\"\n    self.preserve_tags = preserve_tags or set()\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove HTML tags from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing HTML</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with HTML tags removed or converted to Markdown</p> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove HTML tags from the input text.\n\n    Args:\n        text: The input text containing HTML\n\n    Returns:\n        Text with HTML tags removed or converted to Markdown\n    \"\"\"\n    result = text\n\n    if self.to_markdown:\n        # Convert common HTML tags to Markdown\n        # Bold\n        result = re.sub(r\"&lt;strong&gt;(.*?)&lt;/strong&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;b&gt;(.*?)&lt;/b&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        # Italic\n        result = re.sub(r\"&lt;em&gt;(.*?)&lt;/em&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;i&gt;(.*?)&lt;/i&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        # Links\n        result = re.sub(\n            r'&lt;a[^&gt;]*href=[\"\\']([^\"\\']*)[\"\\'][^&gt;]*&gt;(.*?)&lt;/a&gt;',\n            r\"[\\2](\\1)\",\n            result,\n            flags=re.DOTALL,\n        )\n        # Headers\n        for i in range(1, 7):\n            result = re.sub(\n                f\"&lt;h{i}[^&gt;]*&gt;(.*?)&lt;/h{i}&gt;\",\n                f\"{'#' * i} \\\\1\\n\",\n                result,\n                flags=re.DOTALL,\n            )\n        # Code\n        result = re.sub(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", r\"`\\1`\", result, flags=re.DOTALL)\n        # Lists - simple conversion\n        result = re.sub(r\"&lt;li[^&gt;]*&gt;(.*?)&lt;/li&gt;\", r\"- \\1\\n\", result, flags=re.DOTALL)\n        # Paragraphs\n        result = re.sub(r\"&lt;p[^&gt;]*&gt;(.*?)&lt;/p&gt;\", r\"\\1\\n\\n\", result, flags=re.DOTALL)\n        # Line breaks\n        result = re.sub(r\"&lt;br\\s*/?&gt;\", \"\\n\", result)\n\n    if self.preserve_tags:\n        # Remove all tags except preserved ones\n        # This is a simplified implementation\n        tags_pattern = r\"&lt;/?(?!\" + \"|\".join(self.preserve_tags) + r\"\\b)[^&gt;]+&gt;\"\n        result = re.sub(tags_pattern, \"\", result)\n    else:\n        # Remove all HTML tags\n        result = re.sub(r\"&lt;[^&gt;]+&gt;\", \"\", result)\n\n    # Clean up excessive newlines\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n\n    return result.strip()\n</code></pre>"},{"location":"api-reference/cleaner/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import StripHTML\n\n# Basic HTML stripping\nstripper = StripHTML()\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\nstripper = StripHTML(to_markdown=True)\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n\n# Preserve specific tags\nstripper = StripHTML(preserve_tags={\"p\", \"div\"})\nresult = stripper.process(\"&lt;div&gt;Keep &lt;b&gt;Remove&lt;/b&gt;&lt;/div&gt;\")\n# Output: \"&lt;div&gt;Keep Remove&lt;/div&gt;\"\n</code></pre>"},{"location":"api-reference/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines into single spaces.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace","title":"prompt_refiner.cleaner.NormalizeWhitespace","text":"<p>               Bases: <code>Operation</code></p> <p>Normalize whitespace in text.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Normalize whitespace by collapsing multiple spaces into one.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with normalized whitespace</p> Source code in <code>src/prompt_refiner/cleaner/whitespace.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Normalize whitespace by collapsing multiple spaces into one.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with normalized whitespace\n    \"\"\"\n    # Replace multiple whitespace with single space and strip edges\n    return \" \".join(text.split())\n</code></pre>"},{"location":"api-reference/cleaner/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import NormalizeWhitespace\n\nnormalizer = NormalizeWhitespace()\nresult = normalizer.process(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre>"},{"location":"api-reference/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters including zero-width spaces and control characters.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode","title":"prompt_refiner.cleaner.FixUnicode","text":"<pre><code>FixUnicode(\n    remove_zero_width=True, remove_control_chars=True\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove or fix problematic Unicode characters.</p> <p>Initialize the Unicode fixer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_zero_width</code> <code>bool</code> <p>Remove zero-width spaces and similar characters</p> <code>True</code> <code>remove_control_chars</code> <code>bool</code> <p>Remove control characters (except newlines and tabs)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def __init__(self, remove_zero_width: bool = True, remove_control_chars: bool = True):\n    \"\"\"\n    Initialize the Unicode fixer.\n\n    Args:\n        remove_zero_width: Remove zero-width spaces and similar characters\n        remove_control_chars: Remove control characters (except newlines and tabs)\n    \"\"\"\n    self.remove_zero_width = remove_zero_width\n    self.remove_control_chars = remove_control_chars\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Clean problematic Unicode characters from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with problematic Unicode characters removed</p> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Clean problematic Unicode characters from text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with problematic Unicode characters removed\n    \"\"\"\n    result = text\n\n    if self.remove_zero_width:\n        # Remove zero-width characters\n        zero_width_chars = [\n            \"\\u200b\",  # Zero-width space\n            \"\\u200c\",  # Zero-width non-joiner\n            \"\\u200d\",  # Zero-width joiner\n            \"\\ufeff\",  # Zero-width no-break space (BOM)\n            \"\\u2060\",  # Word joiner\n        ]\n        for char in zero_width_chars:\n            result = result.replace(char, \"\")\n\n    if self.remove_control_chars:\n        # Remove control characters except newlines, tabs, and carriage returns\n        # Keep: \\n (0x0A), \\t (0x09), \\r (0x0D)\n        result = \"\".join(\n            char\n            for char in result\n            if not unicodedata.category(char).startswith(\"C\") or char in (\"\\n\", \"\\t\", \"\\r\")\n        )\n\n    # Normalize Unicode to NFC form (canonical composition)\n    result = unicodedata.normalize(\"NFC\", result)\n\n    return result\n</code></pre>"},{"location":"api-reference/cleaner/#examples_2","title":"Examples","text":"<pre><code>from prompt_refiner import FixUnicode\n\n# Remove zero-width spaces and control chars\nfixer = FixUnicode()\nresult = fixer.process(\"Hello\\u200bWorld\\u0000\")\n# Output: \"HelloWorld\"\n\n# Only remove zero-width spaces\nfixer = FixUnicode(remove_control_chars=False)\nresult = fixer.process(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre>"},{"location":"api-reference/cleaner/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/cleaner/#web-scraping","title":"Web Scraping","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    Refiner()\n    .pipe(StripHTML(to_markdown=True))\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import Refiner, NormalizeWhitespace, FixUnicode\n\ntext_normalizer = (\n    Refiner()\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/compressor/","title":"Compressor Module","text":"<p>The Compressor module provides operations for reducing text size through smart truncation and deduplication.</p>"},{"location":"api-reference/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens","title":"prompt_refiner.compressor.TruncateTokens","text":"<pre><code>TruncateTokens(\n    max_tokens,\n    strategy=\"head\",\n    respect_sentence_boundary=True,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p> <p>Initialize the truncation operation.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to keep</p> required <code>strategy</code> <code>Literal['head', 'tail', 'middle_out']</code> <p>Truncation strategy: - \"head\": Keep the beginning of the text - \"tail\": Keep the end of the text (useful for conversation history) - \"middle_out\": Keep beginning and end, remove middle</p> <code>'head'</code> <code>respect_sentence_boundary</code> <code>bool</code> <p>If True, truncate at sentence boundaries</p> <code>True</code> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int,\n    strategy: Literal[\"head\", \"tail\", \"middle_out\"] = \"head\",\n    respect_sentence_boundary: bool = True,\n):\n    \"\"\"\n    Initialize the truncation operation.\n\n    Args:\n        max_tokens: Maximum number of tokens to keep\n        strategy: Truncation strategy:\n            - \"head\": Keep the beginning of the text\n            - \"tail\": Keep the end of the text (useful for conversation history)\n            - \"middle_out\": Keep beginning and end, remove middle\n        respect_sentence_boundary: If True, truncate at sentence boundaries\n    \"\"\"\n    self.max_tokens = max_tokens\n    self.strategy = strategy\n    self.respect_sentence_boundary = respect_sentence_boundary\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Truncate text to max_tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text respecting sentence boundaries if configured</p> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate text to max_tokens.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Truncated text respecting sentence boundaries if configured\n    \"\"\"\n    estimated_tokens = self._estimate_tokens(text)\n\n    if estimated_tokens &lt;= self.max_tokens:\n        return text\n\n    if self.respect_sentence_boundary:\n        sentences = self._split_sentences(text)\n\n        if self.strategy == \"head\":\n            return self._truncate_head_sentences(sentences)\n        elif self.strategy == \"tail\":\n            return self._truncate_tail_sentences(sentences)\n        elif self.strategy == \"middle_out\":\n            return self._truncate_middle_out_sentences(sentences)\n    else:\n        # Fallback to word-based truncation\n        words = text.split()\n\n        if self.strategy == \"head\":\n            return \" \".join(words[: self.max_tokens])\n        elif self.strategy == \"tail\":\n            return \" \".join(words[-self.max_tokens :])\n        elif self.strategy == \"middle_out\":\n            half = self.max_tokens // 2\n            start_words = words[:half]\n            end_words = words[-(self.max_tokens - half) :]\n            return \" \".join(start_words) + \" ... \" + \" \".join(end_words)\n\n    return text\n</code></pre>"},{"location":"api-reference/compressor/#truncation-strategies","title":"Truncation Strategies","text":"<ul> <li><code>head</code>: Keep the beginning of the text (default)</li> <li><code>tail</code>: Keep the end of the text (useful for conversation history)</li> <li><code>middle_out</code>: Keep beginning and end, remove middle</li> </ul>"},{"location":"api-reference/compressor/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\nresult = truncator.process(long_text)\n\n# Keep last 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\nresult = truncator.process(long_text)\n\n# Keep first and last 50 tokens, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\nresult = truncator.process(long_text)\n\n# Truncate at word boundaries (faster, less precise)\ntruncator = TruncateTokens(\n    max_tokens=100,\n    strategy=\"head\",\n    respect_sentence_boundary=False\n)\nresult = truncator.process(long_text)\n</code></pre>"},{"location":"api-reference/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or highly similar text chunks, useful for RAG contexts.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate","title":"prompt_refiner.compressor.Deduplicate","text":"<pre><code>Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\",\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove duplicate or highly similar text chunks (useful for RAG contexts).</p> Performance Characteristics <p>This operation uses an O(n\u00b2) comparison algorithm, where each chunk is compared against all previously seen chunks. The total complexity is O(n\u00b2 \u00d7 comparison_cost), where comparison_cost depends on the selected similarity method: - Jaccard: O(m) where m is the chunk length (word-based) - Levenshtein: O(m\u2081 \u00d7 m\u2082) where m\u2081, m\u2082 are the chunk lengths (character-based)</p> <p>For typical RAG contexts (10-50 chunks), performance is acceptable with either method. For larger inputs (200+ chunks), consider using paragraph granularity to reduce the number of comparisons, or use Jaccard method for better performance.</p> <p>Initialize the deduplication operation.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for considering text similar (0.0-1.0)</p> <code>0.85</code> <code>method</code> <code>Literal['levenshtein', 'jaccard']</code> <p>Similarity calculation method - \"jaccard\": Jaccard similarity (word-based, faster)     * Complexity: O(m) per comparison where m is chunk length     * Recommended for most use cases (10-200 chunks)     * Fast even with long chunks - \"levenshtein\": Levenshtein distance (character-based)     * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison     * More precise but computationally expensive     * Can be slow with long chunks (1000+ characters)</p> <code>'jaccard'</code> <code>granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Text granularity to deduplicate at - \"sentence\": Deduplicate at sentence level     * More comparisons (more chunks) but smaller chunk sizes     * Better for fine-grained deduplication - \"paragraph\": Deduplicate at paragraph level     * Fewer comparisons but larger chunk sizes     * Recommended for large documents to reduce n\u00b2 scaling</p> <code>'paragraph'</code> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    similarity_threshold: float = 0.85,\n    method: Literal[\"levenshtein\", \"jaccard\"] = \"jaccard\",\n    granularity: Literal[\"sentence\", \"paragraph\"] = \"paragraph\",\n):\n    \"\"\"\n    Initialize the deduplication operation.\n\n    Args:\n        similarity_threshold: Threshold for considering text similar (0.0-1.0)\n        method: Similarity calculation method\n            - \"jaccard\": Jaccard similarity (word-based, faster)\n                * Complexity: O(m) per comparison where m is chunk length\n                * Recommended for most use cases (10-200 chunks)\n                * Fast even with long chunks\n            - \"levenshtein\": Levenshtein distance (character-based)\n                * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison\n                * More precise but computationally expensive\n                * Can be slow with long chunks (1000+ characters)\n        granularity: Text granularity to deduplicate at\n            - \"sentence\": Deduplicate at sentence level\n                * More comparisons (more chunks) but smaller chunk sizes\n                * Better for fine-grained deduplication\n            - \"paragraph\": Deduplicate at paragraph level\n                * Fewer comparisons but larger chunk sizes\n                * Recommended for large documents to reduce n\u00b2 scaling\n    \"\"\"\n    self.similarity_threshold = similarity_threshold\n    self.method = method\n    self.granularity = granularity\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove duplicate or similar text chunks.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with duplicates removed</p> Performance Note <p>This method uses O(n\u00b2) comparisons where n is the number of chunks. For large inputs (200+ chunks), consider using paragraph granularity to reduce the number of chunks, or ensure you're using the jaccard method for better performance.</p> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove duplicate or similar text chunks.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with duplicates removed\n\n    Performance Note:\n        This method uses O(n\u00b2) comparisons where n is the number of chunks.\n        For large inputs (200+ chunks), consider using paragraph granularity\n        to reduce the number of chunks, or ensure you're using the jaccard\n        method for better performance.\n    \"\"\"\n    chunks = self._split_text(text)\n\n    if not chunks:\n        return text\n\n    # Keep track of unique chunks\n    unique_chunks = []\n    seen_chunks = []\n\n    for chunk in chunks:\n        is_duplicate = False\n\n        # Check similarity with all previously seen chunks\n        for seen_chunk in seen_chunks:\n            similarity = self._calculate_similarity(chunk, seen_chunk)\n            if similarity &gt;= self.similarity_threshold:\n                is_duplicate = True\n                break\n\n        if not is_duplicate:\n            unique_chunks.append(chunk)\n            seen_chunks.append(chunk)\n\n    # Reconstruct text\n    if self.granularity == \"paragraph\":\n        return \"\\n\\n\".join(unique_chunks)\n    else:  # sentence\n        return \" \".join(unique_chunks)\n</code></pre>"},{"location":"api-reference/compressor/#similarity-methods","title":"Similarity Methods","text":"<ul> <li><code>jaccard</code>: Jaccard similarity (word-based, faster) - default</li> <li><code>levenshtein</code>: Levenshtein distance (character-based, more accurate)</li> </ul>"},{"location":"api-reference/compressor/#granularity-levels","title":"Granularity Levels","text":"<ul> <li><code>paragraph</code>: Deduplicate at paragraph level (split by <code>\\n\\n</code>) - default</li> <li><code>sentence</code>: Deduplicate at sentence level (split by <code>.</code>, <code>!</code>, <code>?</code>)</li> </ul>"},{"location":"api-reference/compressor/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# Basic deduplication (85% similarity threshold)\ndeduper = Deduplicate(similarity_threshold=0.85)\nresult = deduper.process(text_with_duplicates)\n\n# More aggressive (70% similarity)\ndeduper = Deduplicate(similarity_threshold=0.70)\nresult = deduper.process(text_with_duplicates)\n\n# Character-level similarity\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"levenshtein\"\n)\nresult = deduper.process(text_with_duplicates)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    granularity=\"sentence\"\n)\nresult = deduper.process(text_with_duplicates)\n</code></pre>"},{"location":"api-reference/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.85))  # Remove duplicates first\n    .pipe(TruncateTokens(max_tokens=2000))        # Then fit in context window\n)\n</code></pre>"},{"location":"api-reference/compressor/#conversation-history-compression","title":"Conversation History Compression","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nconversation_compressor = (\n    Refiner()\n    .pipe(Deduplicate(granularity=\"sentence\"))\n    .pipe(TruncateTokens(max_tokens=1000, strategy=\"tail\"))  # Keep recent messages\n)\n</code></pre>"},{"location":"api-reference/compressor/#document-summarization-prep","title":"Document Summarization Prep","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nsummarization_prep = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.90))  # Remove near-duplicates\n    .pipe(TruncateTokens(max_tokens=4000, strategy=\"middle_out\"))  # Keep intro + conclusion\n)\n</code></pre>"},{"location":"api-reference/packer/","title":"Packer Module API Reference","text":"<p>The Packer module provides specialized packers for managing context budgets with priority-based item selection. Version 0.1.3+ introduces two specialized packers following the Single Responsibility Principle.</p>"},{"location":"api-reference/packer/#messagespacker","title":"MessagesPacker","text":"<p>Optimized for chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict[str, str]]</code> directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker","title":"prompt_refiner.packer.MessagesPacker","text":"<pre><code>MessagesPacker(max_tokens, model=None)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for chat completion APIs.</p> <p>Designed for: - OpenAI Chat Completions (gpt-4, gpt-3.5-turbo, etc.) - Anthropic Messages API (claude-3-opus, claude-3-sonnet, etc.) - Any API using ChatML-style message format</p> <p>Returns: List[Dict[str, str]] with 'role' and 'content' keys</p> Example <p>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER packer = MessagesPacker(max_tokens=1000) packer.add(\"You are helpful.\", role=\"system\", priority=PRIORITY_SYSTEM) packer.add(\"Hello!\", role=\"user\", priority=PRIORITY_USER) messages = packer.pack()</p> <p>Initialize messages packer.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum token budget</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting</p> <code>None</code> Source code in <code>src/prompt_refiner/packer/messages_packer.py</code> <pre><code>def __init__(self, max_tokens: int, model: Optional[str] = None):\n    \"\"\"\n    Initialize messages packer.\n\n    Args:\n        max_tokens: Maximum token budget\n        model: Optional model name for precise token counting\n    \"\"\"\n    super().__init__(max_tokens, model)\n\n    # Pre-deduct request-level overhead (priming tokens)\n    # This ensures we never exceed the budget in edge cases\n    self.effective_max_tokens -= PER_REQUEST_OVERHEAD\n\n    logger.debug(\n        f\"MessagesPacker initialized with {max_tokens} tokens \"\n        f\"(effective: {self.effective_max_tokens} after {PER_REQUEST_OVERHEAD} \"\n        f\"token request overhead)\"\n    )\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--messages-role-system-content-role-user-content","title":"messages = [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}]","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--use-directly-openaichatcompletionscreatemessagesmessages","title":"Use directly: openai.chat.completions.create(messages=messages)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into message format for chat APIs.</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of message dictionaries with 'role' and 'content' keys.</p> <code>List[Dict[str, str]]</code> <p>Items without role default to 'user'.</p> Example <p>messages = packer.pack() openai.chat.completions.create(model=\"gpt-4\", messages=messages)</p> Source code in <code>src/prompt_refiner/packer/messages_packer.py</code> <pre><code>def pack(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Pack items into message format for chat APIs.\n\n    Returns:\n        List of message dictionaries with 'role' and 'content' keys.\n        Items without role default to 'user'.\n\n    Example:\n        &gt;&gt;&gt; messages = packer.pack()\n        &gt;&gt;&gt; openai.chat.completions.create(model=\"gpt-4\", messages=messages)\n    \"\"\"\n    selected_items = self._greedy_select()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty message list\")\n        return []\n\n    messages = []\n    for item in selected_items:\n        # Default to 'user' role if not specified\n        role = item.role or \"user\"\n        messages.append({\"role\": role, \"content\": item.content})\n\n    logger.info(f\"Packed {len(messages)} messages for chat API\")\n    return messages\n</code></pre>"},{"location":"api-reference/packer/#textpacker","title":"TextPacker","text":"<p>Optimized for text completion APIs (Llama Base, GPT-3). Returns <code>str</code> directly with multiple text formats.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker","title":"prompt_refiner.packer.TextPacker","text":"<pre><code>TextPacker(\n    max_tokens,\n    model=None,\n    text_format=TextFormat.RAW,\n    separator=None,\n)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for text completion APIs.</p> <p>Designed for: - Base models (Llama-2-base, GPT-3, etc.) - Completion endpoints (not chat) - Custom prompt templates</p> <p>Returns: str (formatted text ready for completion API)</p> <p>Supports multiple text formatting strategies to prevent instruction drifting: - RAW: Simple concatenation with separators - MARKDOWN: Grouped sections (INSTRUCTIONS, CONTEXT, CONVERSATION, INPUT) - XML: Semantic content tags</p> Example <p>from prompt_refiner import TextPacker, TextFormat, PRIORITY_SYSTEM, PRIORITY_HIGH packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN) packer.add(\"You are helpful.\", role=\"system\", priority=PRIORITY_SYSTEM) packer.add(\"Context document\", priority=PRIORITY_HIGH) prompt = packer.pack()</p> <p>Initialize text packer.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum token budget</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting</p> <code>None</code> <code>text_format</code> <code>TextFormat</code> <p>Text formatting strategy (RAW, MARKDOWN, XML)</p> <code>RAW</code> <code>separator</code> <code>Optional[str]</code> <p>String to join items (default: \"\\n\\n\" for clarity)</p> <code>None</code> Source code in <code>src/prompt_refiner/packer/text_packer.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int,\n    model: Optional[str] = None,\n    text_format: TextFormat = TextFormat.RAW,\n    separator: Optional[str] = None,\n):\n    \"\"\"\n    Initialize text packer.\n\n    Args:\n        max_tokens: Maximum token budget\n        model: Optional model name for precise token counting\n        text_format: Text formatting strategy (RAW, MARKDOWN, XML)\n        separator: String to join items (default: \"\\\\n\\\\n\" for clarity)\n    \"\"\"\n    super().__init__(max_tokens, model)\n    self.text_format = text_format\n    self.separator = separator if separator is not None else \"\\n\\n\"\n\n    # For MARKDOWN grouped format: Pre-deduct fixed header costs (\"entrance fee\")\n    # This prevents overestimating overhead for each item\n    if self.text_format == TextFormat.MARKDOWN:\n        self._reserve_fixed_headers()\n\n    logger.debug(\n        f\"TextPacker initialized with format={text_format.value}, \"\n        f\"separator={repr(self.separator)}\"\n    )\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--prompt-instructionsnyou-are-helpfulnn-contextncontext-document","title":"prompt = \"### INSTRUCTIONS:\\nYou are helpful.\\n\\n### CONTEXT:\\nContext document\"","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--use-directly-completioncreatepromptprompt","title":"Use directly: completion.create(prompt=prompt)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into formatted text for completion APIs.</p> <p>MARKDOWN format uses grouped sections to reduce token overhead: - INSTRUCTIONS: System prompts - CONTEXT: RAG documents (items without role) - CONVERSATION: User/assistant history - INPUT: Final user query</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted text string ready for completion API</p> Example <p>prompt = packer.pack() response = completion.create(model=\"llama-2-70b\", prompt=prompt)</p> Source code in <code>src/prompt_refiner/packer/text_packer.py</code> <pre><code>def pack(self) -&gt; str:\n    \"\"\"\n    Pack items into formatted text for completion APIs.\n\n    MARKDOWN format uses grouped sections to reduce token overhead:\n    - INSTRUCTIONS: System prompts\n    - CONTEXT: RAG documents (items without role)\n    - CONVERSATION: User/assistant history\n    - INPUT: Final user query\n\n    Returns:\n        Formatted text string ready for completion API\n\n    Example:\n        &gt;&gt;&gt; prompt = packer.pack()\n        &gt;&gt;&gt; response = completion.create(model=\"llama-2-70b\", prompt=prompt)\n    \"\"\"\n    selected_items = self._greedy_select()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty string\")\n        return \"\"\n\n    # MARKDOWN format: Use grouped sections (saves tokens)\n    if self.text_format == TextFormat.MARKDOWN:\n        result = self._pack_markdown_grouped(selected_items)\n    else:\n        # RAW and XML: Use item-by-item formatting\n        parts = []\n        for item in selected_items:\n            formatted = self._format_item(item)\n            parts.append(formatted)\n        result = self.separator.join(parts)\n\n    logger.info(\n        f\"Packed {len(selected_items)} items into \"\n        f\"{self._count_tokens(result)} token text \"\n        f\"(format={self.text_format.value})\"\n    )\n    return result\n</code></pre>"},{"location":"api-reference/packer/#basepacker","title":"BasePacker","text":"<p>Abstract base class providing common packer functionality. You typically won't use this directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker","title":"prompt_refiner.packer.BasePacker","text":"<pre><code>BasePacker(max_tokens, model=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for token budget packers.</p> <p>Provides common functionality: - Adding items with priorities - JIT refinement with operations - Greedy selection algorithm - Token counting with safety buffer</p> <p>Subclasses must implement: - _calculate_overhead(): Calculate format-specific overhead - pack(): Format and return packed items</p> <p>Initialize packer with token budget.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum token budget</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting (requires tiktoken)</p> <code>None</code> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def __init__(self, max_tokens: int, model: Optional[str] = None):\n    \"\"\"\n    Initialize packer with token budget.\n\n    Args:\n        max_tokens: Maximum token budget\n        model: Optional model name for precise token counting (requires tiktoken)\n    \"\"\"\n    self.raw_max_tokens = max_tokens\n    self._items: List[PackableItem] = []\n    self._insertion_counter = 0\n    self._token_counter = CountTokens(model=model)\n\n    # Apply safety buffer for estimation mode\n    if not self._token_counter.is_precise:\n        self.effective_max_tokens = int(max_tokens * 0.9)\n        logger.debug(\n            f\"Using estimation mode with 10% safety buffer: \"\n            f\"{self.effective_max_tokens}/{max_tokens}\"\n        )\n    else:\n        self.effective_max_tokens = max_tokens\n        logger.debug(f\"Using precise mode with tiktoken: {self.effective_max_tokens} tokens\")\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add","title":"add","text":"<pre><code>add(\n    content,\n    role=None,\n    priority=PRIORITY_MEDIUM,\n    refine_with=None,\n)\n</code></pre> <p>Add an item to the packer.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text content to add</p> required <code>role</code> <code>Optional[str]</code> <p>Optional role for message APIs (system, user, assistant)</p> <code>None</code> <code>priority</code> <code>int</code> <p>Priority level (use PRIORITY_* constants)</p> <code>PRIORITY_MEDIUM</code> <code>refine_with</code> <code>Optional[Union[Operation, List[Operation]]]</code> <p>Optional operation(s) to apply before adding</p> <code>None</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add(\n    self,\n    content: str,\n    role: Optional[str] = None,\n    priority: int = PRIORITY_MEDIUM,\n    refine_with: Optional[Union[Operation, List[Operation]]] = None,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Add an item to the packer.\n\n    Args:\n        content: Text content to add\n        role: Optional role for message APIs (system, user, assistant)\n        priority: Priority level (use PRIORITY_* constants)\n        refine_with: Optional operation(s) to apply before adding\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # JIT refinement\n    if refine_with:\n        if isinstance(refine_with, list):\n            for op in refine_with:\n                content = op.process(content)\n        else:\n            content = refine_with.process(content)\n\n    # Count base tokens (without format overhead)\n    tokens = self._count_tokens(content)\n\n    item = PackableItem(\n        content=content,\n        tokens=tokens,\n        priority=priority,\n        insertion_index=self._insertion_counter,\n        role=role,\n    )\n\n    self._items.append(item)\n    self._insertion_counter += 1\n\n    logger.debug(f\"Added item: {tokens} tokens, priority={priority}, role={role}\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add_messages","title":"add_messages","text":"<pre><code>add_messages(messages, priority=PRIORITY_MEDIUM)\n</code></pre> <p>Batch add messages (convenience method).</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, str]]</code> <p>List of message dicts with 'role' and 'content' keys</p> required <code>priority</code> <code>int</code> <p>Priority level for all messages</p> <code>PRIORITY_MEDIUM</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add_messages(\n    self,\n    messages: List[Dict[str, str]],\n    priority: int = PRIORITY_MEDIUM,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Batch add messages (convenience method).\n\n    Args:\n        messages: List of message dicts with 'role' and 'content' keys\n        priority: Priority level for all messages\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    for msg in messages:\n        self.add(content=msg[\"content\"], role=msg[\"role\"], priority=priority)\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset the packer, removing all items.</p> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def reset(self) -&gt; \"BasePacker\":\n    \"\"\"\n    Reset the packer, removing all items.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._items.clear()\n    self._insertion_counter = 0\n    logger.debug(\"Packer reset\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.get_items","title":"get_items","text":"<pre><code>get_items()\n</code></pre> <p>Get information about all added items.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries containing item metadata</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def get_items(self) -&gt; List[dict]:\n    \"\"\"\n    Get information about all added items.\n\n    Returns:\n        List of dictionaries containing item metadata\n    \"\"\"\n    return [\n        {\n            \"priority\": item.priority,\n            \"tokens\": item.tokens,\n            \"insertion_index\": item.insertion_index,\n            \"role\": item.role,\n        }\n        for item in self._items\n    ]\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.pack","title":"pack  <code>abstractmethod</code>","text":"<pre><code>pack()\n</code></pre> <p>Pack items into final format.</p> <p>Subclasses must implement this to return format-specific output: - MessagesPacker: Returns List[Dict[str, str]] - TextPacker: Returns str</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>@abstractmethod\ndef pack(self):\n    \"\"\"\n    Pack items into final format.\n\n    Subclasses must implement this to return format-specific output:\n    - MessagesPacker: Returns List[Dict[str, str]]\n    - TextPacker: Returns str\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/packer/#constants","title":"Constants","text":""},{"location":"api-reference/packer/#priority-constants","title":"Priority Constants","text":"<pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_USER,     # 10 - Critical user input\n    PRIORITY_HIGH,     # 20 - Important context (core RAG documents)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG documents)\n    PRIORITY_LOW,      # 40 - Optional content (old conversation history)\n)\n</code></pre>"},{"location":"api-reference/packer/#overhead-constants","title":"Overhead Constants","text":"<pre><code>from prompt_refiner import (\n    PER_MESSAGE_OVERHEAD,  # 4 tokens - ChatML format overhead per message\n    PER_REQUEST_OVERHEAD,  # 3 tokens - Base request overhead (reserved for future use)\n)\n</code></pre> <p>These constants reflect the approximate token overhead for OpenAI's ChatML format (<code>&lt;|im_start|&gt;role\\n...\\n&lt;|im_end|&gt;</code>).</p>"},{"location":"api-reference/packer/#textformat-enum","title":"TextFormat Enum","text":"<pre><code>from prompt_refiner import TextFormat\n\nTextFormat.RAW       # No delimiters, simple concatenation\nTextFormat.MARKDOWN  # Use ### ROLE: headers (grouped sections in v0.1.3+)\nTextFormat.XML       # Use &lt;role&gt;content&lt;/role&gt; tags\n</code></pre>"},{"location":"api-reference/packer/#token-counting-modes","title":"Token Counting Modes","text":"<p>Estimation vs Precise Mode</p> <p>Both MessagesPacker and TextPacker use the same token counting as CountTokens:</p> <p>Estimation Mode (Default) - Uses character-based approximation: ~1 token \u2248 4 characters - Applies 10% safety buffer to prevent context overflow - Example: <code>max_tokens=1000</code> \u2192 <code>effective_max_tokens=900</code></p> <pre><code>packer = MessagesPacker(max_tokens=1000)  # 900 effective tokens\n</code></pre> <p>Precise Mode (Optional) - Requires <code>tiktoken</code>: <code>pip install llm-prompt-refiner[token]</code> - Exact token counting, no safety buffer (100% capacity) - Opt-in by passing a <code>model</code> parameter</p> <pre><code>packer = MessagesPacker(max_tokens=1000, model=\"gpt-4\")  # 1000 effective tokens\n</code></pre> <p>Recommendation: Use precise mode in production when you need maximum token utilization.</p>"},{"location":"api-reference/packer/#messagespacker-examples","title":"MessagesPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER\n\npacker = MessagesPacker(max_tokens=500)\n\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()  # List[Dict[str, str]]\n# Use directly: openai.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"api-reference/packer/#rag-with-conversation-history","title":"RAG with Conversation History","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_LOW,\n    StripHTML\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on provided context.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with JIT cleaning\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=\"system\",\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n\n# Old conversation history (can be dropped if needed)\nold_messages = [\n    {\"role\": \"user\", \"content\": \"What is this library?\"},\n    {\"role\": \"assistant\", \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages, priority=PRIORITY_LOW)\n\n# Current query (must include)\npacker.add(\n    \"How does it reduce costs?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#textpacker-examples","title":"TextPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, PRIORITY_SYSTEM, PRIORITY_USER\n\npacker = TextPacker(\n    max_tokens=500,\n    text_format=TextFormat.MARKDOWN\n)\n\npacker.add(\n    \"You are a QA assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"Context: Prompt-refiner is a library...\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nprompt = packer.pack()  # str\n# Use with: completion.create(prompt=prompt)\n</code></pre>"},{"location":"api-reference/packer/#text-format-comparison","title":"Text Format Comparison","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, PRIORITY_SYSTEM, PRIORITY_USER\n\n# RAW format (simple concatenation)\npacker = TextPacker(max_tokens=200, text_format=TextFormat.RAW)\npacker.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nprompt = packer.pack()\n# Output:\n# System prompt\n#\n# User query\n\n# MARKDOWN format (grouped sections in v0.1.3+)\npacker = TextPacker(max_tokens=200, text_format=TextFormat.MARKDOWN)\npacker.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Doc 1\", priority=PRIORITY_HIGH)\npacker.add(\"Doc 2\", priority=PRIORITY_HIGH)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nprompt = packer.pack()\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Doc 1\n# - Doc 2\n#\n# ### INPUT:\n# User query\n\n# XML format\npacker = TextPacker(max_tokens=200, text_format=TextFormat.XML)\npacker.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nprompt = packer.pack()\n# Output:\n# &lt;system&gt;\n# System prompt\n# &lt;/system&gt;\n#\n# &lt;user&gt;\n# User query\n# &lt;/user&gt;\n</code></pre>"},{"location":"api-reference/packer/#common-features","title":"Common Features","text":""},{"location":"api-reference/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Both packers support Just-In-Time refinement:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Single operation\npacker.add(\n    \"&lt;div&gt;HTML content&lt;/div&gt;\",\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n\n# Multiple operations\npacker.add(\n    \"&lt;p&gt;  Messy   HTML  &lt;/p&gt;\",\n    priority=PRIORITY_HIGH,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"api-reference/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER\n\nmessages = (\n    MessagesPacker(max_tokens=500)\n    .add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\n    .add(\"User query\", role=\"user\", priority=PRIORITY_USER)\n    .pack()\n)\n</code></pre>"},{"location":"api-reference/packer/#inspection","title":"Inspection","text":"<pre><code>packer = MessagesPacker(max_tokens=1000)\npacker.add(\"Item 1\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Item 2\", role=\"user\", priority=PRIORITY_USER)\n\n# Inspect items before packing\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Tokens: {item['tokens']}\")\n</code></pre>"},{"location":"api-reference/packer/#reset","title":"Reset","text":"<pre><code>packer = MessagesPacker(max_tokens=1000)\npacker.add(\"First batch\", role=\"user\", priority=PRIORITY_HIGH)\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=\"user\", priority=PRIORITY_HIGH)\nmessages2 = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#algorithm-details","title":"Algorithm Details","text":"<ol> <li>Add Phase: Items are added with priorities, optional roles, and optional JIT refinement</li> <li>Token Counting:</li> <li>MessagesPacker: content tokens + 4 tokens overhead (ChatML format)</li> <li>TextPacker RAW: content tokens + separator overhead</li> <li>TextPacker MARKDOWN: content tokens + marginal overhead (3-4 tokens per item after fixed header reservation)</li> <li>TextPacker XML: content tokens + tag overhead</li> <li>Sort Phase: Items are sorted by priority (lower number = higher priority)</li> <li>Greedy Packing: Items are selected sequentially if they fit within the token budget</li> <li>Order Restoration: Selected items are restored to insertion order for natural reading flow</li> <li>Format Phase:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code></li> <li>TextPacker: Returns formatted <code>str</code> based on <code>text_format</code></li> </ol>"},{"location":"api-reference/packer/#tips","title":"Tips","text":"<p>Choose the Right Packer</p> <ul> <li>Use MessagesPacker for chat APIs (OpenAI, Anthropic)</li> <li>Use TextPacker for completion APIs (Llama Base, GPT-3)</li> </ul> <p>Choose the Right Priority</p> <ul> <li><code>PRIORITY_SYSTEM</code>: System prompts, critical instructions</li> <li><code>PRIORITY_USER</code>: User input, current queries</li> <li><code>PRIORITY_HIGH</code>: Core context, most relevant documents</li> <li><code>PRIORITY_MEDIUM</code>: Supporting context, moderately relevant</li> <li><code>PRIORITY_LOW</code>: Optional content, old history</li> </ul> <p>Clean Before Packing</p> <p>Use <code>refine_with</code> to clean items before token counting:</p> <pre><code>packer.add(\n    dirty_html,\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n</code></pre> <p>Monitor Token Usage</p> <p>Check effective token budget and utilization:</p> <pre><code>packer = MessagesPacker(max_tokens=1000)\nprint(f\"Effective budget: {packer.effective_max_tokens}\")  # 900 in estimation mode\n</code></pre> <p>Grouped MARKDOWN Saves Tokens</p> <p>TextPacker with MARKDOWN format groups items by section, saving tokens:</p> <pre><code># Old (per-item headers): ### CONTEXT:\\nDoc 1\\n\\n### CONTEXT:\\nDoc 2\n# New (grouped): ### CONTEXT:\\n- Doc 1\\n- Doc 2\n</code></pre>"},{"location":"api-reference/refiner/","title":"Refiner Class","text":"<p>The <code>Refiner</code> class is the core pipeline builder that allows you to chain multiple operations together.</p>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner","title":"prompt_refiner.refiner.Refiner","text":"<pre><code>Refiner()\n</code></pre> <p>A pipeline builder for prompt refining operations.</p> <p>Initialize an empty refiner pipeline.</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize an empty refiner pipeline.\"\"\"\n    self._operations: List[Operation] = []\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner-functions","title":"Functions","text":""},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.pipe","title":"pipe","text":"<pre><code>pipe(operation)\n</code></pre> <p>Add an operation to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>Operation</code> <p>The operation to add</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def pipe(self, operation: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Add an operation to the pipeline.\n\n    Args:\n        operation: The operation to add\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._operations.append(operation)\n    return self\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.run","title":"run","text":"<pre><code>run(text)\n</code></pre> <p>Execute the pipeline on the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>The processed text after all operations</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def run(self, text: str) -&gt; str:\n    \"\"\"\n    Execute the pipeline on the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text after all operations\n    \"\"\"\n    result = text\n    for operation in self._operations:\n        result = operation.process(result)\n    return result\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre> <p>Support pipe operator syntax for adding operations to the pipeline.</p> <p>Enables continued chaining: (op1 | op2) | op3</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Operation</code> <p>The operation to add to the pipeline</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Example <p>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100) result = pipeline.run(text)</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __or__(self, other: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Support pipe operator syntax for adding operations to the pipeline.\n\n    Enables continued chaining: (op1 | op2) | op3\n\n    Args:\n        other: The operation to add to the pipeline\n\n    Returns:\n        Self for method chaining\n\n    Example:\n        &gt;&gt;&gt; from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n        &gt;&gt;&gt; pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100)\n        &gt;&gt;&gt; result = pipeline.run(text)\n    \"\"\"\n    return self.pipe(other)\n</code></pre>"},{"location":"api-reference/refiner/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/refiner/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre>"},{"location":"api-reference/refiner/#fluent-api-with-pipe","title":"Fluent API with .pipe()","text":"<p>The <code>Refiner</code> class supports method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the fluent API\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre> <p>Both approaches work identically - choose the one that fits your style.</p>"},{"location":"api-reference/refiner/#pipeline-execution","title":"Pipeline Execution","text":"<p>When you call <code>run(text)</code>, the Refiner:</p> <ol> <li>Takes the input text</li> <li>Passes it through each operation in sequence</li> <li>Each operation's output becomes the next operation's input</li> <li>Returns the final processed text</li> </ol> <pre><code># Pipeline: text \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 result\nresult = refiner.run(text)\n</code></pre>"},{"location":"api-reference/scrubber/","title":"Scrubber Module","text":"<p>The Scrubber module provides operations for security and privacy, including automatic PII redaction.</p>"},{"location":"api-reference/scrubber/#redactpii","title":"RedactPII","text":"<p>Redact sensitive personally identifiable information (PII) from text using regex patterns.</p>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII","title":"prompt_refiner.scrubber.RedactPII","text":"<pre><code>RedactPII(\n    redact_types=None,\n    custom_patterns=None,\n    custom_keywords=None,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Redact sensitive information from text using regex patterns.</p> <p>Initialize the PII redaction operation.</p> <p>Parameters:</p> Name Type Description Default <code>redact_types</code> <code>Optional[Set[str]]</code> <p>Set of PII types to redact (default: all) Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"</p> <code>None</code> <code>custom_patterns</code> <code>Optional[dict[str, str]]</code> <p>Dictionary of custom regex patterns to apply Format: {\"name\": \"regex_pattern\"}</p> <code>None</code> <code>custom_keywords</code> <code>Optional[Set[str]]</code> <p>Set of custom keywords to redact (case-insensitive)</p> <code>None</code> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def __init__(\n    self,\n    redact_types: Optional[Set[str]] = None,\n    custom_patterns: Optional[dict[str, str]] = None,\n    custom_keywords: Optional[Set[str]] = None,\n):\n    \"\"\"\n    Initialize the PII redaction operation.\n\n    Args:\n        redact_types: Set of PII types to redact (default: all)\n            Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"\n        custom_patterns: Dictionary of custom regex patterns to apply\n            Format: {\"name\": \"regex_pattern\"}\n        custom_keywords: Set of custom keywords to redact (case-insensitive)\n    \"\"\"\n    self.redact_types = redact_types or set(self.PATTERNS.keys())\n    self.custom_patterns = custom_patterns or {}\n    self.custom_keywords = custom_keywords or set()\n</code></pre>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII-functions","title":"Functions","text":""},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Redact PII from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with PII redacted</p> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Redact PII from the input text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with PII redacted\n    \"\"\"\n    result = text\n\n    # Apply standard PII patterns\n    for pii_type in self.redact_types:\n        if pii_type in self.PATTERNS:\n            pattern = self.PATTERNS[pii_type]\n            replacement = self.REPLACEMENTS.get(pii_type, \"[REDACTED]\")\n            result = re.sub(pattern, replacement, result)\n\n    # Apply custom patterns\n    for name, pattern in self.custom_patterns.items():\n        replacement = f\"[{name.upper()}]\"\n        result = re.sub(pattern, replacement, result)\n\n    # Apply custom keywords (case-insensitive)\n    for keyword in self.custom_keywords:\n        # Use word boundaries to avoid partial matches\n        pattern = rf\"\\b{re.escape(keyword)}\\b\"\n        result = re.sub(pattern, \"[REDACTED]\", result, flags=re.IGNORECASE)\n\n    return result\n</code></pre>"},{"location":"api-reference/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code>: Email addresses \u2192 <code>[EMAIL]</code></li> <li><code>phone</code>: Phone numbers (US format) \u2192 <code>[PHONE]</code></li> <li><code>ip</code>: IP addresses \u2192 <code>[IP]</code></li> <li><code>credit_card</code>: Credit card numbers \u2192 <code>[CARD]</code></li> <li><code>ssn</code>: Social Security Numbers \u2192 <code>[SSN]</code></li> <li><code>url</code>: URLs \u2192 <code>[URL]</code></li> </ul>"},{"location":"api-reference/scrubber/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact me at john@example.com or 555-123-4567\")\n# Output: \"Contact me at [EMAIL] or [PHONE]\"\n\n# Redact specific types only\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\nresult = redactor.process(\"Email: john@example.com, IP: 192.168.1.1\")\n# Output: \"Email: [EMAIL], IP: 192.168.1.1\"\n\n# Custom patterns\nredactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\nresult = redactor.process(\"Employee EMP-12345 accessed the system\")\n# Output: \"Employee [EMPLOYEE_ID] accessed the system\"\n\n# Custom keywords (case-insensitive)\nredactor = RedactPII(\n    custom_keywords={\"confidential\", \"secret\"}\n)\nresult = redactor.process(\"This is Confidential information\")\n# Output: \"This is [REDACTED] information\"\n</code></pre>"},{"location":"api-reference/scrubber/#combining-options","title":"Combining Options","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact standard PII + custom patterns + keywords\nredactor = RedactPII(\n    redact_types={\"email\", \"phone\", \"ssn\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"},\n    custom_keywords={\"internal\", \"confidential\"}\n)\n\ntext = \"\"\"\nEmployee EMP-12345\nEmail: john@example.com\nPhone: 555-123-4567\nSSN: 123-45-6789\nThis is Confidential information for internal use only.\n\"\"\"\n\nresult = redactor.process(text)\n</code></pre>"},{"location":"api-reference/scrubber/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/scrubber/#before-sending-to-llm-apis","title":"Before Sending to LLM APIs","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nsecure_pipeline = (\n    Refiner()\n    .pipe(RedactPII(redact_types={\"email\", \"phone\", \"ssn\", \"credit_card\"}))\n)\n\n# Safe to send to external APIs\nsecure_text = secure_pipeline.run(user_input)\n</code></pre>"},{"location":"api-reference/scrubber/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nlog_redactor = (\n    Refiner()\n    .pipe(RedactPII())  # Redact all PII types\n)\n\n# Safe to log\nsafe_log = log_redactor.run(sensitive_data)\nlogger.info(safe_log)\n</code></pre>"},{"location":"api-reference/scrubber/#data-export-compliance","title":"Data Export Compliance","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\n# Custom redaction for specific compliance needs\ngdpr_redactor = (\n    Refiner()\n    .pipe(RedactPII(\n        redact_types={\"email\", \"phone\", \"ip\"},\n        custom_keywords={\"customer_name\", \"address\", \"dob\"}\n    ))\n)\n\nexport_data = gdpr_redactor.run(user_data)\n</code></pre>"},{"location":"api-reference/scrubber/#security-best-practices","title":"Security Best Practices","text":"<p>Regex Limitations</p> <p>PII redaction uses regex patterns which may not catch all variations. For production use:</p> <ul> <li>Test thoroughly with your specific data</li> <li>Consider using specialized PII detection services for critical applications</li> <li>Add custom patterns for domain-specific PII</li> <li>Review redacted output before sending to external services</li> </ul> <p>Defense in Depth</p> <p>PII redaction is one layer of security. Always:</p> <ul> <li>Validate and sanitize user input</li> <li>Use proper authentication and authorization</li> <li>Encrypt data in transit and at rest</li> <li>Follow your organization's security policies</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for each module in Prompt Refiner.</p>"},{"location":"examples/#by-module","title":"By Module","text":""},{"location":"examples/#cleaner-examples","title":"Cleaner Examples","text":"<ul> <li>HTML Cleaning - Strip HTML tags and convert to Markdown</li> <li>See more in: Cleaner Module</li> </ul>"},{"location":"examples/#compressor-examples","title":"Compressor Examples","text":"<ul> <li>Deduplication - Remove duplicate content from RAG results</li> <li>See more in: Compressor Module</li> </ul>"},{"location":"examples/#scrubber-examples","title":"Scrubber Examples","text":"<ul> <li>PII Redaction - Redact sensitive information</li> <li>See more in: Scrubber Module</li> </ul>"},{"location":"examples/#analyzer-examples","title":"Analyzer Examples","text":"<ul> <li>Token Analysis - Calculate token savings and ROI</li> <li>See more in: Analyzer Module</li> </ul>"},{"location":"examples/#packer-examples-advanced","title":"Packer Examples (Advanced)","text":"<ul> <li>Context Budget Management - RAG applications, chatbots, and conversation history</li> <li>See more in: Packer Module</li> </ul>"},{"location":"examples/#complete-examples","title":"Complete Examples","text":"<ul> <li>Complete Pipeline - Full optimization with all modules</li> </ul>"},{"location":"examples/#running-examples-locally","title":"Running Examples Locally","text":"<p>All examples are available in the <code>examples/</code> directory:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run an example\npython examples/all_modules_demo.py\n</code></pre>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Getting Started Guide</li> <li>API Reference</li> <li>Report Issues</li> </ul>"},{"location":"examples/complete-pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive example using all 4 modules together.</p>"},{"location":"examples/complete-pipeline/#full-optimization-pipeline","title":"Full Optimization Pipeline","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\n# Messy input with HTML, PII, duplicates\nmessy_input = \"\"\"\n&lt;div&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;We provide excellent service   with   lots   of   spaces.&lt;/p&gt;\n    &lt;p&gt;Our IP address is 192.168.1.1 for reference.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Initialize counter\ncounter = CountTokens(original_text=messy_input)\n\n# Build complete pipeline using pipe operator (recommended)\npipeline = (\n    # Clean dirty data\n    StripHTML()\n    | FixUnicode()\n    | NormalizeWhitespace()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=50, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\", \"ip\"})\n    # Analyze\n    | counter\n)\n\n# Run pipeline\nresult = pipeline.run(messy_input)\n\nprint(\"Optimized result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(counter.format_stats())\n</code></pre> <p>Alternative: Fluent API</p> <p>You can also use <code>.pipe()</code> method chaining: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n    # ... continue with other operations\n)\n</code></pre></p>"},{"location":"examples/complete-pipeline/#full-example","title":"Full Example","text":"<p>See: <code>examples/all_modules_demo.py</code></p> <pre><code>python examples/all_modules_demo.py\n</code></pre>"},{"location":"examples/complete-pipeline/#related","title":"Related","text":"<ul> <li>Pipeline Basics</li> <li>All Modules Overview</li> </ul>"},{"location":"examples/deduplication/","title":"Deduplication Example","text":"<p>Remove duplicate content from RAG retrieval results.</p>"},{"location":"examples/deduplication/#scenario","title":"Scenario","text":"<p>Your RAG system retrieved multiple similar chunks that contain overlapping information.</p>"},{"location":"examples/deduplication/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# RAG results with duplicates\nrag_results = \"\"\"\nPython is a high-level programming language.\n\nPython is a high level programming language.\n\nPython supports multiple programming paradigms.\n\"\"\"\n\npipeline = Deduplicate(similarity_threshold=0.85)\ndeduplicated = pipeline.run(rag_results)\n\nprint(deduplicated)\n# Output: Only unique paragraphs remain\n</code></pre>"},{"location":"examples/deduplication/#adjusting-sensitivity","title":"Adjusting Sensitivity","text":"<pre><code># More aggressive (70% similarity)\npipeline = Deduplicate(similarity_threshold=0.70)\n\n# Sentence-level deduplication\npipeline = Deduplicate(granularity=\"sentence\")\n</code></pre>"},{"location":"examples/deduplication/#performance-considerations","title":"Performance Considerations","text":"<p>When working with large RAG contexts, keep these performance tips in mind:</p>"},{"location":"examples/deduplication/#choosing-a-similarity-method","title":"Choosing a Similarity Method","text":"<pre><code># Fast: Jaccard (word-based) - recommended for most use cases\npipeline = Deduplicate(method=\"jaccard\")\n\n# Precise but slower: Levenshtein (character-based)\n# Only use when you need character-level accuracy\npipeline = Deduplicate(method=\"levenshtein\")\n</code></pre>"},{"location":"examples/deduplication/#scaling-with-input-size","title":"Scaling with Input Size","text":"<p>The deduplication algorithm compares each chunk against all previous chunks (O(n\u00b2)):</p> <ul> <li>10-50 chunks: Fast with either method (typical RAG use case)</li> <li>50-200 chunks: Use Jaccard for better performance</li> <li>200+ chunks: Use <code>granularity=\"paragraph\"</code> to reduce chunk count</li> </ul> <pre><code># For large documents: use paragraph granularity\npipeline = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\"  # Fewer chunks = faster\n)\n</code></pre>"},{"location":"examples/deduplication/#full-example","title":"Full Example","text":"<p>See: <code>examples/compressor/deduplication.py</code></p>"},{"location":"examples/deduplication/#related","title":"Related","text":"<ul> <li>Deduplicate API Reference</li> <li>Compressor Module Guide</li> </ul>"},{"location":"examples/html-cleaning/","title":"HTML Cleaning Example","text":"<p>Clean HTML content from web scraping or user input.</p>"},{"location":"examples/html-cleaning/#scenario","title":"Scenario","text":"<p>You've scraped content from a website and need to clean it before sending to an LLM API.</p>"},{"location":"examples/html-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\nhtml_content = \"\"\"\n&lt;div class=\"article\"&gt;\n    &lt;h1&gt;Understanding &lt;strong&gt;LLMs&lt;/strong&gt;&lt;/h1&gt;\n    &lt;p&gt;Large Language Models are powerful &lt;em&gt;AI systems&lt;/em&gt;.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Remove all HTML and normalize whitespace\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\ncleaned = pipeline.run(html_content)\nprint(cleaned)\n# Output: \"Understanding LLMs Large Language Models are powerful AI systems.\"\n</code></pre>"},{"location":"examples/html-cleaning/#converting-to-markdown","title":"Converting to Markdown","text":"<pre><code># Convert HTML to Markdown instead of removing\npipeline = (\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n)\n\nmarkdown = pipeline.run(html_content)\nprint(markdown)\n# Output:\n# # Understanding **LLMs**\n#\n# Large Language Models are powerful *AI systems*.\n</code></pre>"},{"location":"examples/html-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/html_cleaning.py</code></p> <pre><code>python examples/cleaner/html_cleaning.py\n</code></pre>"},{"location":"examples/html-cleaning/#related","title":"Related","text":"<ul> <li>StripHTML API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/packer/","title":"Packer Examples","text":"<p>Advanced examples for managing context budgets with MessagesPacker and TextPacker.</p> <p>When to Use Packer</p> <p>Packer is ideal for:</p> <ul> <li>RAG Applications: Pack multiple retrieved documents within token budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"examples/packer/#example-1-basic-rag-with-messagespacker","title":"Example 1: Basic RAG with MessagesPacker","text":"<p>Pack RAG documents for chat APIs with priority-based selection:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_MEDIUM,\n)\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on the provided context only.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with different priorities\npacker.add(\n    \"Product X costs $99 and includes 1-year warranty.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH  # Most relevant document\n)\n\npacker.add(\n    \"We offer free shipping on orders over $50.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM  # Less relevant document\n)\n\npacker.add(\n    \"Customer reviews rate Product X 4.5/5 stars.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM\n)\n\n# User query (must include)\npacker.add(\n    \"What is the price of Product X?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into messages format\nmessages = packer.pack()\n\n# Use directly with OpenAI\n# response = client.chat.completions.create(\n#     model=\"gpt-4\",\n#     messages=messages\n# )\n\nprint(f\"Packed {len(messages)} messages\")\nfor msg in messages:\n    print(f\"{msg['role']}: {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-2-rag-with-dirty-html-jit-refinement","title":"Example 2: RAG with Dirty HTML (JIT Refinement)","text":"<p>Clean web-scraped RAG documents on-the-fly:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    StripHTML,\n    NormalizeWhitespace\n)\n\npacker = MessagesPacker(max_tokens=800)\n\n# System prompt\npacker.add(\n    \"You are a helpful product assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents from web scraping (with HTML)\ndocs = [\n    \"&lt;div class='product'&gt;&lt;h2&gt;Product Features&lt;/h2&gt;&lt;p&gt;  Waterproof   design  &lt;/p&gt;&lt;/div&gt;\",\n    \"&lt;html&gt;&lt;body&gt;   &lt;p&gt;Available in  &lt;b&gt;5 colors&lt;/b&gt;:  red, blue...&lt;/p&gt;  &lt;/body&gt;&lt;/html&gt;\",\n    \"&lt;article&gt;   Battery life:  &lt;strong&gt;  48 hours  &lt;/strong&gt;  continuous use  &lt;/article&gt;\"\n]\n\n# Clean each document before adding (JIT refinement)\nfor doc in docs:\n    packer.add(\n        doc,\n        role=\"system\",\n        priority=PRIORITY_HIGH,\n        refine_with=[StripHTML(), NormalizeWhitespace()]  # Clean on-the-fly!\n    )\n\n# User query\npacker.add(\n    \"What are the key features?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# All HTML is automatically cleaned before packing\nprint(\"Cleaned messages:\")\nfor msg in messages:\n    if msg['role'] == 'system' and 'Waterproof' in msg['content']:\n        print(f\"Before: {docs[0][:50]}...\")\n        print(f\"After:  {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-3-chatbot-with-conversation-history","title":"Example 3: Chatbot with Conversation History","text":"<p>Manage conversation history with priorities - old messages can be dropped:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_LOW,\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (highest priority)\npacker.add(\n    \"You are a helpful customer support agent.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG: Current relevant documentation\npacker.add(\n    \"Return policy: 30-day money-back guarantee for all products.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH\n)\n\n# Old conversation history (can be dropped if budget is tight)\nold_conversation = [\n    {\"role\": \"user\", \"content\": \"What are your business hours?\"},\n    {\"role\": \"assistant\", \"content\": \"We're open 9 AM - 5 PM EST, Monday-Friday.\"},\n    {\"role\": \"user\", \"content\": \"Do you ship internationally?\"},\n    {\"role\": \"assistant\", \"content\": \"Yes, we ship to over 50 countries worldwide.\"}\n]\n\npacker.add_messages(old_conversation, priority=PRIORITY_LOW)\n\n# Current user query (highest priority)\npacker.add(\n    \"What is your return policy?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# If budget is tight, old history is dropped, but system prompt + current query are kept\nprint(f\"Packed {len(messages)} messages (old history may be dropped)\")\n</code></pre>"},{"location":"examples/packer/#example-4-textpacker-for-base-models","title":"Example 4: TextPacker for Base Models","text":"<p>Use TextPacker with Llama or GPT-3 base models:</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\n# Use MARKDOWN format for better structure\npacker = TextPacker(\n    max_tokens=600,\n    text_format=TextFormat.MARKDOWN\n)\n\n# System instructions\npacker.add(\n    \"You are a QA assistant. Answer based on the context provided.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents (no role = treated as context)\npacker.add(\n    \"Prompt-refiner is a Python library for optimizing LLM inputs.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"It reduces token usage by 4-15% through cleaning and compression.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"The library has zero dependencies by default.\",\n    priority=PRIORITY_HIGH\n)\n\n# User query\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into formatted text\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# ### INSTRUCTIONS:\n# You are a QA assistant. Answer based on the context provided.\n#\n# ### CONTEXT:\n# - Prompt-refiner is a Python library for optimizing LLM inputs.\n# - It reduces token usage by 4-15% through cleaning and compression.\n# - The library has zero dependencies by default.\n#\n# ### INPUT:\n# What is prompt-refiner?\n\n# Use with completion API\n# response = client.completions.create(\n#     model=\"llama-2-70b\",\n#     prompt=prompt\n# )\n</code></pre>"},{"location":"examples/packer/#example-5-textpacker-with-xml-format","title":"Example 5: TextPacker with XML Format","text":"<p>Use XML format (Anthropic best practice for Claude base models):</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\npacker = TextPacker(\n    max_tokens=500,\n    text_format=TextFormat.XML\n)\n\npacker.add(\n    \"You are a code review assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"Code snippet: def hello(): return 'world'\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"Please review this code for best practices.\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# &lt;system&gt;\n# You are a code review assistant.\n# &lt;/system&gt;\n#\n# &lt;context&gt;\n# Code snippet: def hello(): return 'world'\n# &lt;/context&gt;\n#\n# &lt;user&gt;\n# Please review this code for best practices.\n# &lt;/user&gt;\n</code></pre>"},{"location":"examples/packer/#example-6-precise-mode-for-maximum-token-utilization","title":"Example 6: Precise Mode for Maximum Token Utilization","text":"<p>Use precise mode with tiktoken for 100% token budget utilization:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER\n\n# Install tiktoken: pip install llm-prompt-refiner[token]\n\n# Estimation mode (default): 10% safety buffer\npacker_estimate = MessagesPacker(max_tokens=1000)\nprint(f\"Estimation mode: {packer_estimate.effective_max_tokens} effective tokens\")\n# Output: Estimation mode: 900 effective tokens\n\n# Precise mode: 100% budget utilization (no safety buffer)\npacker_precise = MessagesPacker(max_tokens=1000, model=\"gpt-4\")\nprint(f\"Precise mode: {packer_precise.effective_max_tokens} effective tokens\")\n# Output: Precise mode: 997 effective tokens (1000 - 3 request overhead)\n\n# Use precise mode for production to maximize token capacity\npacker_precise.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker_precise.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nmessages = packer_precise.pack()\n</code></pre>"},{"location":"examples/packer/#example-7-inspection-and-debugging","title":"Example 7: Inspection and Debugging","text":"<p>Inspect items before packing to understand token distribution:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_HIGH, PRIORITY_USER\n\npacker = MessagesPacker(max_tokens=500)\n\npacker.add(\"System prompt here\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Document 1\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"Document 2\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\n\n# Inspect items before packing\nitems = packer.get_items()\n\nprint(\"Items before packing:\")\nfor i, item in enumerate(items):\n    print(f\"{i+1}. Priority: {item['priority']}, Tokens: {item['tokens']}, Role: {item['role']}\")\n\n# Pack and see which items fit\nmessages = packer.pack()\nprint(f\"\\nPacked {len(messages)}/{len(items)} items\")\n</code></pre>"},{"location":"examples/packer/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Choose the Right Packer:</li> <li><code>MessagesPacker</code> for chat APIs (OpenAI, Anthropic)</li> <li> <p><code>TextPacker</code> for completion APIs (Llama Base, GPT-3)</p> </li> <li> <p>Set Priorities Correctly:</p> </li> <li><code>PRIORITY_SYSTEM</code> (0): System prompts, absolute must-have</li> <li><code>PRIORITY_USER</code> (10): User queries, critical</li> <li><code>PRIORITY_HIGH</code> (20): Core RAG documents</li> <li><code>PRIORITY_MEDIUM</code> (30): Supporting context</li> <li> <p><code>PRIORITY_LOW</code> (40): Old conversation history</p> </li> <li> <p>Use JIT Refinement:</p> </li> <li>Clean dirty documents with <code>refine_with</code> parameter</li> <li> <p>Chain multiple operations: <code>refine_with=[StripHTML(), NormalizeWhitespace()]</code></p> </li> <li> <p>Optimize for Production:</p> </li> <li>Use precise mode with <code>model</code> parameter for 100% token utilization</li> <li>Choose appropriate text format for base models (MARKDOWN recommended)</li> </ol>"},{"location":"examples/packer/#related-documentation","title":"Related Documentation","text":"<ul> <li>Packer Module Guide</li> <li>Packer API Reference</li> <li>Getting Started</li> </ul>"},{"location":"examples/pii-redaction/","title":"PII Redaction Example","text":"<p>Automatically redact sensitive information before sending to APIs.</p>"},{"location":"examples/pii-redaction/#scenario","title":"Scenario","text":"<p>User input contains personal information that should not be sent to external LLM APIs.</p>"},{"location":"examples/pii-redaction/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import RedactPII\n\nuser_input = \"\"\"\nPlease contact me at john.doe@example.com or call 555-123-4567.\nMy account number is EMP-12345.\n\"\"\"\n\npipeline = RedactPII()\nsecure = pipeline.run(user_input)\n\nprint(secure)\n# Output:\n# Please contact me at [EMAIL] or call [PHONE].\n# My account number is EMP-12345.\n</code></pre>"},{"location":"examples/pii-redaction/#custom-patterns","title":"Custom Patterns","text":"<pre><code>pipeline = RedactPII(\n    redact_types={\"email\", \"phone\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n\nsecure = pipeline.run(user_input)\n# Now EMP-12345 is also redacted as [EMPLOYEE_ID]\n</code></pre>"},{"location":"examples/pii-redaction/#full-example","title":"Full Example","text":"<p>See: <code>examples/scrubber/pii_redaction.py</code></p>"},{"location":"examples/pii-redaction/#related","title":"Related","text":"<ul> <li>RedactPII API Reference</li> <li>Scrubber Module Guide</li> </ul>"},{"location":"examples/token-analysis/","title":"Token Analysis Example","text":"<p>Measure optimization impact and calculate cost savings.</p>"},{"location":"examples/token-analysis/#scenario","title":"Scenario","text":"<p>You want to demonstrate the value of prompt optimization.</p>"},{"location":"examples/token-analysis/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   from   HTML  &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | counter\n)\n\nresult = pipeline.run(original_text)\n\n# Show statistics\nprint(counter.format_stats())\n# Output:\n# Original: 10 tokens\n# Cleaned: 4 tokens\n# Saved: 6 tokens (60.0%)\n</code></pre>"},{"location":"examples/token-analysis/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\n\n# GPT-4 pricing: $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\n\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings_per_request = original_cost - cleaned_cost\n\nprint(f\"Savings: ${savings_per_request:.4f} per request\")\n\n# Project annual savings\nrequests_per_day = 10000\nannual_savings = savings_per_request * requests_per_day * 365\nprint(f\"Annual savings: ${annual_savings:.2f}\")\n</code></pre>"},{"location":"examples/token-analysis/#full-example","title":"Full Example","text":"<p>See: <code>examples/analyzer/token_counting.py</code></p>"},{"location":"examples/token-analysis/#related","title":"Related","text":"<ul> <li>CountTokens API Reference</li> <li>Analyzer Module Guide</li> </ul>"},{"location":"modules/analyzer/","title":"Analyzer Module","text":"<p>Track optimization impact and demonstrate value with token counting and statistics.</p>"},{"location":"modules/analyzer/#counttokens-operation","title":"CountTokens Operation","text":"<p>Measure token usage before and after optimization.</p>"},{"location":"modules/analyzer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World&lt;/p&gt;\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"modules/analyzer/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\ncost_per_token = 0.03 / 1000  # GPT-4 pricing\nsavings = stats['saved'] * cost_per_token\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data from various sources.</p>"},{"location":"modules/cleaner/#overview","title":"Overview","text":"<p>When working with real-world text data, you often encounter:</p> <ul> <li>HTML tags from web scraping</li> <li>Excessive whitespace and formatting issues</li> <li>Problematic Unicode characters</li> </ul> <p>The Cleaner module addresses these issues efficiently.</p>"},{"location":"modules/cleaner/#operations","title":"Operations","text":""},{"location":"modules/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags or convert them to Markdown.</p> <p>Use cases:</p> <ul> <li>Web scraping</li> <li>Email content processing</li> <li>User-generated HTML content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import StripHTML\n\n# Remove all HTML\ncleaner = StripHTML()\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\ncleaner = StripHTML(to_markdown=True)\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines.</p> <p>Use cases:</p> <ul> <li>Text from PDFs</li> <li>User input normalization</li> <li>Copy-pasted content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import NormalizeWhitespace\n\ncleaner = NormalizeWhitespace()\nresult = cleaner.run(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters.</p> <p>Use cases:</p> <ul> <li>Zero-width spaces from copy-paste</li> <li>Control characters</li> <li>Invisible characters causing issues</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import FixUnicode\n\ncleaner = FixUnicode()\nresult = cleaner.run(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/cleaner/#web-content-pipeline","title":"Web Content Pipeline","text":"<pre><code>from prompt_refiner import StripHTML, FixUnicode, NormalizeWhitespace\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import FixUnicode, NormalizeWhitespace\n\nnormalizer = (\n    FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples</li> <li>Full API Reference</li> <li>Explore Other Modules</li> </ul>"},{"location":"modules/compressor/","title":"Compressor Module","text":"<p>Reduce text size while preserving meaning through smart truncation and deduplication.</p>"},{"location":"modules/compressor/#operations","title":"Operations","text":""},{"location":"modules/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Smart text truncation respecting sentence boundaries.</p> <pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\n\n# Keep last 100 tokens (for conversation history)\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\n\n# Keep beginning and end, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or similar content chunks.</p> <pre><code>from prompt_refiner import Deduplicate\n\n# Remove paragraphs with 85% similarity\ndeduper = Deduplicate(similarity_threshold=0.85)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(granularity=\"sentence\")\n</code></pre> <p>Performance Considerations:</p> <ul> <li>Method Choice: Use <code>jaccard</code> (default) for most cases - it's fast and works well with typical prompts. Only use <code>levenshtein</code> when you need character-level precision.</li> <li>Complexity: Deduplication uses O(n\u00b2) comparisons where n is the number of chunks. For 50 chunks, this is ~1,225 comparisons.</li> <li>Large Inputs: For 200+ chunks, use <code>granularity=\"paragraph\"</code> to reduce chunk count and speed up processing.</li> <li>Jaccard: O(m) per comparison - fast even with long chunks</li> <li>Levenshtein: O(m\u2081 \u00d7 m\u2082) per comparison - can be slow with chunks over 1000 characters</li> </ul> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"modules/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n</code></pre> <p>View Examples</p>"},{"location":"modules/overview/","title":"Modules Overview","text":"<p>Prompt Refiner is organized into 5 specialized modules, each designed to handle a specific aspect of prompt optimization.</p>"},{"location":"modules/overview/#the-5-core-modules","title":"The 5 Core Modules","text":""},{"location":"modules/overview/#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<p>The Cleaner module removes unwanted artifacts from your text.</p> <p>Operations:</p> <ul> <li>StripHTML - Remove or convert HTML tags</li> <li>NormalizeWhitespace - Collapse excessive whitespace</li> <li>FixUnicode - Remove problematic Unicode characters</li> </ul> <p>When to use:</p> <ul> <li>Processing web-scraped content</li> <li>Cleaning user-generated text</li> <li>Normalizing text from various sources</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<p>The Compressor module reduces token count while preserving meaning.</p> <p>Operations:</p> <ul> <li>TruncateTokens - Smart text truncation with sentence boundaries</li> <li>Deduplicate - Remove similar or duplicate content</li> </ul> <p>When to use:</p> <ul> <li>Fitting content within context windows</li> <li>Optimizing RAG retrieval results</li> <li>Reducing API costs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<p>The Scrubber module protects sensitive information.</p> <p>Operations:</p> <ul> <li>RedactPII - Automatically redact personally identifiable information</li> </ul> <p>When to use:</p> <ul> <li>Before sending data to external APIs</li> <li>Compliance with privacy regulations</li> <li>Protecting user data in logs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#4-analyzer-show-value","title":"4. Analyzer - Show Value","text":"<p>The Analyzer module tracks optimization impact.</p> <p>Operations:</p> <ul> <li>CountTokens - Measure token savings and calculate ROI</li> </ul> <p>When to use:</p> <ul> <li>Demonstrating cost savings</li> <li>A/B testing optimization strategies</li> <li>Monitoring optimization impact</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#5-packer-context-budget-management","title":"5. Packer - Context Budget Management","text":"<p>The Packer module manages context budgets with intelligent priority-based item selection.</p> <p>Operations:</p> <ul> <li>MessagesPacker - Pack items for chat completion APIs</li> <li>TextPacker - Pack items for text completion APIs</li> </ul> <p>When to use:</p> <ul> <li>RAG applications with multiple documents</li> <li>Chatbots with conversation history</li> <li>Managing context windows with size limits</li> <li>Combining system prompts, user input, and documents</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#combining-modules","title":"Combining Modules","text":"<p>The real power comes from combining modules:</p>"},{"location":"modules/overview/#pipeline-example","title":"Pipeline Example","text":"<pre><code>from prompt_refiner import (\n    Refiner,\n    StripHTML, NormalizeWhitespace,  # Cleaner\n    TruncateTokens,                  # Compressor\n    RedactPII,                       # Scrubber\n    CountTokens                      # Analyzer\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    Refiner()\n    # Clean first\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    # Then compress\n    .pipe(TruncateTokens(max_tokens=1000))\n    # Secure\n    .pipe(RedactPII())\n    # Analyze\n    .pipe(counter)\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"modules/overview/#packer-example","title":"Packer Example","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    StripHTML\n)\n\n# Manage RAG context budget for chat APIs\npacker = MessagesPacker(max_tokens=1000)\n\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Clean documents before packing\nfor doc in retrieved_docs:\n    packer.add(\n        doc.content,\n        role=\"system\",\n        priority=PRIORITY_HIGH,\n        refine_with=StripHTML()\n    )\n\nmessages = packer.pack()  # Returns List[Dict] directly\n</code></pre>"},{"location":"modules/overview/#module-relationships","title":"Module Relationships","text":"<pre><code>graph LR\n    A[Raw Input] --&gt; B[Cleaner]\n    B --&gt; C[Compressor]\n    C --&gt; D[Scrubber]\n    D --&gt; E[Analyzer]\n    E --&gt; F[Optimized Output]\n\n    G[Multiple Items] --&gt; H[Packer]\n    H --&gt; I[Packed Context]\n</code></pre>"},{"location":"modules/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Clean before compressing, compress before redacting</li> <li>Use Packer for RAG: When managing multiple documents with priorities</li> <li>Test your pipeline: Different inputs may need different operations</li> <li>Measure impact: Use CountTokens to track savings</li> <li>Start simple: Begin with one module and add more as needed</li> </ol>"},{"location":"modules/packer/","title":"Packer Module","text":"<p>Intelligently manage context budgets with priority-based item packing for RAG applications and context window management.</p>"},{"location":"modules/packer/#overview-v013","title":"Overview (v0.1.3+)","text":"<p>The Packer module provides two specialized packers following the Single Responsibility Principle:</p> <ul> <li><code>MessagesPacker</code>: For chat completion APIs (OpenAI, Anthropic)</li> <li><code>TextPacker</code>: For text completion APIs (Llama Base, GPT-3)</li> </ul>"},{"location":"modules/packer/#messagespacker","title":"MessagesPacker","text":"<p>Pack items into chat message format for chat completion APIs.</p>"},{"location":"modules/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER, PRIORITY_HIGH\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=1000)\n\n# Add items with priorities and roles\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\npacker.add(\n    \"Product documentation: Feature A, B, C...\",\n    role=\"system\",\n    priority=PRIORITY_HIGH\n)\n\n# Pack into messages format\nmessages = packer.pack()  # Returns List[Dict[str, str]]\n\n# Use directly with chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"modules/packer/#rag-conversation-history-example","title":"RAG + Conversation History Example","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_LOW,\n    StripHTML\n)\n\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on the provided context.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with JIT cleaning\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=\"system\",\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n\n# Old conversation history (can be dropped if needed)\nold_messages = [\n    {\"role\": \"user\", \"content\": \"What is this library?\"},\n    {\"role\": \"assistant\", \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages, priority=PRIORITY_LOW)\n\n# Current query (must include)\npacker.add(\n    \"How does it reduce costs?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into messages\nmessages = packer.pack()  # List[Dict[str, str]]\n</code></pre>"},{"location":"modules/packer/#textpacker","title":"TextPacker","text":"<p>Pack items into formatted text for text completion APIs (base models).</p>"},{"location":"modules/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, PRIORITY_SYSTEM, PRIORITY_HIGH, PRIORITY_USER\n\n# Create packer with MARKDOWN format\npacker = TextPacker(\n    max_tokens=1000,\n    text_format=TextFormat.MARKDOWN\n)\n\n# Add items\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"Product documentation...\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into formatted text\nprompt = packer.pack()  # Returns str\n\n# Use with completion APIs\n# response = client.completions.create(prompt=prompt)\n</code></pre>"},{"location":"modules/packer/#text-formats","title":"Text Formats","text":"<p>RAW Format (default): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.RAW)\n# Output: Simple concatenation with separators\n</code></pre></p> <p>MARKDOWN Format (recommended for base models): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Document 1\n# - Document 2\n#\n# ### CONVERSATION:\n# User: Hello\n# Assistant: Hi\n#\n# ### INPUT:\n# Final query\n</code></pre></p> <p>XML Format (Anthropic best practice): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.XML)\n# Output: &lt;role&gt;content&lt;/role&gt; tags\n</code></pre></p>"},{"location":"modules/packer/#rag-example-with-grouped-sections","title":"RAG Example with Grouped Sections","text":"<pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_MEDIUM,\n    PRIORITY_USER,\n    StripHTML\n)\n\npacker = TextPacker(max_tokens=500, text_format=TextFormat.MARKDOWN)\n\n# System prompt\npacker.add(\n    \"Answer based on context.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents (no role = context)\npacker.add(\n    \"&lt;p&gt;Document 1...&lt;/p&gt;\",\n    priority=PRIORITY_HIGH,\n    refine_with=StripHTML()\n)\n\npacker.add(\n    \"Document 2...\",\n    priority=PRIORITY_MEDIUM\n)\n\n# User query\npacker.add(\n    \"What is the answer?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nprompt = packer.pack()  # str\n</code></pre>"},{"location":"modules/packer/#priority-constants","title":"Priority Constants","text":"<pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_USER,     # 10 - Critical user input\n    PRIORITY_HIGH,     # 20 - Important context (core RAG docs)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG docs)\n    PRIORITY_LOW,      # 40 - Optional content (old history)\n)\n</code></pre>"},{"location":"modules/packer/#common-features","title":"Common Features","text":""},{"location":"modules/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Apply operations before adding items:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\npacker.add(\n    \"&lt;div&gt;  Messy   HTML  &lt;/div&gt;\",\n    priority=PRIORITY_HIGH,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"modules/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>messages = (\n    MessagesPacker(max_tokens=500)\n    .add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\n    .add(\"User query\", role=\"user\", priority=PRIORITY_USER)\n    .pack()\n)\n</code></pre>"},{"location":"modules/packer/#inspection","title":"Inspection","text":"<pre><code>packer = MessagesPacker(max_tokens=1000)\npacker.add(\"Item 1\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Item 2\", role=\"user\", priority=PRIORITY_USER)\n\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Tokens: {item['tokens']}\")\n</code></pre>"},{"location":"modules/packer/#reset","title":"Reset","text":"<pre><code>packer = MessagesPacker(max_tokens=1000)\npacker.add(\"First batch\", role=\"user\", priority=PRIORITY_HIGH)\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=\"user\", priority=PRIORITY_HIGH)\nmessages2 = packer.pack()\n</code></pre>"},{"location":"modules/packer/#how-it-works","title":"How It Works","text":"<ol> <li>Add items with priorities, roles, and optional JIT refinement</li> <li>Sort by priority (lower number = higher priority)</li> <li>Greedy packing - select items that fit within budget</li> <li>Restore insertion order for natural reading flow</li> <li>Format output:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code></li> <li>TextPacker: Returns <code>str</code> (formatted based on text_format)</li> </ol>"},{"location":"modules/packer/#token-overhead-optimization","title":"Token Overhead Optimization","text":""},{"location":"modules/packer/#messagespacker_1","title":"MessagesPacker","text":"<ul> <li>Pre-calculates ChatML format overhead (~4 tokens per message)</li> <li>100% token budget utilization in precise mode</li> </ul>"},{"location":"modules/packer/#textpacker-markdown","title":"TextPacker (MARKDOWN)","text":"<ul> <li>\"Entrance fee\" strategy: Pre-reserves 30 tokens for section headers</li> <li>Marginal costs: Only counts bullet points and newlines per item</li> <li>Result: Fits more documents compared to per-item header calculation</li> </ul>"},{"location":"modules/packer/#use-cases","title":"Use Cases","text":"<ul> <li>RAG Applications: Pack retrieved documents into context budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"modules/packer/#new-in-v013","title":"New in v0.1.3","text":"<p>The Packer module now provides two specialized packers:</p> <pre><code>from prompt_refiner import MessagesPacker, TextPacker\n\n# For chat APIs (OpenAI, Anthropic)\nmessages_packer = MessagesPacker(max_tokens=1000)\nmessages = messages_packer.pack()  # List[Dict[str, str]]\n\n# For completion APIs (Llama Base, GPT-3)\ntext_packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\ntext = text_packer.pack()  # str\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/scrubber/","title":"Scrubber Module","text":"<p>Protect sensitive information with automatic PII redaction.</p>"},{"location":"modules/scrubber/#redactpii-operation","title":"RedactPII Operation","text":"<p>Automatically redact personally identifiable information using regex patterns.</p>"},{"location":"modules/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code> - Email addresses</li> <li><code>phone</code> - Phone numbers</li> <li><code>ip</code> - IP addresses</li> <li><code>credit_card</code> - Credit card numbers</li> <li><code>ssn</code> - Social Security Numbers</li> <li><code>url</code> - URLs</li> </ul>"},{"location":"modules/scrubber/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact john@example.com or 555-123-4567\")\n# Output: \"Contact [EMAIL] or [PHONE]\"\n\n# Redact specific types\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\n</code></pre>"},{"location":"modules/scrubber/#custom-patterns","title":"Custom Patterns","text":"<pre><code>redactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"user-guide/custom-operations/","title":"Custom Operations","text":"<p>Create your own operations to extend Prompt Refiner.</p>"},{"location":"user-guide/custom-operations/#creating-a-custom-operation","title":"Creating a Custom Operation","text":"<p>All operations inherit from the <code>Operation</code> base class and implement the <code>process</code> method:</p> <pre><code>from prompt_refiner import Operation\n\nclass RemoveEmojis(Operation):\n    \"\"\"Remove emoji characters from text.\"\"\"\n\n    def process(self, text: str) -&gt; str:\n        import re\n        # Simple emoji removal pattern\n        emoji_pattern = re.compile(\n            \"[\"\n            \"\\U0001F600-\\U0001F64F\"  # emoticons\n            \"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n            \"]+\", flags=re.UNICODE\n        )\n        return emoji_pattern.sub(\"\", text)\n</code></pre>"},{"location":"user-guide/custom-operations/#using-your-custom-operation","title":"Using Your Custom Operation","text":"<p>Use it like any built-in operation:</p> <pre><code>from prompt_refiner import Refiner, NormalizeWhitespace\n\npipeline = (\n    Refiner()\n    .pipe(RemoveEmojis())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = pipeline.run(\"Hello \ud83d\ude00 World \ud83c\udf0d!\")\n# Output: \"Hello World !\"\n</code></pre>"},{"location":"user-guide/custom-operations/#more-examples","title":"More Examples","text":""},{"location":"user-guide/custom-operations/#remove-urls","title":"Remove URLs","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveURLs(Operation):\n    def process(self, text: str) -&gt; str:\n        url_pattern = r'https?://\\S+|www\\.\\S+'\n        return re.sub(url_pattern, '[URL]', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#lowercase-text","title":"Lowercase Text","text":"<pre><code>from prompt_refiner import Operation\n\nclass Lowercase(Operation):\n    def process(self, text: str) -&gt; str:\n        return text.lower()\n</code></pre>"},{"location":"user-guide/custom-operations/#remove-numbers","title":"Remove Numbers","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveNumbers(Operation):\n    def process(self, text: str) -&gt; str:\n        return re.sub(r'\\d+', '', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#guidelines","title":"Guidelines","text":"<ol> <li>Single responsibility - Each operation should do one thing well</li> <li>Immutable - Don't modify the input, return a new string</li> <li>Deterministic - Same input should always produce same output</li> <li>Document - Add docstrings explaining what it does</li> </ol>"},{"location":"user-guide/custom-operations/#contributing","title":"Contributing","text":"<p>Have a useful operation? Consider contributing it to Prompt Refiner!</p> <p>See contributing guide \u2192</p>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Learn how to use Prompt Refiner effectively to optimize your LLM inputs.</p>"},{"location":"user-guide/overview/#what-is-prompt-refiner","title":"What is Prompt Refiner?","text":"<p>Prompt Refiner is a library for cleaning and optimizing text before sending it to LLM APIs. It helps you:</p> <ul> <li>Save money by reducing token usage</li> <li>Improve quality by cleaning and normalizing text</li> <li>Enhance security by redacting PII</li> <li>Track value by measuring optimization impact</li> </ul>"},{"location":"user-guide/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/overview/#operations","title":"Operations","text":"<p>An Operation is a single transformation that processes text:</p> <pre><code>from prompt_refiner import StripHTML\n\noperation = StripHTML()\nresult = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n# Output: \"Hello\"\n</code></pre> <p>All operations implement the same interface: <code>process(text: str) -&gt; str</code></p>"},{"location":"user-guide/overview/#pipelines","title":"Pipelines","text":"<p>A Pipeline chains multiple operations together:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Using the pipe operator (recommended)\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\nresult = pipeline.run(\"&lt;p&gt;Hello    World&lt;/p&gt;\")\n# Output: \"Hello World\"\n</code></pre> <p>Alternatively, use the fluent API: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"user-guide/overview/#the-4-modules","title":"The 4 Modules","text":"<ul> <li>Cleaner - Clean dirty data</li> <li>Compressor - Reduce size</li> <li>Scrubber - Security &amp; privacy</li> <li>Analyzer - Track metrics</li> </ul>"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about pipelines</li> <li>Create custom operations</li> <li>Browse examples</li> </ul>"},{"location":"user-guide/pipelines/","title":"Pipeline Basics","text":"<p>Learn how to build effective pipelines with Prompt Refiner.</p>"},{"location":"user-guide/pipelines/#two-ways-to-build-pipelines","title":"Two Ways to Build Pipelines","text":"<p>Prompt Refiner supports two syntax options for building pipelines:</p>"},{"location":"user-guide/pipelines/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<p>The pipe operator (<code>|</code>) provides a clean, Pythonic syntax similar to LangChain:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More concise - no need to import or instantiate <code>Refiner()</code> - Familiar to LangChain, LangGraph, and modern Python framework users - Cleaner visual appearance</p>"},{"location":"user-guide/pipelines/#fluent-api","title":"Fluent API","text":"<p>The fluent API uses method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(TruncateTokens(max_tokens=1000))\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More explicit - clear that you're creating a Refiner pipeline - Traditional method chaining pattern</p> <p>Choose One Style</p> <p>Pick one syntax style per project and use it consistently. Both work identically under the hood. Don't mix styles in the same pipeline.</p>"},{"location":"user-guide/pipelines/#the-pipeline-pattern","title":"The Pipeline Pattern","text":"<p>A pipeline chains operations in sequence:</p> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre> <p>All operations process the text in order, with each operation's output becoming the next operation's input.</p>"},{"location":"user-guide/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<ol> <li>Text enters the pipeline</li> <li>Each operation processes it in order</li> <li>Output of one operation becomes input of the next</li> <li>Final result is returned</li> </ol> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre>"},{"location":"user-guide/pipelines/#order-matters","title":"Order Matters","text":"<p>Operations run in the order you add them:</p> <pre><code># \u2705 Correct: Clean HTML first, then normalize\npipeline = StripHTML() | NormalizeWhitespace()\n\n# \u274c Wrong order - normalizes first, HTML remains\npipeline = NormalizeWhitespace() | StripHTML()\n</code></pre>"},{"location":"user-guide/pipelines/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/pipelines/#1-clean-before-compressing","title":"1. Clean Before Compressing","text":"<pre><code>pipeline = (\n    StripHTML()                  # Clean first\n    | NormalizeWhitespace()\n    | TruncateTokens()           # Then compress\n)\n</code></pre>"},{"location":"user-guide/pipelines/#2-compress-before-redacting","title":"2. Compress Before Redacting","text":"<pre><code>pipeline = (\n    TruncateTokens()  # Compress first\n    | RedactPII()     # Then redact\n)\n</code></pre>"},{"location":"user-guide/pipelines/#3-analyze-last","title":"3. Analyze Last","text":"<pre><code>counter = CountTokens(original_text=text)\npipeline = (\n    StripHTML()\n    | TruncateTokens()\n    | counter  # Analyze at the end\n)\n</code></pre>"},{"location":"user-guide/pipelines/#multiple-pipelines","title":"Multiple Pipelines","text":"<p>Create different pipelines for different use cases:</p> <pre><code># Pipeline for web content\nweb_pipeline = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n\n# Pipeline for RAG\nrag_pipeline = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n\n# Pipeline for secure processing\nsecure_pipeline = RedactPII()\n\n# Use them\ncleaned_web = web_pipeline.run(html_content)\noptimized_rag = rag_pipeline.run(rag_context)\nsafe_text = secure_pipeline.run(user_input)\n</code></pre> <p>Learn about custom operations \u2192</p>"}]}