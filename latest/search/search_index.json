{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prompt Refiner","text":"<p>A lightweight Python library for building production LLM applications. Save 10-20% on API costs and manage context windows intelligently.</p>"},{"location":"#overview","title":"Overview","text":"<p>Prompt Refiner solves two core problems for production LLM applications:</p> <ol> <li>Token Optimization - Clean dirty inputs (HTML, whitespace, PII) to reduce API costs by 10-20%</li> <li>Context Management - Pack system prompts, RAG docs, and chat history into token budgets with smart priority-based selection</li> </ol> <p>Perfect for RAG applications, chatbots, and any production system that needs to manage LLM context windows efficiently.</p> <p>Proven Effectiveness</p> <p>Benchmarked on 30 real-world test cases, Prompt Refiner achieves 4-15% token reduction while maintaining 96-99% quality. Aggressive optimization can save up to ~$54/month on GPT-4 at scale (1M tokens/month).</p> <p>Processing overhead is &lt; 0.5ms per 1k tokens - negligible compared to network and LLM latency.</p> <p>See benchmark results \u2192</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#option-1-preset-strategies-easiest","title":"Option 1: Preset Strategies (Easiest)","text":"<p>New in v0.1.5: Use benchmark-tested preset strategies for instant token optimization:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy, AggressiveStrategy\n\n# Minimal: 4.3% reduction, 98.7% quality\nrefiner = MinimalStrategy().create_refiner()\ncleaned = refiner.run(\"&lt;div&gt;Your HTML content&lt;/div&gt;\")\n\n# Aggressive: 15% reduction, 96.4% quality\nrefiner = AggressiveStrategy(max_tokens=150).create_refiner()\ncleaned = refiner.run(long_context)\n</code></pre> <p>Learn more about strategies \u2192</p>"},{"location":"#option-2-custom-pipelines-flexible","title":"Option 2: Custom Pipelines (Flexible)","text":"<p>Build custom cleaning pipelines with the pipe operator:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\n# Define a cleaning pipeline\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000, strategy=\"middle_out\")\n)\n\nraw_input = \"&lt;div&gt;  User input with &lt;b&gt;lots&lt;/b&gt; of   spaces... &lt;/div&gt;\"\nclean_prompt = pipeline.run(raw_input)\n# Output: \"User input with lots of spaces...\"\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"#5-core-modules","title":"5 Core Modules","text":"<p>Prompt Refiner is organized into 5 specialized modules:</p>"},{"location":"#text-processing-operations","title":"Text Processing Operations","text":""},{"location":"#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<ul> <li>StripHTML() - Remove HTML tags, convert to Markdown</li> <li>NormalizeWhitespace() - Collapse excessive whitespace</li> <li>FixUnicode() - Remove zero-width spaces and problematic Unicode</li> <li>JsonCleaner() - Strip nulls/empties from JSON, minify</li> </ul> <p>Learn more about Cleaner \u2192</p>"},{"location":"#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<ul> <li>TruncateTokens() - Smart truncation with sentence boundaries<ul> <li>Strategies: <code>\"head\"</code>, <code>\"tail\"</code>, <code>\"middle_out\"</code></li> </ul> </li> <li>Deduplicate() - Remove similar content (great for RAG)</li> </ul> <p>Learn more about Compressor \u2192</p>"},{"location":"#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<ul> <li>RedactPII() - Automatically redact emails, phones, IPs, credit cards, URLs, SSNs</li> </ul> <p>Learn more about Scrubber \u2192</p>"},{"location":"#context-budget-management","title":"Context Budget Management","text":""},{"location":"#4-packer-intelligent-context-packing-v013","title":"4. Packer - Intelligent Context Packing (v0.1.3+)","text":"<p>For RAG applications and chatbots, the Packer module manages context budgets with priority-based selection:</p> <ul> <li>MessagesPacker() - For chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict]</code></li> <li>TextPacker() - For text completion APIs (Llama Base, GPT-3). Returns <code>str</code></li> </ul> <p>Key Features: - Smart priority-based selection (auto-prioritizes: system &gt; query &gt; context &gt; history) - JIT refinement with <code>refine_with</code> parameter - Automatic format overhead calculation - Semantic roles for clear intent</p> <pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY, StripHTML\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"You are helpful.\", role=ROLE_SYSTEM)\n\n# Clean RAG documents on-the-fly\npacker.add(\n    \"&lt;div&gt;RAG doc...&lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n\npacker.add(\"User question?\", role=ROLE_QUERY)\n\nmessages = packer.pack()  # Returns List[Dict] ready for chat APIs\n</code></pre> <p>Learn more about Packer \u2192</p>"},{"location":"#5-strategy-preset-strategies-v015","title":"5. Strategy - Preset Strategies (v0.1.5+)","text":"<p>For quick setup, use benchmark-tested preset strategies:</p> <ul> <li>MinimalStrategy - 4.3% reduction, 98.7% quality (HTML + Whitespace)</li> <li>StandardStrategy - 4.8% reduction, 98.4% quality (+ Deduplication)</li> <li>AggressiveStrategy - 15% reduction, 96.4% quality (+ Truncation)</li> </ul> <pre><code>from prompt_refiner.strategy import StandardStrategy\n\n# Quick setup with preset\nrefiner = StandardStrategy().create_refiner()\ncleaned = refiner.run(\"&lt;div&gt;Your HTML content&lt;/div&gt;\")\n\n# Extend with additional operations\nrefiner.pipe(RedactPII(redact_types={\"email\"}))\n</code></pre> <p>Learn more about Strategy \u2192</p>"},{"location":"#measurement-analysis","title":"Measurement &amp; Analysis","text":"<p>Track optimization impact without transforming prompts:</p> <ul> <li>CountTokens() - Calculate token savings and ROI</li> <li>Estimation mode (default): Character-based approximation</li> <li>Precise mode (with tiktoken): Exact token counts</li> </ul> <p>Learn more about Analyzer \u2192</p>"},{"location":"#complete-example","title":"Complete Example","text":"<pre><code>from prompt_refiner import (\n    # Core Modules\n    StripHTML, NormalizeWhitespace, FixUnicode, JsonCleaner,  # Cleaner\n    Deduplicate, TruncateTokens,  # Compressor\n    RedactPII,  # Scrubber\n    # Measurement\n    CountTokens\n)\n\noriginal_text = \"\"\"Your messy input here...\"\"\"\n\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    # Clean\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n    | FixUnicode()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\"})\n    # Analyze\n    | counter\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())  # Shows token savings\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Get Started</p> <p>Install Prompt Refiner and build your first pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting Started</p> </li> <li> <p>API Reference</p> <p>Complete API documentation for all operations and modules</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>Examples</p> <p>Browse practical examples for each module</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>Contributing</p> <p>Learn how to contribute to the project</p> <p>:octicons-arrow-right-24: Contributing Guide</p> </li> </ul>"},{"location":"benchmark/","title":"Benchmark Results","text":"<p>Prompt Refiner's effectiveness has been validated through comprehensive testing covering both quality &amp; cost savings and performance &amp; latency.</p>"},{"location":"benchmark/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"benchmark/#quality-cost-benchmark","title":"\ud83c\udfaf Quality &amp; Cost Benchmark","text":"<p>Comprehensive A/B testing on 30 real-world test cases measuring token reduction and response quality.</p> <p>Jump to Quality Benchmark \u2192</p>"},{"location":"benchmark/#latency-benchmark","title":"\u26a1 Latency Benchmark","text":"<p>Performance testing measuring processing overhead of refining operations.</p> <p>Jump to Latency Benchmark \u2192</p>"},{"location":"benchmark/#quality-cost-results","title":"Quality &amp; Cost Results","text":"<p>The benchmark measures two critical factors:</p> <ul> <li>Token Reduction - How much we can reduce prompt size (cost savings)</li> <li>Response Quality - Whether responses remain semantically equivalent</li> </ul> <p>Quality is evaluated using two methods: 1. Cosine Similarity - Semantic similarity of response embeddings (0-1 scale) 2. LLM Judge - GPT-4 evaluation of response equivalence</p>"},{"location":"benchmark/#results-summary","title":"Results Summary","text":"<p>We tested 3 optimization strategies on 30 test cases (15 SQuAD Q&amp;A pairs + 15 RAG scenarios):</p> Strategy Token Reduction Quality (Cosine) Judge Approval Overall Equivalent Minimal 4.3% 0.987 86.7% 86.7% Standard 4.8% 0.984 90.0% 86.7% Aggressive 15.0% 0.964 80.0% 66.7%"},{"location":"benchmark/#strategy-definitions","title":"Strategy Definitions","text":"<p>Minimal (Conservative cleaning): <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p> <p>Standard (Recommended for most use cases): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p> <p>Aggressive (Maximum savings): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150, strategy=\"head\")\n)\n</code></pre></p>"},{"location":"benchmark/#key-findings","title":"Key Findings","text":""},{"location":"benchmark/#standard-strategy-best-balance","title":"\ud83c\udfaf Standard Strategy: Best Balance","text":"<p>The Standard strategy offers the best balance: - 4.8% token reduction with minimal quality impact - 90% judge approval - highest among all strategies - 0.984 cosine similarity - nearly perfect semantic preservation</p>"},{"location":"benchmark/#cost-savings","title":"\ud83d\udcb0 Cost Savings","text":"<p>Real-world cost savings for production applications:</p> GPT-4 TurboGPT-4 <p>Input cost: $0.01 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $4.30 $4.80 $15.00 1M tokens/month $43 $48 $150 10M tokens/month $430 $480 $1,500 <p>Input cost: $0.03 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $13 $14 $45 1M tokens/month $129 $144 $450 10M tokens/month $1,290 $1,440 $4,500"},{"location":"benchmark/#performance-by-scenario","title":"\ud83d\udcca Performance by Scenario","text":"<p>RAG Scenarios (with duplicates and HTML): - Minimal: 17% reduction on average - Standard: 31% reduction on average - Aggressive: 49% reduction on complex documents</p> <p>SQuAD Q&amp;A (clean academic text): - All strategies: 2-5% reduction (less messy data = less to clean)</p> <p>Key Insight</p> <p>Token savings scale with input messiness. RAG contexts with HTML, duplicates, and whitespace see 3-10x more reduction than clean text.</p>"},{"location":"benchmark/#visualizations","title":"Visualizations","text":""},{"location":"benchmark/#token-reduction-vs-quality","title":"Token Reduction vs Quality","text":"<p>The scatter plot shows each strategy's position in the cost-quality tradeoff space. Standard strategy achieves near-optimal quality while maintaining solid savings.</p>"},{"location":"benchmark/#test-dataset","title":"Test Dataset","text":"<p>The benchmark uses 30 carefully curated test cases:</p>"},{"location":"benchmark/#squad-samples-15-cases","title":"SQuAD Samples (15 cases)","text":"<p>Question-answer pairs with context covering: - History (\"When did Beyonce start becoming popular?\") - Science (\"What is DNA?\") - Geography, literature, technology</p>"},{"location":"benchmark/#rag-scenarios-15-cases","title":"RAG Scenarios (15 cases)","text":"<p>Realistic retrieval-augmented generation use cases: - E-commerce product catalogs with HTML - Documentation with excessive whitespace - Customer support tickets with duplicates - Code search results - Recipe collections</p>"},{"location":"benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>Want to validate these results yourself?</p>"},{"location":"benchmark/#prerequisites","title":"Prerequisites","text":"<pre><code># Install benchmark dependencies\nuv pip install -e \".[benchmark]\"\n\n# Set up OpenAI API key\ncd benchmark/custom\ncp .env.example .env\n# Edit .env and add your OPENAI_API_KEY\n</code></pre>"},{"location":"benchmark/#run-the-benchmark","title":"Run the Benchmark","text":"<pre><code>cd benchmark/custom\npython benchmark.py\n</code></pre> <p>This will: 1. Test 30 cases with 3 strategies (90 total comparisons) 2. Generate detailed report with visualizations 3. Save results to <code>./results/</code> directory</p> <p>Estimated cost: ~$2-5 per full run (using gpt-4o-mini)</p>"},{"location":"benchmark/#advanced-options","title":"Advanced Options","text":"<pre><code># Use a different model\npython benchmark.py --model gpt-4o\n\n# Test specific strategies only\npython benchmark.py --strategies minimal standard\n\n# Use fewer test cases (faster, cheaper)\npython benchmark.py --limit 10\n</code></pre>"},{"location":"benchmark/#recommendations","title":"Recommendations","text":"<p>Based on benchmark results:</p>"},{"location":"benchmark/#for-production-rag-applications","title":"For Production RAG Applications","text":"<p>Use Standard strategy - Best balance of savings and quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p>"},{"location":"benchmark/#for-high-volume-cost-sensitive-applications","title":"For High-Volume, Cost-Sensitive Applications","text":"<p>Consider Aggressive strategy if 15% cost reduction outweighs slightly lower quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150)\n)\n</code></pre></p>"},{"location":"benchmark/#for-quality-critical-applications","title":"For Quality-Critical Applications","text":"<p>Use Minimal strategy for maximum quality preservation <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p>"},{"location":"benchmark/#latency-performance","title":"Latency &amp; Performance","text":"<p>\"What's the latency overhead?\" - Negligible. Prompt Refiner adds &lt; 0.5ms per 1k tokens of overhead.</p>"},{"location":"benchmark/#performance-results","title":"Performance Results","text":"Strategy @ 1k tokens @ 10k tokens @ 50k tokens Overhead per 1k tokens Minimal (HTML + Whitespace) 0.05ms 0.48ms 2.39ms 0.05ms Standard (+ Deduplicate) 0.26ms 2.47ms 12.27ms 0.25ms Aggressive (+ Truncate) 0.26ms 2.46ms 12.38ms 0.25ms"},{"location":"benchmark/#key-performance-insights","title":"Key Performance Insights","text":"<ul> <li>\u26a1 Minimal strategy: Only 0.05ms per 1k tokens (faster than a network packet)</li> <li>\ud83c\udfaf Standard strategy: 0.25ms per 1k tokens - adds ~2.5ms to a 10k token prompt</li> <li>\ud83d\udcca Context: Network + LLM TTFT is typically 600ms+, refining adds &lt; 0.5% overhead</li> <li>\ud83d\ude80 Individual operations (HTML, whitespace) are &lt; 0.5ms per 1k tokens</li> </ul>"},{"location":"benchmark/#real-world-impact","title":"Real-World Impact","text":"<pre><code>10k token RAG context refining: ~2.5ms overhead\nNetwork latency: ~100ms\nLLM Processing (TTFT): ~500ms+\nTotal overhead: &lt; 0.5% of request time\n</code></pre> <p>Performance Takeaway</p> <p>Refining overhead is negligible compared to network + LLM latency (600ms+). Standard refining adds ~2.5ms overhead - less than 0.5% of total request time.</p>"},{"location":"benchmark/#running-the-latency-benchmark","title":"Running the Latency Benchmark","text":"<p>The latency benchmark requires no API keys and runs completely offline:</p> <pre><code>cd benchmark/latency\npython benchmark.py\n</code></pre> <p>This will: 1. Test individual operations at multiple scales (1k, 10k, 50k tokens) 2. Test complete strategies (Minimal, Standard, Aggressive) 3. Report average, median, and P95 latency metrics 4. Show per-1k-token normalized overhead</p> <p>Cost: $0 (runs locally, no API calls)</p> <p>Duration: ~30-60 seconds</p>"},{"location":"benchmark/#learn-more","title":"Learn More","text":"<ul> <li>View Quality Benchmark Documentation</li> <li>View Latency Benchmark Documentation</li> <li>Browse Test Cases</li> <li>Examine Raw Results</li> </ul>"},{"location":"benchmark/#contributing","title":"Contributing","text":"<p>Have ideas to improve the benchmark? We welcome: - New test cases (especially domain-specific scenarios) - Additional evaluation metrics - Alternative refining strategies - Multi-model comparisons</p> <p>Open an issue or submit a PR!</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Prompt Refiner!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>uv package manager</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run tests\nmake test\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>prompt-refiner/\n\u251c\u2500\u2500 src/prompt_refiner/     # Source code\n\u2502   \u251c\u2500\u2500 cleaner/           # Cleaner module\n\u2502   \u251c\u2500\u2500 compressor/        # Compressor module\n\u2502   \u251c\u2500\u2500 scrubber/          # Scrubber module\n\u2502   \u2514\u2500\u2500 analyzer/          # Analyzer module\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 examples/              # Example scripts\n\u2514\u2500\u2500 docs/                  # Documentation\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following existing patterns</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_cleaner.py -v\n</code></pre>"},{"location":"contributing/#4-format-code","title":"4. Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"contributing/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write clear docstrings (Google style)</li> <li>Keep functions small and focused</li> </ul>"},{"location":"contributing/#example","title":"Example","text":"<pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text\n    \"\"\"\n    return text.strip()\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Aim for high test coverage</li> <li>Test edge cases</li> </ul> <pre><code>def test_strip_html():\n    operation = StripHTML()\n    result = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n    assert result == \"Hello\"\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update relevant documentation files</li> <li>Add examples for new features</li> <li>Keep API reference up to date (auto-generated from docstrings)</li> </ul>"},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code>make docs-serve\n</code></pre> <p>Then visit http://127.0.0.1:8000</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with Prompt Refiner in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"Default (Lightweight)With Precise Token Counting <p>Zero dependencies - perfect for most use cases:</p> <pre><code>pip install llm-prompt-refiner\n</code></pre> <p>Install with optional <code>tiktoken</code> for precise token counting:</p> <pre><code>pip install llm-prompt-refiner[token]\n</code></pre> <p>Then opt-in by passing a <code>model</code> parameter:</p> <pre><code>from prompt_refiner import CountTokens, MessagesPacker\n\ncounter = CountTokens(model=\"gpt-4\")\npacker = MessagesPacker(max_tokens=1000, model=\"gpt-4\")\n</code></pre>"},{"location":"getting-started/#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's create a simple pipeline to clean HTML and normalize whitespace:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Process some text\nraw_input = \"\"\"\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Welcome&lt;/h1&gt;\n        &lt;p&gt;This  has    excessive   spaces.&lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nclean_output = pipeline.run(raw_input)\nprint(clean_output)\n# Output: \"Welcome This has excessive spaces.\"\n</code></pre>"},{"location":"getting-started/#understanding-the-pipeline-pattern","title":"Understanding the Pipeline Pattern","text":"<p>Prompt Refiner uses a pipeline pattern where you chain operations together:</p> <ol> <li>Create operations - Initialize the operations you need</li> <li>Chain with <code>|</code> operator - Combine operations in order</li> <li>Run with <code>.run()</code> - Execute the pipeline on your text</li> </ol> <pre><code>pipeline = (\n    Operation1()            # 1. Create operations\n    | Operation2()          # 2. Chain with | operator\n    | Operation3()\n)\n\nresult = pipeline.run(text)  # 3. Run\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use the traditional fluent API with <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(Operation3())\n)\n</code></pre></p> <p>Order Matters</p> <p>Operations run in the order you add them. For example, you should typically clean HTML before normalizing whitespace.</p>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#pattern-1-web-content-cleaning","title":"Pattern 1: Web Content Cleaning","text":"<p>Clean content scraped from the web:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)  # Convert to Markdown\n    | FixUnicode()               # Fix Unicode issues\n    | NormalizeWhitespace()      # Normalize spaces\n)\n</code></pre>"},{"location":"getting-started/#pattern-2-rag-context-optimization","title":"Pattern 2: RAG Context Optimization","text":"<p>Optimize retrieved context for RAG applications:</p> <pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate(similarity_threshold=0.85)  # Remove duplicates\n    | TruncateTokens(max_tokens=2000)       # Fit in context window\n)\n</code></pre>"},{"location":"getting-started/#pattern-3-secure-pii-handling","title":"Pattern 3: Secure PII Handling","text":"<p>Redact sensitive information before sending to APIs:</p> <pre><code>from prompt_refiner import RedactPII\n\nsecure_pipeline = RedactPII(redact_types={\"email\", \"phone\", \"ssn\"})\n</code></pre>"},{"location":"getting-started/#pattern-4-full-optimization-with-tracking","title":"Pattern 4: Full Optimization with Tracking","text":"<p>Complete optimization with metrics:</p> <pre><code>from prompt_refiner import (\n    StripHTML, NormalizeWhitespace,\n    TruncateTokens, RedactPII, CountTokens\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\nfull_pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n    | RedactPII()\n    | counter\n)\n\nresult = full_pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"getting-started/#pattern-5-advanced-rag-with-context-budget-v013","title":"Pattern 5: Advanced - RAG with Context Budget (v0.1.3+)","text":"<p>For RAG applications, manage context budgets with smart priority-based packing:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY,\n    StripHTML, NormalizeWhitespace\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (auto-prioritized: highest)\npacker.add(\n    \"Answer based on provided context.\",\n    role=ROLE_SYSTEM\n)\n\n# RAG documents with JIT cleaning (auto-prioritized: high)\npacker.add(\n    \"&lt;div&gt;Document 1...&lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n\n# Current user query (auto-prioritized: critical)\npacker.add(\n    \"What is the answer?\",\n    role=ROLE_QUERY\n)\n\nmessages = packer.pack()  # Ready for chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"getting-started/#proven-results","title":"Proven Results","text":"<p>Curious about the real-world effectiveness? Check out our comprehensive benchmark results:</p> <p>Benchmark Highlights</p> <ul> <li>4-15% token reduction across 30 test cases</li> <li>96-99% quality preservation (cosine similarity + LLM judge)</li> <li>Real cost savings: $48-$150/month per 1M tokens</li> </ul> <p>View Full Benchmark \u2192</p>"},{"location":"getting-started/#exploring-modules","title":"Exploring Modules","text":"<p>Prompt Refiner has 5 specialized modules:</p> <ul> <li>Cleaner - Clean dirty data (HTML, whitespace, Unicode, JSON)</li> <li>Compressor - Reduce size (truncation, deduplication)</li> <li>Scrubber - Security and privacy (PII redaction)</li> <li>Packer - Context budget management for RAG and chatbots (v0.1.3+)</li> <li>Strategy - Preset strategies for quick setup (v0.1.5+)</li> <li>Analyzer - Metrics and analysis (token counting)</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn the Modules</p> <p>Deep dive into each of the 5 core modules</p> <p>:octicons-arrow-right-24: Modules Overview</p> </li> <li> <p>Browse Examples</p> <p>See practical examples for each operation</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for all Prompt Refiner classes and operations.</p> <p>This section contains auto-generated documentation from the codebase docstrings. All operations inherit from the base <code>Operation</code> class and implement a <code>process(text: str) -&gt; str</code> method.</p>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":"<ul> <li> <p>:material-pipe:{ .lg .middle } Refiner</p> <p>Pipeline builder for chaining operations</p> <p>:octicons-arrow-right-24: Refiner API</p> </li> <li> <p>:material-broom:{ .lg .middle } Cleaner</p> <p>Operations for cleaning dirty data</p> <p>:octicons-arrow-right-24: Cleaner API</p> </li> <li> <p>:material-compress:{ .lg .middle } Compressor</p> <p>Operations for reducing size</p> <p>:octicons-arrow-right-24: Compressor API</p> </li> <li> <p>:material-shield-lock:{ .lg .middle } Scrubber</p> <p>Operations for security and privacy</p> <p>:octicons-arrow-right-24: Scrubber API</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Analyzer</p> <p>Operations for metrics and analysis</p> <p>:octicons-arrow-right-24: Analyzer API</p> </li> <li> <p>:material-package-variant:{ .lg .middle } Packer</p> <p>Context budget management with priorities</p> <p>:octicons-arrow-right-24: Packer API</p> </li> </ul>"},{"location":"api-reference/#operation-base-class","title":"Operation Base Class","text":"<p>All operations in Prompt Refiner inherit from the <code>Operation</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Operation(ABC):\n    @abstractmethod\n    def process(self, text: str) -&gt; str:\n        \"\"\"Process the input text and return the result.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#usage-pattern","title":"Usage Pattern","text":"<p>Operations are used within a <code>Refiner</code> pipeline:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = refiner.run(\"Your text here...\")\n</code></pre>"},{"location":"api-reference/#module-organization","title":"Module Organization","text":"<ul> <li>Refiner - Core pipeline builder class</li> <li>Cleaner - <code>StripHTML</code>, <code>NormalizeWhitespace</code>, <code>FixUnicode</code></li> <li>Compressor - <code>TruncateTokens</code>, <code>Deduplicate</code></li> <li>Scrubber - <code>RedactPII</code></li> <li>Analyzer - <code>CountTokens</code></li> <li>Packer - <code>ContextPacker</code></li> </ul>"},{"location":"api-reference/analyzer/","title":"Analyzer Module","text":"<p>The Analyzer module provides operations for measuring optimization impact and tracking metrics.</p>"},{"location":"api-reference/analyzer/#counttokens","title":"CountTokens","text":"<p>Count tokens and provide before/after statistics to demonstrate optimization value.</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens","title":"prompt_refiner.analyzer.CountTokens","text":"<pre><code>CountTokens(original_text=None, model=None)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Count tokens and provide statistics before/after processing.</p> <p>Supports two modes: - Precise mode: Uses tiktoken if installed (pip install llm-prompt-refiner[token]) - Estimation mode: Uses character-based approximation (1 token \u2248 4 characters)</p> <p>Initialize the token counter.</p> <p>Parameters:</p> Name Type Description Default <code>original_text</code> <code>Optional[str]</code> <p>Optional original text to compare against</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name for tiktoken encoding. If None, uses character-based    estimation. If specified, attempts to use tiktoken for precise counting.</p> <code>None</code> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def __init__(self, original_text: Optional[str] = None, model: Optional[str] = None):\n    \"\"\"\n    Initialize the token counter.\n\n    Args:\n        original_text: Optional original text to compare against\n        model: Model name for tiktoken encoding. If None, uses character-based\n               estimation. If specified, attempts to use tiktoken for precise counting.\n    \"\"\"\n    self.original_text = original_text\n    self.model = model\n    self._stats: Optional[dict] = None\n    self.is_precise = False\n    self._encoding = None\n\n    # Only try tiktoken if user explicitly requests it by passing a model\n    if model is not None:\n        try:\n            import tiktoken\n\n            try:\n                self._encoding = tiktoken.encoding_for_model(model)\n            except KeyError:\n                # Fall back to cl100k_base if model not found\n                self._encoding = tiktoken.get_encoding(\"cl100k_base\")\n            self.is_precise = True\n            logger.debug(f\"Using tiktoken for precise token counting (model: {model})\")\n        except ImportError:\n            # User requested precise mode but tiktoken not installed\n            self.is_precise = False\n            logger.warning(\n                f\"Model '{model}' specified but tiktoken not installed. \"\n                \"Falling back to character-based estimation. \"\n                \"Install with: pip install llm-prompt-refiner[token]\"\n            )\n    else:\n        # User didn't specify model - use estimation mode directly\n        self.is_precise = False\n        logger.debug(\"Using character-based token estimation (model not specified)\")\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens-functions","title":"Functions","text":""},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Count tokens in the text and store statistics.</p> <p>This operation doesn't modify the text, it just analyzes it.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>The same text (unchanged)</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Count tokens in the text and store statistics.\n\n    This operation doesn't modify the text, it just analyzes it.\n\n    Args:\n        text: The input text\n\n    Returns:\n        The same text (unchanged)\n    \"\"\"\n    current_tokens = self._estimate_tokens(text)\n\n    if self.original_text is not None:\n        original_tokens = self._estimate_tokens(self.original_text)\n        saved_tokens = original_tokens - current_tokens\n        saving_percent = (saved_tokens / original_tokens * 100) if original_tokens &gt; 0 else 0\n\n        self._stats = {\n            \"original\": original_tokens,\n            \"cleaned\": current_tokens,\n            \"saved\": saved_tokens,\n            \"saving_percent\": f\"{saving_percent:.1f}%\",\n        }\n    else:\n        self._stats = {\n            \"tokens\": current_tokens,\n        }\n\n    return text\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.get_stats","title":"get_stats","text":"<pre><code>get_stats()\n</code></pre> <p>Get token statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing token statistics</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get token statistics.\n\n    Returns:\n        Dictionary containing token statistics\n    \"\"\"\n    return self._stats or {}\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.format_stats","title":"format_stats","text":"<pre><code>format_stats()\n</code></pre> <p>Format statistics as a human-readable string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted statistics string</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def format_stats(self) -&gt; str:\n    \"\"\"\n    Format statistics as a human-readable string.\n\n    Returns:\n        Formatted statistics string\n    \"\"\"\n    if not self._stats:\n        return \"No statistics available\"\n\n    if \"original\" in self._stats:\n        return (\n            f\"Original: {self._stats['original']} tokens\\n\"\n            f\"Cleaned: {self._stats['cleaned']} tokens\\n\"\n            f\"Saved: {self._stats['saved']} tokens ({self._stats['saving_percent']})\"\n        )\n    else:\n        return f\"Tokens: {self._stats['tokens']}\"\n</code></pre>"},{"location":"api-reference/analyzer/#token-counting-modes","title":"Token Counting Modes","text":"<p>Two Counting Modes</p> <p>CountTokens supports two modes:</p> <p>Estimation Mode (Default) - Zero dependencies, uses character-based approximation: ~1 token \u2248 4 characters - Fast and lightweight, good for most use cases - Applies 10% safety buffer in ContextPacker to prevent overflow</p> <pre><code>counter = CountTokens()  # Estimation mode\n</code></pre> <p>Precise Mode (Optional) - Requires <code>tiktoken</code>: <code>pip install llm-prompt-refiner[token]</code> - Exact token counting using OpenAI's tokenizer - No safety buffer needed, 100% capacity utilization - Opt-in by passing a <code>model</code> parameter</p> <pre><code>counter = CountTokens(model=\"gpt-4\")  # Precise mode\n</code></pre>"},{"location":"api-reference/analyzer/#examples","title":"Examples","text":""},{"location":"api-reference/analyzer/#basic-token-counting","title":"Basic Token Counting","text":"<pre><code>from prompt_refiner import CountTokens\n\ncounter = CountTokens()\ncounter.process(\"Hello World\")\n\nstats = counter.get_stats()\nprint(stats)\n# {'tokens': 2}\n</code></pre>"},{"location":"api-reference/analyzer/#beforeafter-comparison","title":"Before/After Comparison","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\n# Build pipeline with counter at the end\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\n\n# Get statistics\nstats = counter.get_stats()\nprint(stats)\n# {\n#   'original': 6,\n#   'cleaned': 2,\n#   'saved': 4,\n#   'saving_percent': '66.7%'\n# }\n\n# Formatted output\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"api-reference/analyzer/#cost-calculation-example","title":"Cost Calculation Example","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"\"\"Your long text here...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nstats = counter.get_stats()\n\n# Calculate cost savings\n# Example: GPT-4 pricing - $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings = original_cost - cleaned_cost\n\nprint(f\"Original cost: ${original_cost:.4f}\")\nprint(f\"Cleaned cost: ${cleaned_cost:.4f}\")\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre>"},{"location":"api-reference/analyzer/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/analyzer/#roi-demonstration","title":"ROI Demonstration","text":"<pre><code>from prompt_refiner import (\n    Refiner, StripHTML, NormalizeWhitespace,\n    Deduplicate, TruncateTokens, CountTokens\n)\n\noriginal_text = \"\"\"Your messy input...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nfull_optimization = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=1000))\n    .pipe(counter)\n)\n\nresult = full_optimization.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#ab-testing-different-strategies","title":"A/B Testing Different Strategies","text":"<pre><code>from prompt_refiner import Refiner, TruncateTokens, Deduplicate, CountTokens\n\noriginal_text = \"\"\"Your text...\"\"\"\n\n# Strategy A: Just truncate\ncounter_a = CountTokens(original_text=original_text)\nstrategy_a = (\n    Refiner()\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_a)\n)\nstrategy_a.run(original_text)\n\n# Strategy B: Deduplicate then truncate\ncounter_b = CountTokens(original_text=original_text)\nstrategy_b = (\n    Refiner()\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_b)\n)\nstrategy_b.run(original_text)\n\nprint(\"Strategy A:\", counter_a.format_stats())\nprint(\"Strategy B:\", counter_b.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nfrom prompt_refiner import Refiner, StripHTML, CountTokens\n\nlogger = logging.getLogger(__name__)\n\ndef process_user_input(text):\n    counter = CountTokens(original_text=text)\n\n    refiner = (\n        Refiner()\n        .pipe(StripHTML())\n        .pipe(counter)\n    )\n\n    result = refiner.run(text)\n    stats = counter.get_stats()\n\n    # Log optimization impact\n    logger.info(\n        f\"Processed input: \"\n        f\"original={stats['original']} tokens, \"\n        f\"cleaned={stats['cleaned']} tokens, \"\n        f\"saved={stats['saved']} tokens ({stats['saving_percent']})\"\n    )\n\n    return result\n</code></pre>"},{"location":"api-reference/analyzer/#tips","title":"Tips","text":"<p>Always Use with Original Text</p> <p>To see before/after comparisons, always initialize <code>CountTokens</code> with the original text:</p> <pre><code>counter = CountTokens(original_text=original_text)\n</code></pre> <p>Otherwise, you'll only get the final token count.</p> <p>Place at End of Pipeline</p> <p>For accurate \"after\" measurements, place <code>CountTokens</code> as the last operation in your pipeline:</p> <pre><code>refiner = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(CountTokens(original_text=text))  # Last!\n)\n</code></pre>"},{"location":"api-reference/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data, including HTML removal, whitespace normalization, Unicode fixing, and JSON compression.</p>"},{"location":"api-reference/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML","title":"prompt_refiner.cleaner.StripHTML","text":"<pre><code>StripHTML(preserve_tags=None, to_markdown=False)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p> <p>Initialize the HTML stripper.</p> <p>Parameters:</p> Name Type Description Default <code>preserve_tags</code> <code>Optional[Set[str]]</code> <p>Set of tag names to preserve (e.g., {'p', 'li', 'table'})</p> <code>None</code> <code>to_markdown</code> <code>bool</code> <p>Convert common HTML tags to Markdown syntax</p> <code>False</code> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def __init__(\n    self,\n    preserve_tags: Optional[Set[str]] = None,\n    to_markdown: bool = False,\n):\n    \"\"\"\n    Initialize the HTML stripper.\n\n    Args:\n        preserve_tags: Set of tag names to preserve (e.g., {'p', 'li', 'table'})\n        to_markdown: Convert common HTML tags to Markdown syntax\n    \"\"\"\n    self.preserve_tags = preserve_tags or set()\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove HTML tags from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing HTML</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with HTML tags removed or converted to Markdown</p> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove HTML tags from the input text.\n\n    Args:\n        text: The input text containing HTML\n\n    Returns:\n        Text with HTML tags removed or converted to Markdown\n    \"\"\"\n    result = text\n\n    if self.to_markdown:\n        # Convert common HTML tags to Markdown\n        # Bold\n        result = re.sub(r\"&lt;strong&gt;(.*?)&lt;/strong&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;b&gt;(.*?)&lt;/b&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        # Italic\n        result = re.sub(r\"&lt;em&gt;(.*?)&lt;/em&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;i&gt;(.*?)&lt;/i&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        # Links\n        result = re.sub(\n            r'&lt;a[^&gt;]*href=[\"\\']([^\"\\']*)[\"\\'][^&gt;]*&gt;(.*?)&lt;/a&gt;',\n            r\"[\\2](\\1)\",\n            result,\n            flags=re.DOTALL,\n        )\n        # Headers\n        for i in range(1, 7):\n            result = re.sub(\n                f\"&lt;h{i}[^&gt;]*&gt;(.*?)&lt;/h{i}&gt;\",\n                f\"{'#' * i} \\\\1\\n\",\n                result,\n                flags=re.DOTALL,\n            )\n        # Code\n        result = re.sub(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", r\"`\\1`\", result, flags=re.DOTALL)\n        # Lists - simple conversion\n        result = re.sub(r\"&lt;li[^&gt;]*&gt;(.*?)&lt;/li&gt;\", r\"- \\1\\n\", result, flags=re.DOTALL)\n        # Paragraphs\n        result = re.sub(r\"&lt;p[^&gt;]*&gt;(.*?)&lt;/p&gt;\", r\"\\1\\n\\n\", result, flags=re.DOTALL)\n        # Line breaks\n        result = re.sub(r\"&lt;br\\s*/?&gt;\", \"\\n\", result)\n\n    if self.preserve_tags:\n        # Remove all tags except preserved ones\n        # This is a simplified implementation\n        tags_pattern = r\"&lt;/?(?!\" + \"|\".join(self.preserve_tags) + r\"\\b)[^&gt;]+&gt;\"\n        result = re.sub(tags_pattern, \"\", result)\n    else:\n        # Remove all HTML tags\n        result = re.sub(r\"&lt;[^&gt;]+&gt;\", \"\", result)\n\n    # Clean up excessive newlines\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n\n    return result.strip()\n</code></pre>"},{"location":"api-reference/cleaner/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import StripHTML\n\n# Basic HTML stripping\nstripper = StripHTML()\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\nstripper = StripHTML(to_markdown=True)\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n\n# Preserve specific tags\nstripper = StripHTML(preserve_tags={\"p\", \"div\"})\nresult = stripper.process(\"&lt;div&gt;Keep &lt;b&gt;Remove&lt;/b&gt;&lt;/div&gt;\")\n# Output: \"&lt;div&gt;Keep Remove&lt;/div&gt;\"\n</code></pre>"},{"location":"api-reference/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines into single spaces.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace","title":"prompt_refiner.cleaner.NormalizeWhitespace","text":"<p>               Bases: <code>Operation</code></p> <p>Normalize whitespace in text.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Normalize whitespace by collapsing multiple spaces into one.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with normalized whitespace</p> Source code in <code>src/prompt_refiner/cleaner/whitespace.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Normalize whitespace by collapsing multiple spaces into one.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with normalized whitespace\n    \"\"\"\n    # Replace multiple whitespace with single space and strip edges\n    return \" \".join(text.split())\n</code></pre>"},{"location":"api-reference/cleaner/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import NormalizeWhitespace\n\nnormalizer = NormalizeWhitespace()\nresult = normalizer.process(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre>"},{"location":"api-reference/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters including zero-width spaces and control characters.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode","title":"prompt_refiner.cleaner.FixUnicode","text":"<pre><code>FixUnicode(\n    remove_zero_width=True, remove_control_chars=True\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove or fix problematic Unicode characters.</p> <p>Initialize the Unicode fixer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_zero_width</code> <code>bool</code> <p>Remove zero-width spaces and similar characters</p> <code>True</code> <code>remove_control_chars</code> <code>bool</code> <p>Remove control characters (except newlines and tabs)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def __init__(self, remove_zero_width: bool = True, remove_control_chars: bool = True):\n    \"\"\"\n    Initialize the Unicode fixer.\n\n    Args:\n        remove_zero_width: Remove zero-width spaces and similar characters\n        remove_control_chars: Remove control characters (except newlines and tabs)\n    \"\"\"\n    self.remove_zero_width = remove_zero_width\n    self.remove_control_chars = remove_control_chars\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Clean problematic Unicode characters from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with problematic Unicode characters removed</p> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Clean problematic Unicode characters from text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with problematic Unicode characters removed\n    \"\"\"\n    result = text\n\n    if self.remove_zero_width:\n        # Remove zero-width characters\n        zero_width_chars = [\n            \"\\u200b\",  # Zero-width space\n            \"\\u200c\",  # Zero-width non-joiner\n            \"\\u200d\",  # Zero-width joiner\n            \"\\ufeff\",  # Zero-width no-break space (BOM)\n            \"\\u2060\",  # Word joiner\n        ]\n        for char in zero_width_chars:\n            result = result.replace(char, \"\")\n\n    if self.remove_control_chars:\n        # Remove control characters except newlines, tabs, and carriage returns\n        # Keep: \\n (0x0A), \\t (0x09), \\r (0x0D)\n        result = \"\".join(\n            char\n            for char in result\n            if not unicodedata.category(char).startswith(\"C\") or char in (\"\\n\", \"\\t\", \"\\r\")\n        )\n\n    # Normalize Unicode to NFC form (canonical composition)\n    result = unicodedata.normalize(\"NFC\", result)\n\n    return result\n</code></pre>"},{"location":"api-reference/cleaner/#examples_2","title":"Examples","text":"<pre><code>from prompt_refiner import FixUnicode\n\n# Remove zero-width spaces and control chars\nfixer = FixUnicode()\nresult = fixer.process(\"Hello\\u200bWorld\\u0000\")\n# Output: \"HelloWorld\"\n\n# Only remove zero-width spaces\nfixer = FixUnicode(remove_control_chars=False)\nresult = fixer.process(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre>"},{"location":"api-reference/cleaner/#jsoncleaner","title":"JsonCleaner","text":"<p>Clean and minify JSON by removing null values and empty containers.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner","title":"prompt_refiner.cleaner.JsonCleaner","text":"<pre><code>JsonCleaner(strip_nulls=True, strip_empty=True)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Cleans and minifies JSON strings. Removes null values, empty containers, and extra whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>strip_nulls</code> <code>bool</code> <p>If True, remove null/None values from objects and arrays (default: True)</p> <code>True</code> <code>strip_empty</code> <code>bool</code> <p>If True, remove empty dicts, lists, and strings (default: True)</p> <code>True</code> Example <p>from prompt_refiner import JsonCleaner cleaner = JsonCleaner(strip_nulls=True, strip_empty=True)</p> <p>dirty_json = ''' ... { ...   \"name\": \"Alice\", ...   \"age\": null, ...   \"address\": {}, ...   \"tags\": [], ...   \"bio\": \"\" ... } ... ''' result = cleaner.run(dirty_json) print(result)</p> Use Cases <ul> <li>RAG Context Compression: Strip nulls/empties from API responses before feeding to LLM</li> <li>Cost Optimization: Reduce token count by removing unnecessary JSON structure</li> <li>Data Cleaning: Normalize JSON from multiple sources with inconsistent null handling</li> </ul> <p>Initialize JSON cleaner.</p> <p>Parameters:</p> Name Type Description Default <code>strip_nulls</code> <code>bool</code> <p>Remove null/None values</p> <code>True</code> <code>strip_empty</code> <code>bool</code> <p>Remove empty containers (dict, list, str)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/json.py</code> <pre><code>def __init__(self, strip_nulls: bool = True, strip_empty: bool = True):\n    \"\"\"\n    Initialize JSON cleaner.\n\n    Args:\n        strip_nulls: Remove null/None values\n        strip_empty: Remove empty containers (dict, list, str)\n    \"\"\"\n    self.strip_nulls = strip_nulls\n    self.strip_empty = strip_empty\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Process the input JSON (string or object). Returns a minified JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, Dict, List]</code> <p>JSON string, dict, or list to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>Minified JSON string with nulls/empties removed</p> Note <p>If input is not valid JSON, returns input unchanged.</p> Source code in <code>src/prompt_refiner/cleaner/json.py</code> <pre><code>def process(self, text: Union[str, Dict, List]) -&gt; str:\n    \"\"\"\n    Process the input JSON (string or object).\n    Returns a minified JSON string.\n\n    Args:\n        text: JSON string, dict, or list to clean\n\n    Returns:\n        Minified JSON string with nulls/empties removed\n\n    Note:\n        If input is not valid JSON, returns input unchanged.\n    \"\"\"\n    # 1. Parse Input (Handle both string JSON and raw Dict/List)\n    data = text\n    if isinstance(text, str):\n        try:\n            data = json.loads(text)\n        except json.JSONDecodeError:\n            # If it's not valid JSON, return as-is to allow pipeline to continue safely\n            return text\n\n    # 2. Clean Structure\n    cleaned_data = self._clean_data(data)\n\n    # 3. Dump Minified String (No whitespace)\n    return json.dumps(cleaned_data, ensure_ascii=False, separators=(\",\", \":\"))\n</code></pre>"},{"location":"api-reference/cleaner/#examples_3","title":"Examples","text":"<pre><code>from prompt_refiner import JsonCleaner\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndirty_json = \"\"\"\n{\n    \"name\": \"Alice\",\n    \"age\": null,\n    \"address\": {},\n    \"tags\": [],\n    \"bio\": \"\"\n}\n\"\"\"\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\"}\n\n# Only strip nulls, keep empties\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=False)\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\",\"address\":{},\"tags\":[],\"bio\":\"\"}\n\n# Only minify (no cleaning)\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\",\"age\":null,\"address\":{},\"tags\":[],\"bio\":\"\"}\n\n# Works with dict/list inputs too\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndata = {\"name\": \"Bob\", \"tags\": [], \"age\": None}\nresult = cleaner.process(data)\n# Output: {\"name\":\"Bob\"}\n</code></pre>"},{"location":"api-reference/cleaner/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/cleaner/#web-scraping","title":"Web Scraping","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    Refiner()\n    .pipe(StripHTML(to_markdown=True))\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import Refiner, NormalizeWhitespace, FixUnicode\n\ntext_normalizer = (\n    Refiner()\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#rag-json-compression","title":"RAG JSON Compression","text":"<pre><code>from prompt_refiner import Refiner, JsonCleaner, TruncateTokens\n\nrag_compressor = (\n    Refiner()\n    .pipe(JsonCleaner(strip_nulls=True, strip_empty=True))\n    .pipe(TruncateTokens(max_tokens=500, strategy=\"head\"))\n)\n</code></pre>"},{"location":"api-reference/compressor/","title":"Compressor Module","text":"<p>The Compressor module provides operations for reducing text size through smart truncation and deduplication.</p>"},{"location":"api-reference/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens","title":"prompt_refiner.compressor.TruncateTokens","text":"<pre><code>TruncateTokens(\n    max_tokens,\n    strategy=\"head\",\n    respect_sentence_boundary=True,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p> <p>Initialize the truncation operation.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to keep</p> required <code>strategy</code> <code>Literal['head', 'tail', 'middle_out']</code> <p>Truncation strategy: - \"head\": Keep the beginning of the text - \"tail\": Keep the end of the text (useful for conversation history) - \"middle_out\": Keep beginning and end, remove middle</p> <code>'head'</code> <code>respect_sentence_boundary</code> <code>bool</code> <p>If True, truncate at sentence boundaries</p> <code>True</code> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int,\n    strategy: Literal[\"head\", \"tail\", \"middle_out\"] = \"head\",\n    respect_sentence_boundary: bool = True,\n):\n    \"\"\"\n    Initialize the truncation operation.\n\n    Args:\n        max_tokens: Maximum number of tokens to keep\n        strategy: Truncation strategy:\n            - \"head\": Keep the beginning of the text\n            - \"tail\": Keep the end of the text (useful for conversation history)\n            - \"middle_out\": Keep beginning and end, remove middle\n        respect_sentence_boundary: If True, truncate at sentence boundaries\n    \"\"\"\n    self.max_tokens = max_tokens\n    self.strategy = strategy\n    self.respect_sentence_boundary = respect_sentence_boundary\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Truncate text to max_tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text respecting sentence boundaries if configured</p> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate text to max_tokens.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Truncated text respecting sentence boundaries if configured\n    \"\"\"\n    estimated_tokens = self._estimate_tokens(text)\n\n    if estimated_tokens &lt;= self.max_tokens:\n        return text\n\n    if self.respect_sentence_boundary:\n        sentences = self._split_sentences(text)\n\n        if self.strategy == \"head\":\n            return self._truncate_head_sentences(sentences)\n        elif self.strategy == \"tail\":\n            return self._truncate_tail_sentences(sentences)\n        elif self.strategy == \"middle_out\":\n            return self._truncate_middle_out_sentences(sentences)\n    else:\n        # Fallback to word-based truncation\n        words = text.split()\n\n        if self.strategy == \"head\":\n            return \" \".join(words[: self.max_tokens])\n        elif self.strategy == \"tail\":\n            return \" \".join(words[-self.max_tokens :])\n        elif self.strategy == \"middle_out\":\n            half = self.max_tokens // 2\n            start_words = words[:half]\n            end_words = words[-(self.max_tokens - half) :]\n            return \" \".join(start_words) + \" ... \" + \" \".join(end_words)\n\n    return text\n</code></pre>"},{"location":"api-reference/compressor/#truncation-strategies","title":"Truncation Strategies","text":"<ul> <li><code>head</code>: Keep the beginning of the text (default)</li> <li><code>tail</code>: Keep the end of the text (useful for conversation history)</li> <li><code>middle_out</code>: Keep beginning and end, remove middle</li> </ul>"},{"location":"api-reference/compressor/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\nresult = truncator.process(long_text)\n\n# Keep last 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\nresult = truncator.process(long_text)\n\n# Keep first and last 50 tokens, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\nresult = truncator.process(long_text)\n\n# Truncate at word boundaries (faster, less precise)\ntruncator = TruncateTokens(\n    max_tokens=100,\n    strategy=\"head\",\n    respect_sentence_boundary=False\n)\nresult = truncator.process(long_text)\n</code></pre>"},{"location":"api-reference/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or highly similar text chunks, useful for RAG contexts.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate","title":"prompt_refiner.compressor.Deduplicate","text":"<pre><code>Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\",\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove duplicate or highly similar text chunks (useful for RAG contexts).</p> Performance Characteristics <p>This operation uses an O(n\u00b2) comparison algorithm, where each chunk is compared against all previously seen chunks. The total complexity is O(n\u00b2 \u00d7 comparison_cost), where comparison_cost depends on the selected similarity method: - Jaccard: O(m) where m is the chunk length (word-based) - Levenshtein: O(m\u2081 \u00d7 m\u2082) where m\u2081, m\u2082 are the chunk lengths (character-based)</p> <p>For typical RAG contexts (10-50 chunks), performance is acceptable with either method. For larger inputs (200+ chunks), consider using paragraph granularity to reduce the number of comparisons, or use Jaccard method for better performance.</p> <p>Initialize the deduplication operation.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for considering text similar (0.0-1.0)</p> <code>0.85</code> <code>method</code> <code>Literal['levenshtein', 'jaccard']</code> <p>Similarity calculation method - \"jaccard\": Jaccard similarity (word-based, faster)     * Complexity: O(m) per comparison where m is chunk length     * Recommended for most use cases (10-200 chunks)     * Fast even with long chunks - \"levenshtein\": Levenshtein distance (character-based)     * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison     * More precise but computationally expensive     * Can be slow with long chunks (1000+ characters)</p> <code>'jaccard'</code> <code>granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Text granularity to deduplicate at - \"sentence\": Deduplicate at sentence level     * More comparisons (more chunks) but smaller chunk sizes     * Better for fine-grained deduplication - \"paragraph\": Deduplicate at paragraph level     * Fewer comparisons but larger chunk sizes     * Recommended for large documents to reduce n\u00b2 scaling</p> <code>'paragraph'</code> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    similarity_threshold: float = 0.85,\n    method: Literal[\"levenshtein\", \"jaccard\"] = \"jaccard\",\n    granularity: Literal[\"sentence\", \"paragraph\"] = \"paragraph\",\n):\n    \"\"\"\n    Initialize the deduplication operation.\n\n    Args:\n        similarity_threshold: Threshold for considering text similar (0.0-1.0)\n        method: Similarity calculation method\n            - \"jaccard\": Jaccard similarity (word-based, faster)\n                * Complexity: O(m) per comparison where m is chunk length\n                * Recommended for most use cases (10-200 chunks)\n                * Fast even with long chunks\n            - \"levenshtein\": Levenshtein distance (character-based)\n                * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison\n                * More precise but computationally expensive\n                * Can be slow with long chunks (1000+ characters)\n        granularity: Text granularity to deduplicate at\n            - \"sentence\": Deduplicate at sentence level\n                * More comparisons (more chunks) but smaller chunk sizes\n                * Better for fine-grained deduplication\n            - \"paragraph\": Deduplicate at paragraph level\n                * Fewer comparisons but larger chunk sizes\n                * Recommended for large documents to reduce n\u00b2 scaling\n    \"\"\"\n    self.similarity_threshold = similarity_threshold\n    self.method = method\n    self.granularity = granularity\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove duplicate or similar text chunks.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with duplicates removed</p> Performance Note <p>This method uses O(n\u00b2) comparisons where n is the number of chunks. For large inputs (200+ chunks), consider using paragraph granularity to reduce the number of chunks, or ensure you're using the jaccard method for better performance.</p> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove duplicate or similar text chunks.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with duplicates removed\n\n    Performance Note:\n        This method uses O(n\u00b2) comparisons where n is the number of chunks.\n        For large inputs (200+ chunks), consider using paragraph granularity\n        to reduce the number of chunks, or ensure you're using the jaccard\n        method for better performance.\n    \"\"\"\n    chunks = self._split_text(text)\n\n    if not chunks:\n        return text\n\n    # Keep track of unique chunks\n    unique_chunks = []\n    seen_chunks = []\n\n    for chunk in chunks:\n        is_duplicate = False\n\n        # Check similarity with all previously seen chunks\n        for seen_chunk in seen_chunks:\n            similarity = self._calculate_similarity(chunk, seen_chunk)\n            if similarity &gt;= self.similarity_threshold:\n                is_duplicate = True\n                break\n\n        if not is_duplicate:\n            unique_chunks.append(chunk)\n            seen_chunks.append(chunk)\n\n    # Reconstruct text\n    if self.granularity == \"paragraph\":\n        return \"\\n\\n\".join(unique_chunks)\n    else:  # sentence\n        return \" \".join(unique_chunks)\n</code></pre>"},{"location":"api-reference/compressor/#similarity-methods","title":"Similarity Methods","text":"<ul> <li><code>jaccard</code>: Jaccard similarity (word-based, faster) - default</li> <li><code>levenshtein</code>: Levenshtein distance (character-based, more accurate)</li> </ul>"},{"location":"api-reference/compressor/#granularity-levels","title":"Granularity Levels","text":"<ul> <li><code>paragraph</code>: Deduplicate at paragraph level (split by <code>\\n\\n</code>) - default</li> <li><code>sentence</code>: Deduplicate at sentence level (split by <code>.</code>, <code>!</code>, <code>?</code>)</li> </ul>"},{"location":"api-reference/compressor/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# Basic deduplication (85% similarity threshold)\ndeduper = Deduplicate(similarity_threshold=0.85)\nresult = deduper.process(text_with_duplicates)\n\n# More aggressive (70% similarity)\ndeduper = Deduplicate(similarity_threshold=0.70)\nresult = deduper.process(text_with_duplicates)\n\n# Character-level similarity\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"levenshtein\"\n)\nresult = deduper.process(text_with_duplicates)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    granularity=\"sentence\"\n)\nresult = deduper.process(text_with_duplicates)\n</code></pre>"},{"location":"api-reference/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.85))  # Remove duplicates first\n    .pipe(TruncateTokens(max_tokens=2000))        # Then fit in context window\n)\n</code></pre>"},{"location":"api-reference/compressor/#conversation-history-compression","title":"Conversation History Compression","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nconversation_compressor = (\n    Refiner()\n    .pipe(Deduplicate(granularity=\"sentence\"))\n    .pipe(TruncateTokens(max_tokens=1000, strategy=\"tail\"))  # Keep recent messages\n)\n</code></pre>"},{"location":"api-reference/compressor/#document-summarization-prep","title":"Document Summarization Prep","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nsummarization_prep = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.90))  # Remove near-duplicates\n    .pipe(TruncateTokens(max_tokens=4000, strategy=\"middle_out\"))  # Keep intro + conclusion\n)\n</code></pre>"},{"location":"api-reference/packer/","title":"Packer Module API Reference","text":"<p>The Packer module provides specialized packers for managing context budgets with priority-based item selection. Version 0.1.3+ introduces two specialized packers following the Single Responsibility Principle.</p>"},{"location":"api-reference/packer/#messagespacker","title":"MessagesPacker","text":"<p>Optimized for chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict[str, str]]</code> directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker","title":"prompt_refiner.packer.MessagesPacker","text":"<pre><code>MessagesPacker(\n    max_tokens=None, model=None, track_savings=False\n)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for chat completion APIs.</p> <p>Designed for: - OpenAI Chat Completions (gpt-4, gpt-3.5-turbo, etc.) - Anthropic Messages API (claude-3-opus, claude-3-sonnet, etc.) - Any API using ChatML-style message format</p> <p>Returns: List[Dict[str, str]] with 'role' and 'content' keys</p> Example <p>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER</p> <p>Initialize messages packer.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum token budget. If None, includes all items without limit.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting</p> <code>None</code> <code>track_savings</code> <code>bool</code> <p>Enable automatic token savings tracking for refine_with operations (default: False)</p> <code>False</code> Source code in <code>src/prompt_refiner/packer/messages_packer.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: Optional[int] = None,\n    model: Optional[str] = None,\n    track_savings: bool = False,\n):\n    \"\"\"\n    Initialize messages packer.\n\n    Args:\n        max_tokens: Maximum token budget. If None, includes all items without limit.\n        model: Optional model name for precise token counting\n        track_savings: Enable automatic token savings tracking for refine_with\n            operations (default: False)\n    \"\"\"\n    super().__init__(max_tokens, model, track_savings)\n\n    # Pre-deduct request-level overhead (priming tokens) if budget is limited\n    if self.effective_max_tokens is not None:\n        self.effective_max_tokens -= PER_REQUEST_OVERHEAD\n        logger.debug(\n            f\"MessagesPacker initialized with {max_tokens} tokens \"\n            f\"(effective: {self.effective_max_tokens} after {PER_REQUEST_OVERHEAD} \"\n            f\"token request overhead)\"\n        )\n    else:\n        logger.debug(\"MessagesPacker initialized in unlimited mode\")\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--with-token-budget","title":"With token budget","text":"<p>packer = MessagesPacker(max_tokens=1000) packer.add(\"You are helpful.\", role=\"system\", priority=PRIORITY_SYSTEM) packer.add(\"Hello!\", role=\"user\", priority=PRIORITY_USER) messages = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--use-directly-openaichatcompletionscreatemessagesmessages","title":"Use directly: openai.chat.completions.create(messages=messages)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--without-token-budget-unlimited-mode","title":"Without token budget (unlimited mode)","text":"<p>packer = MessagesPacker()  # All items included packer.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM) packer.add(\"User query\", role=\"user\", priority=PRIORITY_USER) messages = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into message format for chat APIs.</p> <p>Automatically maps semantic roles to API-compatible roles: - ROLE_CONTEXT \u2192 \"user\" (RAG documents as user-provided context) - ROLE_QUERY \u2192 \"user\" (current user question) - Other roles (system, user, assistant) remain unchanged</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of message dictionaries with 'role' and 'content' keys,</p> <code>List[Dict[str, str]]</code> <p>ready for OpenAI, Anthropic, and other chat completion APIs.</p> Example <p>messages = packer.pack() openai.chat.completions.create(model=\"gpt-4\", messages=messages)</p> Source code in <code>src/prompt_refiner/packer/messages_packer.py</code> <pre><code>def pack(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Pack items into message format for chat APIs.\n\n    Automatically maps semantic roles to API-compatible roles:\n    - ROLE_CONTEXT \u2192 \"user\" (RAG documents as user-provided context)\n    - ROLE_QUERY \u2192 \"user\" (current user question)\n    - Other roles (system, user, assistant) remain unchanged\n\n    Returns:\n        List of message dictionaries with 'role' and 'content' keys,\n        ready for OpenAI, Anthropic, and other chat completion APIs.\n\n    Example:\n        &gt;&gt;&gt; messages = packer.pack()\n        &gt;&gt;&gt; openai.chat.completions.create(model=\"gpt-4\", messages=messages)\n    \"\"\"\n    selected_items = self._greedy_select()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty message list\")\n        return []\n\n    messages = []\n    for item in selected_items:\n        # Map semantic roles to API-compatible roles\n        api_role = item.role\n\n        if item.role == ROLE_CONTEXT:\n            # RAG documents become user messages (context provided by user)\n            api_role = \"user\"\n        elif item.role == ROLE_QUERY:\n            # Current query becomes user message\n            api_role = \"user\"\n        # Other roles (system, user, assistant) remain unchanged\n\n        messages.append({\"role\": api_role, \"content\": item.content})\n\n    logger.info(f\"Packed {len(messages)} messages for chat API\")\n    return messages\n</code></pre>"},{"location":"api-reference/packer/#textpacker","title":"TextPacker","text":"<p>Optimized for text completion APIs (Llama Base, GPT-3). Returns <code>str</code> directly with multiple text formats.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker","title":"prompt_refiner.packer.TextPacker","text":"<pre><code>TextPacker(\n    max_tokens=None,\n    model=None,\n    text_format=TextFormat.RAW,\n    separator=None,\n    track_savings=False,\n)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for text completion APIs.</p> <p>Designed for: - Base models (Llama-2-base, GPT-3, etc.) - Completion endpoints (not chat) - Custom prompt templates</p> <p>Returns: str (formatted text ready for completion API)</p> <p>Supports multiple text formatting strategies to prevent instruction drifting: - RAW: Simple concatenation with separators - MARKDOWN: Grouped sections (INSTRUCTIONS, CONTEXT, CONVERSATION, INPUT) - XML: Semantic content tags</p> Example <p>from prompt_refiner import TextPacker, TextFormat, PRIORITY_SYSTEM, PRIORITY_HIGH</p> <p>Initialize text packer.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum token budget. If None, includes all items without limit.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting</p> <code>None</code> <code>text_format</code> <code>TextFormat</code> <p>Text formatting strategy (RAW, MARKDOWN, XML)</p> <code>RAW</code> <code>separator</code> <code>Optional[str]</code> <p>String to join items (default: \"\\n\\n\" for clarity)</p> <code>None</code> <code>track_savings</code> <code>bool</code> <p>Enable automatic token savings tracking for refine_with operations (default: False)</p> <code>False</code> Source code in <code>src/prompt_refiner/packer/text_packer.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: Optional[int] = None,\n    model: Optional[str] = None,\n    text_format: TextFormat = TextFormat.RAW,\n    separator: Optional[str] = None,\n    track_savings: bool = False,\n):\n    \"\"\"\n    Initialize text packer.\n\n    Args:\n        max_tokens: Maximum token budget. If None, includes all items without limit.\n        model: Optional model name for precise token counting\n        text_format: Text formatting strategy (RAW, MARKDOWN, XML)\n        separator: String to join items (default: \"\\\\n\\\\n\" for clarity)\n        track_savings: Enable automatic token savings tracking for refine_with\n            operations (default: False)\n    \"\"\"\n    super().__init__(max_tokens, model, track_savings)\n    self.text_format = text_format\n    self.separator = separator if separator is not None else \"\\n\\n\"\n\n    # For MARKDOWN grouped format: Pre-deduct fixed header costs (\"entrance fee\")\n    # This prevents overestimating overhead for each item\n    if self.text_format == TextFormat.MARKDOWN and self.effective_max_tokens is not None:\n        self._reserve_fixed_headers()\n\n    logger.debug(\n        f\"TextPacker initialized with format={text_format.value}, \"\n        f\"separator={repr(self.separator)}, \"\n        f\"unlimited={self.effective_max_tokens is None}\"\n    )\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--with-token-budget","title":"With token budget","text":"<p>packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN) packer.add(\"You are helpful.\", role=\"system\", priority=PRIORITY_SYSTEM) packer.add(\"Context document\", priority=PRIORITY_HIGH) prompt = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--use-directly-completioncreatepromptprompt","title":"Use directly: completion.create(prompt=prompt)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--without-token-budget-unlimited-mode","title":"Without token budget (unlimited mode)","text":"<p>packer = TextPacker()  # All items included packer.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM) prompt = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into formatted text for completion APIs.</p> <p>MARKDOWN format uses grouped sections to reduce token overhead: - INSTRUCTIONS: System prompts (ROLE_SYSTEM) - CONTEXT: RAG documents (ROLE_CONTEXT) - CONVERSATION: User/assistant history (ROLE_USER, ROLE_ASSISTANT) - INPUT: Current user query (ROLE_QUERY)</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted text string ready for completion API</p> Example <p>prompt = packer.pack() response = completion.create(model=\"llama-2-70b\", prompt=prompt)</p> Source code in <code>src/prompt_refiner/packer/text_packer.py</code> <pre><code>def pack(self) -&gt; str:\n    \"\"\"\n    Pack items into formatted text for completion APIs.\n\n    MARKDOWN format uses grouped sections to reduce token overhead:\n    - INSTRUCTIONS: System prompts (ROLE_SYSTEM)\n    - CONTEXT: RAG documents (ROLE_CONTEXT)\n    - CONVERSATION: User/assistant history (ROLE_USER, ROLE_ASSISTANT)\n    - INPUT: Current user query (ROLE_QUERY)\n\n    Returns:\n        Formatted text string ready for completion API\n\n    Example:\n        &gt;&gt;&gt; prompt = packer.pack()\n        &gt;&gt;&gt; response = completion.create(model=\"llama-2-70b\", prompt=prompt)\n    \"\"\"\n    selected_items = self._greedy_select()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty string\")\n        return \"\"\n\n    # MARKDOWN format: Use grouped sections (saves tokens)\n    if self.text_format == TextFormat.MARKDOWN:\n        result = self._pack_markdown_grouped(selected_items)\n    else:\n        # RAW and XML: Use item-by-item formatting\n        parts = []\n        for item in selected_items:\n            formatted = self._format_item(item)\n            parts.append(formatted)\n        result = self.separator.join(parts)\n\n    logger.info(\n        f\"Packed {len(selected_items)} items into \"\n        f\"{self._count_tokens(result)} token text \"\n        f\"(format={self.text_format.value})\"\n    )\n    return result\n</code></pre>"},{"location":"api-reference/packer/#basepacker","title":"BasePacker","text":"<p>Abstract base class providing common packer functionality. You typically won't use this directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker","title":"prompt_refiner.packer.BasePacker","text":"<pre><code>BasePacker(\n    max_tokens=None, model=None, track_savings=False\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for token budget packers.</p> <p>Provides common functionality: - Adding items with priorities - JIT refinement with operations - Greedy selection algorithm - Token counting with safety buffer</p> <p>Subclasses must implement: - _calculate_overhead(): Calculate format-specific overhead - pack(): Format and return packed items</p> <p>Initialize packer with optional token budget.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum token budget. If None, includes all items without limit.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name for precise token counting (requires tiktoken)</p> <code>None</code> <code>track_savings</code> <code>bool</code> <p>Enable automatic token savings tracking for refine_with operations (default: False)</p> <code>False</code> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: Optional[int] = None,\n    model: Optional[str] = None,\n    track_savings: bool = False,\n):\n    \"\"\"\n    Initialize packer with optional token budget.\n\n    Args:\n        max_tokens: Maximum token budget. If None, includes all items without limit.\n        model: Optional model name for precise token counting (requires tiktoken)\n        track_savings: Enable automatic token savings tracking for refine_with\n            operations (default: False)\n    \"\"\"\n    self.raw_max_tokens = max_tokens\n    self._items: List[PackableItem] = []\n    self._insertion_counter = 0\n    self._token_counter = CountTokens(model=model)\n\n    # Token savings tracking (opt-in)\n    self.track_savings = track_savings\n    self._savings_stats = {\n        \"original_tokens\": 0,  # Sum of tokens before refinement\n        \"refined_tokens\": 0,  # Sum of tokens after refinement\n        \"items_refined\": 0,  # Count of items that used refine_with\n    }\n\n    # Calculate effective max tokens\n    if max_tokens is None:\n        self.effective_max_tokens = None\n        logger.debug(\"Unlimited mode: all items will be included\")\n    elif not self._token_counter.is_precise:\n        self.effective_max_tokens = int(max_tokens * 0.9)\n        logger.debug(\n            f\"Using estimation mode with 10% safety buffer: \"\n            f\"{self.effective_max_tokens}/{max_tokens}\"\n        )\n    else:\n        self.effective_max_tokens = max_tokens\n        logger.debug(f\"Using precise mode with tiktoken: {self.effective_max_tokens} tokens\")\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add","title":"add","text":"<pre><code>add(content, role, priority=None, refine_with=None)\n</code></pre> <p>Add an item to the packer.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text content to add</p> required <code>role</code> <code>str</code> <p>Semantic role (required). Use ROLE_* constants: - ROLE_SYSTEM: System instructions - ROLE_QUERY: Current user question - ROLE_CONTEXT: RAG retrieved documents - ROLE_USER: User messages in conversation history - ROLE_ASSISTANT: Assistant messages in history</p> required <code>priority</code> <code>Optional[int]</code> <p>Priority level (use PRIORITY_* constants). If None, infers from role: - ROLE_SYSTEM \u2192 PRIORITY_SYSTEM (0) - ROLE_QUERY \u2192 PRIORITY_QUERY (10) - ROLE_CONTEXT \u2192 PRIORITY_HIGH (20) - ROLE_USER/ROLE_ASSISTANT \u2192 PRIORITY_LOW (40) - Other roles \u2192 PRIORITY_MEDIUM (30)</p> <code>None</code> <code>refine_with</code> <code>Optional[Union[Operation, List[Operation]]]</code> <p>Optional operation(s) to apply before adding</p> <code>None</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add(\n    self,\n    content: str,\n    role: str,\n    priority: Optional[int] = None,\n    refine_with: Optional[Union[Operation, List[Operation]]] = None,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Add an item to the packer.\n\n    Args:\n        content: Text content to add\n        role: Semantic role (required). Use ROLE_* constants:\n            - ROLE_SYSTEM: System instructions\n            - ROLE_QUERY: Current user question\n            - ROLE_CONTEXT: RAG retrieved documents\n            - ROLE_USER: User messages in conversation history\n            - ROLE_ASSISTANT: Assistant messages in history\n        priority: Priority level (use PRIORITY_* constants). If None, infers from role:\n            - ROLE_SYSTEM \u2192 PRIORITY_SYSTEM (0)\n            - ROLE_QUERY \u2192 PRIORITY_QUERY (10)\n            - ROLE_CONTEXT \u2192 PRIORITY_HIGH (20)\n            - ROLE_USER/ROLE_ASSISTANT \u2192 PRIORITY_LOW (40)\n            - Other roles \u2192 PRIORITY_MEDIUM (30)\n        refine_with: Optional operation(s) to apply before adding\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Smart priority defaults based on semantic roles\n    if priority is None:\n        if role == ROLE_SYSTEM:\n            priority = PRIORITY_SYSTEM  # 0 - Highest priority\n        elif role == ROLE_QUERY:\n            priority = PRIORITY_QUERY  # 10 - Current query is critical\n        elif role == ROLE_CONTEXT:\n            priority = PRIORITY_HIGH  # 20 - RAG documents\n        elif role in (ROLE_USER, ROLE_ASSISTANT):\n            priority = PRIORITY_LOW  # 40 - Conversation history\n        else:\n            priority = PRIORITY_MEDIUM  # 30 - Unknown roles\n\n    # JIT refinement with optional tracking\n    if refine_with:\n        # Track original tokens before refinement (if tracking enabled)\n        original_content = content if self.track_savings else None\n\n        # Apply refinement operations\n        if isinstance(refine_with, list):\n            for op in refine_with:\n                content = op.process(content)\n        else:\n            content = refine_with.process(content)\n\n        # Update savings statistics (if tracking enabled)\n        if self.track_savings and original_content is not None:\n            original_tokens = self._count_tokens(original_content)\n            refined_tokens = self._count_tokens(content)\n            self._savings_stats[\"original_tokens\"] += original_tokens\n            self._savings_stats[\"refined_tokens\"] += refined_tokens\n            self._savings_stats[\"items_refined\"] += 1\n\n    # Count base tokens (without format overhead)\n    tokens = self._count_tokens(content)\n\n    item = PackableItem(\n        content=content,\n        tokens=tokens,\n        priority=priority,\n        insertion_index=self._insertion_counter,\n        role=role,\n    )\n\n    self._items.append(item)\n    self._insertion_counter += 1\n\n    logger.debug(f\"Added item: {tokens} tokens, priority={priority}, role={role}\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add_messages","title":"add_messages","text":"<pre><code>add_messages(messages, priority=PRIORITY_LOW)\n</code></pre> <p>Batch add messages (convenience method).</p> <p>Defaults to PRIORITY_LOW because conversation history is usually the first to be dropped in favor of RAG context and current queries.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, str]]</code> <p>List of message dicts with 'role' and 'content' keys</p> required <code>priority</code> <code>int</code> <p>Priority level for all messages (default: PRIORITY_LOW for history)</p> <code>PRIORITY_LOW</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add_messages(\n    self,\n    messages: List[Dict[str, str]],\n    priority: int = PRIORITY_LOW,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Batch add messages (convenience method).\n\n    Defaults to PRIORITY_LOW because conversation history is usually the first\n    to be dropped in favor of RAG context and current queries.\n\n    Args:\n        messages: List of message dicts with 'role' and 'content' keys\n        priority: Priority level for all messages (default: PRIORITY_LOW for history)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    for msg in messages:\n        self.add(content=msg[\"content\"], role=msg[\"role\"], priority=priority)\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset the packer, removing all items and clearing savings statistics.</p> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def reset(self) -&gt; \"BasePacker\":\n    \"\"\"\n    Reset the packer, removing all items and clearing savings statistics.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._items.clear()\n    self._insertion_counter = 0\n\n    # Reset savings tracking\n    if self.track_savings:\n        self._savings_stats = {\n            \"original_tokens\": 0,\n            \"refined_tokens\": 0,\n            \"items_refined\": 0,\n        }\n\n    logger.debug(\"Packer reset\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.get_items","title":"get_items","text":"<pre><code>get_items()\n</code></pre> <p>Get information about all added items.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries containing item metadata</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def get_items(self) -&gt; List[dict]:\n    \"\"\"\n    Get information about all added items.\n\n    Returns:\n        List of dictionaries containing item metadata\n    \"\"\"\n    return [\n        {\n            \"priority\": item.priority,\n            \"tokens\": item.tokens,\n            \"insertion_index\": item.insertion_index,\n            \"role\": item.role,\n        }\n        for item in self._items\n    ]\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.get_token_savings","title":"get_token_savings","text":"<pre><code>get_token_savings()\n</code></pre> <p>Get token savings statistics from refinement operations.</p> <p>Only includes items that used refine_with parameter. Items added without refinement are not counted.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with savings statistics:</p> <code>dict</code> <ul> <li>original_tokens: Total tokens before refinement</li> </ul> <code>dict</code> <ul> <li>refined_tokens: Total tokens after refinement</li> </ul> <code>dict</code> <ul> <li>saved_tokens: Tokens saved (original - refined)</li> </ul> <code>dict</code> <ul> <li>saving_percent: Percentage saved as formatted string (e.g., \"12.5%\")</li> </ul> <code>dict</code> <ul> <li>items_refined: Number of items that were refined</li> </ul> <code>dict</code> <p>Returns empty dict if track_savings=False or no items refined.</p> Example <p>packer = MessagesPacker(max_tokens=1000, track_savings=True) packer.add(html_doc, role=ROLE_CONTEXT, refine_with=StripHTML()) messages = packer.pack() stats = packer.get_token_savings() print(f\"Saved {stats['saved_tokens']} tokens ({stats['saving_percent']})\")</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def get_token_savings(self) -&gt; dict:\n    \"\"\"\n    Get token savings statistics from refinement operations.\n\n    Only includes items that used refine_with parameter. Items added\n    without refinement are not counted.\n\n    Returns:\n        Dictionary with savings statistics:\n        - original_tokens: Total tokens before refinement\n        - refined_tokens: Total tokens after refinement\n        - saved_tokens: Tokens saved (original - refined)\n        - saving_percent: Percentage saved as formatted string (e.g., \"12.5%\")\n        - items_refined: Number of items that were refined\n\n        Returns empty dict if track_savings=False or no items refined.\n\n    Example:\n        &gt;&gt;&gt; packer = MessagesPacker(max_tokens=1000, track_savings=True)\n        &gt;&gt;&gt; packer.add(html_doc, role=ROLE_CONTEXT, refine_with=StripHTML())\n        &gt;&gt;&gt; messages = packer.pack()\n        &gt;&gt;&gt; stats = packer.get_token_savings()\n        &gt;&gt;&gt; print(f\"Saved {stats['saved_tokens']} tokens ({stats['saving_percent']})\")\n    \"\"\"\n    if not self.track_savings:\n        logger.debug(\"Token savings tracking is disabled. Enable with track_savings=True\")\n        return {}\n\n    if self._savings_stats[\"items_refined\"] == 0:\n        logger.debug(\"No items have been refined yet\")\n        return {}\n\n    original = self._savings_stats[\"original_tokens\"]\n    refined = self._savings_stats[\"refined_tokens\"]\n    saved = original - refined\n    saving_percent = (saved / original * 100) if original &gt; 0 else 0.0\n\n    return {\n        \"original_tokens\": original,\n        \"refined_tokens\": refined,\n        \"saved_tokens\": saved,\n        \"saving_percent\": f\"{saving_percent:.1f}%\",\n        \"items_refined\": self._savings_stats[\"items_refined\"],\n    }\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.pack","title":"pack  <code>abstractmethod</code>","text":"<pre><code>pack()\n</code></pre> <p>Pack items into final format.</p> <p>Subclasses must implement this to return format-specific output: - MessagesPacker: Returns List[Dict[str, str]] - TextPacker: Returns str</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>@abstractmethod\ndef pack(self):\n    \"\"\"\n    Pack items into final format.\n\n    Subclasses must implement this to return format-specific output:\n    - MessagesPacker: Returns List[Dict[str, str]]\n    - TextPacker: Returns str\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/packer/#constants","title":"Constants","text":""},{"location":"api-reference/packer/#semantic-role-constants-recommended","title":"Semantic Role Constants (Recommended)","text":"<pre><code>from prompt_refiner import (\n    ROLE_SYSTEM,      # \"system\" - System instructions (auto: PRIORITY_SYSTEM = 0)\n    ROLE_QUERY,       # \"query\" - Current user question (auto: PRIORITY_QUERY = 10)\n    ROLE_CONTEXT,     # \"context\" - RAG documents (auto: PRIORITY_HIGH = 20)\n    ROLE_USER,        # \"user\" - User messages in history (auto: PRIORITY_LOW = 40)\n    ROLE_ASSISTANT,   # \"assistant\" - Assistant messages in history (auto: PRIORITY_LOW = 40)\n)\n</code></pre>"},{"location":"api-reference/packer/#priority-constants-optional","title":"Priority Constants (Optional)","text":"<pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_QUERY,    # 10 - Current user query (critical for response)\n    PRIORITY_HIGH,     # 20 - Important context (core RAG documents)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG documents)\n    PRIORITY_LOW,      # 40 - Optional content (old conversation history)\n)\n</code></pre> <p>Smart Priority Defaults</p> <p>You usually don't need to specify priority! Just use semantic roles and priority is auto-inferred:</p> <pre><code># Recommended: Use semantic roles (priority auto-inferred)\npacker.add(\"System prompt\", role=ROLE_SYSTEM)  # Auto: PRIORITY_SYSTEM (0)\npacker.add(\"User query\", role=ROLE_QUERY)      # Auto: PRIORITY_QUERY (10)\npacker.add(\"RAG doc\", role=ROLE_CONTEXT)       # Auto: PRIORITY_HIGH (20)\n\n# Advanced: Override priority if needed\npacker.add(\"Urgent RAG doc\", role=ROLE_CONTEXT, priority=PRIORITY_QUERY)\n</code></pre>"},{"location":"api-reference/packer/#overhead-constants","title":"Overhead Constants","text":"<pre><code>from prompt_refiner import (\n    PER_MESSAGE_OVERHEAD,  # 4 tokens - ChatML format overhead per message\n    PER_REQUEST_OVERHEAD,  # 3 tokens - Base request overhead (reserved for future use)\n)\n</code></pre> <p>These constants reflect the approximate token overhead for OpenAI's ChatML format (<code>&lt;|im_start|&gt;role\\n...\\n&lt;|im_end|&gt;</code>).</p>"},{"location":"api-reference/packer/#textformat-enum","title":"TextFormat Enum","text":"<pre><code>from prompt_refiner import TextFormat\n\nTextFormat.RAW       # No delimiters, simple concatenation\nTextFormat.MARKDOWN  # Use ### ROLE: headers (grouped sections in v0.1.3+)\nTextFormat.XML       # Use &lt;role&gt;content&lt;/role&gt; tags\n</code></pre>"},{"location":"api-reference/packer/#token-counting-modes","title":"Token Counting Modes","text":"<p>Estimation vs Precise Mode</p> <p>Both MessagesPacker and TextPacker use the same token counting as CountTokens:</p> <p>Estimation Mode (Default) - Uses character-based approximation: ~1 token \u2248 4 characters - Applies 10% safety buffer to prevent context overflow - Example: <code>max_tokens=1000</code> \u2192 <code>effective_max_tokens=900</code></p> <pre><code>packer = MessagesPacker(max_tokens=1000)  # 900 effective tokens\n</code></pre> <p>Precise Mode (Optional) - Requires <code>tiktoken</code>: <code>pip install llm-prompt-refiner[token]</code> - Exact token counting, no safety buffer (100% capacity) - Opt-in by passing a <code>model</code> parameter</p> <pre><code>packer = MessagesPacker(max_tokens=1000, model=\"gpt-4\")  # 1000 effective tokens\n</code></pre> <p>Recommendation: Use precise mode in production when you need maximum token utilization.</p>"},{"location":"api-reference/packer/#token-savings-tracking","title":"Token Savings Tracking","text":"<p>Version 0.1.5+ introduces automatic token savings tracking to measure the optimization impact of <code>refine_with</code> operations.</p>"},{"location":"api-reference/packer/#enable-tracking","title":"Enable Tracking","text":"<pre><code># Opt-in to tracking with track_savings parameter\npacker = MessagesPacker(track_savings=True)\n\n# Add items with refinement\npacker.add(\n    \"&lt;div&gt;  Messy   HTML  &lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n\n# Get savings statistics\nsavings = packer.get_token_savings()\n# Returns: {\n#   'original_tokens': 25,      # Tokens before refinement\n#   'refined_tokens': 12,       # Tokens after refinement\n#   'saved_tokens': 13,         # Tokens saved\n#   'saving_percent': 52.0,     # Percentage saved\n#   'items_refined': 1          # Count of refined items\n# }\n</code></pre>"},{"location":"api-reference/packer/#key-features","title":"Key Features","text":"<ul> <li>Opt-in: Disabled by default (no overhead when not needed)</li> <li>Automatic aggregation: Tracks all items that use <code>refine_with</code></li> <li>Per-item and total: Aggregates savings across all refined items</li> <li>Works with both packers: Available for <code>MessagesPacker</code> and <code>TextPacker</code></li> </ul>"},{"location":"api-reference/packer/#example-with-real-api","title":"Example with Real API","text":"<pre><code>from prompt_refiner import MessagesPacker, StripHTML, ROLE_CONTEXT\nfrom openai import OpenAI\n\nclient = OpenAI()\npacker = MessagesPacker(model=\"gpt-4o\", track_savings=True)\n\n# Add multiple RAG documents with automatic cleaning\nfor doc in scraped_html_docs:\n    packer.add(doc, role=ROLE_CONTEXT, refine_with=StripHTML())\n\n# Pack messages and check savings\nmessages = packer.pack()\nsavings = packer.get_token_savings()\n\nprint(f\"Saved {savings['saved_tokens']} tokens ({savings['saving_percent']:.1f}%)\")\n# Example output: \"Saved 1,234 tokens (23.5%)\"\n\n# Use cleaned messages with API\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages\n)\n</code></pre>"},{"location":"api-reference/packer/#when-to-use","title":"When to Use","text":"<p>\u2705 Use token savings tracking when: - You want to measure ROI of optimization efforts - Demonstrating token savings to stakeholders - A/B testing different refinement strategies - Monitoring optimization impact in production</p> <p>\u274c Skip tracking when: - Not using <code>refine_with</code> parameter (returns empty dict) - Performance is absolutely critical (negligible overhead, but why enable?) - You don't need savings metrics</p> <p>Combine with CountTokens</p> <p>For pipeline optimization (not packer), use <code>CountTokens</code> instead: <pre><code>from prompt_refiner import CountTokens, StripHTML, NormalizeWhitespace\n\ncounter = CountTokens(original_text=dirty_html)\npipeline = StripHTML() | NormalizeWhitespace()\nclean = pipeline.run(dirty_html)\ncounter.process(clean)\nprint(counter.format_stats())\n</code></pre></p>"},{"location":"api-reference/packer/#messagespacker-examples","title":"MessagesPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_QUERY\n\npacker = MessagesPacker(max_tokens=500)\n\npacker.add(\n    \"You are a helpful assistant.\",\n    role=ROLE_SYSTEM  # Auto: PRIORITY_SYSTEM (0)\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=ROLE_QUERY  # Auto: PRIORITY_QUERY (10)\n)\n\nmessages = packer.pack()  # List[Dict[str, str]]\n# Use directly: openai.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"api-reference/packer/#rag-with-conversation-history","title":"RAG with Conversation History","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    ROLE_SYSTEM, ROLE_QUERY, ROLE_CONTEXT,\n    ROLE_USER, ROLE_ASSISTANT,\n    StripHTML\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on provided context.\",\n    role=ROLE_SYSTEM  # Auto: PRIORITY_SYSTEM (0)\n)\n\n# RAG documents with JIT cleaning\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=ROLE_CONTEXT,  # Auto: PRIORITY_HIGH (20)\n    refine_with=StripHTML()\n)\n\n# Old conversation history (can be dropped if needed)\nold_messages = [\n    {\"role\": ROLE_USER, \"content\": \"What is this library?\"},\n    {\"role\": ROLE_ASSISTANT, \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages)  # Auto: PRIORITY_LOW (40) for history\n\n# Current query (must include)\npacker.add(\n    \"How does it reduce costs?\",\n    role=ROLE_QUERY  # Auto: PRIORITY_QUERY (10)\n)\n\nmessages = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#textpacker-examples","title":"TextPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY\n\npacker = TextPacker(\n    max_tokens=500,\n    text_format=TextFormat.MARKDOWN\n)\n\npacker.add(\n    \"You are a QA assistant.\",\n    role=ROLE_SYSTEM  # Auto: PRIORITY_SYSTEM (0)\n)\n\npacker.add(\n    \"Context: Prompt-refiner is a library...\",\n    role=ROLE_CONTEXT  # Auto: PRIORITY_HIGH (20)\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=ROLE_QUERY  # Auto: PRIORITY_QUERY (10)\n)\n\nprompt = packer.pack()  # str\n# Use with: completion.create(prompt=prompt)\n</code></pre>"},{"location":"api-reference/packer/#text-format-comparison","title":"Text Format Comparison","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY\n\n# RAW format (simple concatenation)\npacker = TextPacker(max_tokens=200, text_format=TextFormat.RAW)\npacker.add(\"System prompt\", role=ROLE_SYSTEM)\npacker.add(\"User query\", role=ROLE_QUERY)\nprompt = packer.pack()\n# Output:\n# System prompt\n#\n# User query\n\n# MARKDOWN format (grouped sections in v0.1.3+)\npacker = TextPacker(max_tokens=200, text_format=TextFormat.MARKDOWN)\npacker.add(\"System prompt\", role=ROLE_SYSTEM)\npacker.add(\"Doc 1\", role=ROLE_CONTEXT)\npacker.add(\"Doc 2\", role=ROLE_CONTEXT)\npacker.add(\"User query\", role=ROLE_QUERY)\nprompt = packer.pack()\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Doc 1\n# - Doc 2\n#\n# ### INPUT:\n# User query\n\n# XML format\npacker = TextPacker(max_tokens=200, text_format=TextFormat.XML)\npacker.add(\"System prompt\", role=ROLE_SYSTEM)\npacker.add(\"User query\", role=ROLE_QUERY)\nprompt = packer.pack()\n# Output:\n# &lt;system&gt;\n# System prompt\n# &lt;/system&gt;\n#\n# &lt;query&gt;\n# User query\n# &lt;/query&gt;\n</code></pre>"},{"location":"api-reference/packer/#common-features","title":"Common Features","text":""},{"location":"api-reference/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Both packers support Just-In-Time refinement:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, ROLE_CONTEXT\n\n# Single operation\npacker.add(\n    \"&lt;div&gt;HTML content&lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n\n# Multiple operations\npacker.add(\n    \"&lt;p&gt;  Messy   HTML  &lt;/p&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"api-reference/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_QUERY\n\nmessages = (\n    MessagesPacker(max_tokens=500)\n    .add(\"System prompt\", role=ROLE_SYSTEM)\n    .add(\"User query\", role=ROLE_QUERY)\n    .pack()\n)\n</code></pre>"},{"location":"api-reference/packer/#inspection","title":"Inspection","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_QUERY\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"Item 1\", role=ROLE_SYSTEM)\npacker.add(\"Item 2\", role=ROLE_QUERY)\n\n# Inspect items before packing\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Tokens: {item['tokens']}, Role: {item['role']}\")\n</code></pre>"},{"location":"api-reference/packer/#reset","title":"Reset","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_CONTEXT\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"First batch\", role=ROLE_CONTEXT)\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=ROLE_CONTEXT)\nmessages2 = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#algorithm-details","title":"Algorithm Details","text":"<ol> <li>Add Phase: Items are added with priorities, optional roles, and optional JIT refinement</li> <li>Token Counting:</li> <li>MessagesPacker: content tokens + 4 tokens overhead (ChatML format)</li> <li>TextPacker RAW: content tokens + separator overhead</li> <li>TextPacker MARKDOWN: content tokens + marginal overhead (3-4 tokens per item after fixed header reservation)</li> <li>TextPacker XML: content tokens + tag overhead</li> <li>Sort Phase: Items are sorted by priority (lower number = higher priority)</li> <li>Greedy Packing: Items are selected sequentially if they fit within the token budget</li> <li>Order Restoration: Selected items are restored to insertion order for natural reading flow</li> <li>Format Phase:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code></li> <li>TextPacker: Returns formatted <code>str</code> based on <code>text_format</code></li> </ol>"},{"location":"api-reference/packer/#tips","title":"Tips","text":"<p>Choose the Right Packer</p> <ul> <li>Use MessagesPacker for chat APIs (OpenAI, Anthropic)</li> <li>Use TextPacker for completion APIs (Llama Base, GPT-3)</li> </ul> <p>Use Semantic Roles (Recommended)</p> <p>Semantic roles auto-infer priorities, making code clearer:</p> <ul> <li><code>ROLE_SYSTEM</code>: System instructions \u2192 PRIORITY_SYSTEM (0)</li> <li><code>ROLE_QUERY</code>: Current user question \u2192 PRIORITY_QUERY (10)</li> <li><code>ROLE_CONTEXT</code>: RAG documents \u2192 PRIORITY_HIGH (20)</li> <li><code>ROLE_USER</code> / <code>ROLE_ASSISTANT</code>: Conversation history \u2192 PRIORITY_LOW (40)</li> </ul> <pre><code># Recommended: Clear intent with semantic roles\npacker.add(\"System prompt\", role=ROLE_SYSTEM)\npacker.add(\"Current query\", role=ROLE_QUERY)\npacker.add(\"RAG doc\", role=ROLE_CONTEXT)\n</code></pre> <p>Override Priority When Needed</p> <p>Most of the time semantic roles are enough, but you can override:</p> <pre><code># Make a RAG document urgent (higher priority than normal)\npacker.add(\"Critical doc\", role=ROLE_CONTEXT, priority=PRIORITY_QUERY)\n</code></pre> <p>Clean Before Packing</p> <p>Use <code>refine_with</code> to clean items before token counting:</p> <pre><code>packer.add(\n    dirty_html,\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n</code></pre> <p>Monitor Token Usage</p> <p>Check effective token budget and utilization:</p> <pre><code>packer = MessagesPacker(max_tokens=1000)\nprint(f\"Effective budget: {packer.effective_max_tokens}\")  # 900 in estimation mode\n</code></pre> <p>Grouped MARKDOWN Saves Tokens</p> <p>TextPacker with MARKDOWN format groups items by section, saving tokens:</p> <pre><code># Old (per-item headers): ### CONTEXT:\\nDoc 1\\n\\n### CONTEXT:\\nDoc 2\n# New (grouped): ### CONTEXT:\\n- Doc 1\\n- Doc 2\n</code></pre>"},{"location":"api-reference/refiner/","title":"Refiner Class","text":"<p>The <code>Refiner</code> class is the core pipeline builder that allows you to chain multiple operations together.</p>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner","title":"prompt_refiner.refiner.Refiner","text":"<pre><code>Refiner()\n</code></pre> <p>A pipeline builder for prompt refining operations.</p> <p>Initialize an empty refiner pipeline.</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize an empty refiner pipeline.\"\"\"\n    self._operations: List[Operation] = []\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner-functions","title":"Functions","text":""},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.pipe","title":"pipe","text":"<pre><code>pipe(operation)\n</code></pre> <p>Add an operation to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>Operation</code> <p>The operation to add</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def pipe(self, operation: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Add an operation to the pipeline.\n\n    Args:\n        operation: The operation to add\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._operations.append(operation)\n    return self\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.run","title":"run","text":"<pre><code>run(text)\n</code></pre> <p>Execute the pipeline on the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>The processed text after all operations</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def run(self, text: str) -&gt; str:\n    \"\"\"\n    Execute the pipeline on the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text after all operations\n    \"\"\"\n    result = text\n    for operation in self._operations:\n        result = operation.process(result)\n    return result\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre> <p>Support pipe operator syntax for adding operations to the pipeline.</p> <p>Enables continued chaining: (op1 | op2) | op3</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Operation</code> <p>The operation to add to the pipeline</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Example <p>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100) result = pipeline.run(text)</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __or__(self, other: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Support pipe operator syntax for adding operations to the pipeline.\n\n    Enables continued chaining: (op1 | op2) | op3\n\n    Args:\n        other: The operation to add to the pipeline\n\n    Returns:\n        Self for method chaining\n\n    Example:\n        &gt;&gt;&gt; from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n        &gt;&gt;&gt; pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100)\n        &gt;&gt;&gt; result = pipeline.run(text)\n    \"\"\"\n    return self.pipe(other)\n</code></pre>"},{"location":"api-reference/refiner/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/refiner/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre>"},{"location":"api-reference/refiner/#fluent-api-with-pipe","title":"Fluent API with .pipe()","text":"<p>The <code>Refiner</code> class supports method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the fluent API\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre> <p>Both approaches work identically - choose the one that fits your style.</p>"},{"location":"api-reference/refiner/#pipeline-execution","title":"Pipeline Execution","text":"<p>When you call <code>run(text)</code>, the Refiner:</p> <ol> <li>Takes the input text</li> <li>Passes it through each operation in sequence</li> <li>Each operation's output becomes the next operation's input</li> <li>Returns the final processed text</li> </ol> <pre><code># Pipeline: text \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 result\nresult = refiner.run(text)\n</code></pre>"},{"location":"api-reference/scrubber/","title":"Scrubber Module","text":"<p>The Scrubber module provides operations for security and privacy, including automatic PII redaction.</p>"},{"location":"api-reference/scrubber/#redactpii","title":"RedactPII","text":"<p>Redact sensitive personally identifiable information (PII) from text using regex patterns.</p>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII","title":"prompt_refiner.scrubber.RedactPII","text":"<pre><code>RedactPII(\n    redact_types=None,\n    custom_patterns=None,\n    custom_keywords=None,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Redact sensitive information from text using regex patterns.</p> <p>Initialize the PII redaction operation.</p> <p>Parameters:</p> Name Type Description Default <code>redact_types</code> <code>Optional[Set[str]]</code> <p>Set of PII types to redact (default: all) Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"</p> <code>None</code> <code>custom_patterns</code> <code>Optional[dict[str, str]]</code> <p>Dictionary of custom regex patterns to apply Format: {\"name\": \"regex_pattern\"}</p> <code>None</code> <code>custom_keywords</code> <code>Optional[Set[str]]</code> <p>Set of custom keywords to redact (case-insensitive)</p> <code>None</code> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def __init__(\n    self,\n    redact_types: Optional[Set[str]] = None,\n    custom_patterns: Optional[dict[str, str]] = None,\n    custom_keywords: Optional[Set[str]] = None,\n):\n    \"\"\"\n    Initialize the PII redaction operation.\n\n    Args:\n        redact_types: Set of PII types to redact (default: all)\n            Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"\n        custom_patterns: Dictionary of custom regex patterns to apply\n            Format: {\"name\": \"regex_pattern\"}\n        custom_keywords: Set of custom keywords to redact (case-insensitive)\n    \"\"\"\n    self.redact_types = redact_types or set(self.PATTERNS.keys())\n    self.custom_patterns = custom_patterns or {}\n    self.custom_keywords = custom_keywords or set()\n</code></pre>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII-functions","title":"Functions","text":""},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Redact PII from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with PII redacted</p> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Redact PII from the input text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with PII redacted\n    \"\"\"\n    result = text\n\n    # Apply standard PII patterns\n    for pii_type in self.redact_types:\n        if pii_type in self.PATTERNS:\n            pattern = self.PATTERNS[pii_type]\n            replacement = self.REPLACEMENTS.get(pii_type, \"[REDACTED]\")\n            result = re.sub(pattern, replacement, result)\n\n    # Apply custom patterns\n    for name, pattern in self.custom_patterns.items():\n        replacement = f\"[{name.upper()}]\"\n        result = re.sub(pattern, replacement, result)\n\n    # Apply custom keywords (case-insensitive)\n    for keyword in self.custom_keywords:\n        # Use word boundaries to avoid partial matches\n        pattern = rf\"\\b{re.escape(keyword)}\\b\"\n        result = re.sub(pattern, \"[REDACTED]\", result, flags=re.IGNORECASE)\n\n    return result\n</code></pre>"},{"location":"api-reference/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code>: Email addresses \u2192 <code>[EMAIL]</code></li> <li><code>phone</code>: Phone numbers (US format) \u2192 <code>[PHONE]</code></li> <li><code>ip</code>: IP addresses \u2192 <code>[IP]</code></li> <li><code>credit_card</code>: Credit card numbers \u2192 <code>[CARD]</code></li> <li><code>ssn</code>: Social Security Numbers \u2192 <code>[SSN]</code></li> <li><code>url</code>: URLs \u2192 <code>[URL]</code></li> </ul>"},{"location":"api-reference/scrubber/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact me at john@example.com or 555-123-4567\")\n# Output: \"Contact me at [EMAIL] or [PHONE]\"\n\n# Redact specific types only\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\nresult = redactor.process(\"Email: john@example.com, IP: 192.168.1.1\")\n# Output: \"Email: [EMAIL], IP: 192.168.1.1\"\n\n# Custom patterns\nredactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\nresult = redactor.process(\"Employee EMP-12345 accessed the system\")\n# Output: \"Employee [EMPLOYEE_ID] accessed the system\"\n\n# Custom keywords (case-insensitive)\nredactor = RedactPII(\n    custom_keywords={\"confidential\", \"secret\"}\n)\nresult = redactor.process(\"This is Confidential information\")\n# Output: \"This is [REDACTED] information\"\n</code></pre>"},{"location":"api-reference/scrubber/#combining-options","title":"Combining Options","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact standard PII + custom patterns + keywords\nredactor = RedactPII(\n    redact_types={\"email\", \"phone\", \"ssn\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"},\n    custom_keywords={\"internal\", \"confidential\"}\n)\n\ntext = \"\"\"\nEmployee EMP-12345\nEmail: john@example.com\nPhone: 555-123-4567\nSSN: 123-45-6789\nThis is Confidential information for internal use only.\n\"\"\"\n\nresult = redactor.process(text)\n</code></pre>"},{"location":"api-reference/scrubber/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/scrubber/#before-sending-to-llm-apis","title":"Before Sending to LLM APIs","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nsecure_pipeline = (\n    Refiner()\n    .pipe(RedactPII(redact_types={\"email\", \"phone\", \"ssn\", \"credit_card\"}))\n)\n\n# Safe to send to external APIs\nsecure_text = secure_pipeline.run(user_input)\n</code></pre>"},{"location":"api-reference/scrubber/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nlog_redactor = (\n    Refiner()\n    .pipe(RedactPII())  # Redact all PII types\n)\n\n# Safe to log\nsafe_log = log_redactor.run(sensitive_data)\nlogger.info(safe_log)\n</code></pre>"},{"location":"api-reference/scrubber/#data-export-compliance","title":"Data Export Compliance","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\n# Custom redaction for specific compliance needs\ngdpr_redactor = (\n    Refiner()\n    .pipe(RedactPII(\n        redact_types={\"email\", \"phone\", \"ip\"},\n        custom_keywords={\"customer_name\", \"address\", \"dob\"}\n    ))\n)\n\nexport_data = gdpr_redactor.run(user_data)\n</code></pre>"},{"location":"api-reference/scrubber/#security-best-practices","title":"Security Best Practices","text":"<p>Regex Limitations</p> <p>PII redaction uses regex patterns which may not catch all variations. For production use:</p> <ul> <li>Test thoroughly with your specific data</li> <li>Consider using specialized PII detection services for critical applications</li> <li>Add custom patterns for domain-specific PII</li> <li>Review redacted output before sending to external services</li> </ul> <p>Defense in Depth</p> <p>PII redaction is one layer of security. Always:</p> <ul> <li>Validate and sanitize user input</li> <li>Use proper authentication and authorization</li> <li>Encrypt data in transit and at rest</li> <li>Follow your organization's security policies</li> </ul>"},{"location":"api-reference/strategy/","title":"Strategy Module API Reference","text":"<p>The Strategy module provides benchmark-tested preset strategies for token optimization. Use these when you want quick savings without manually configuring individual operations.</p>"},{"location":"api-reference/strategy/#overview","title":"Overview","text":"<p>Version 0.1.5+ introduces three preset strategies optimized for different use cases:</p> Strategy Token Reduction Quality Use Case Minimal 4.3% 98.7% Maximum quality, minimal risk Standard 4.8% 98.4% RAG contexts with duplicates Aggressive 15% 96.4% Cost optimization, long contexts <p>All strategies return a <code>Refiner</code> instance, making them fully compatible with the existing API and extensible with <code>.pipe()</code>.</p>"},{"location":"api-reference/strategy/#minimalstrategy","title":"MinimalStrategy","text":"<p>Basic cleaning with minimal token reduction, prioritizing quality preservation.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy","title":"prompt_refiner.strategy.MinimalStrategy","text":"<pre><code>MinimalStrategy(strip_html=True, to_markdown=False)\n</code></pre> <p>               Bases: <code>BaseStrategy</code></p> <p>Minimal strategy: Basic cleaning with minimal token reduction.</p> <p>Operations: - StripHTML: Remove HTML tags - NormalizeWhitespace: Collapse excessive whitespace</p> <p>Characteristics: - Token reduction: ~4.3% - Quality: 98.7% (cosine similarity) - Use case: When quality is paramount, minimal risk - Latency: 0.05ms per 1k tokens</p> <p>Initialize minimal strategy.</p> <p>Parameters:</p> Name Type Description Default <code>strip_html</code> <code>bool</code> <p>Whether to strip HTML tags (default: True)</p> <code>True</code> <code>to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> Source code in <code>src/prompt_refiner/strategy/minimal.py</code> <pre><code>def __init__(self, strip_html: bool = True, to_markdown: bool = False):\n    \"\"\"\n    Initialize minimal strategy.\n\n    Args:\n        strip_html: Whether to strip HTML tags (default: True)\n        to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n    \"\"\"\n    self.strip_html = strip_html\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy.get_operations","title":"get_operations","text":"<pre><code>get_operations()\n</code></pre> <p>Get operations for minimal strategy.</p> Source code in <code>src/prompt_refiner/strategy/minimal.py</code> <pre><code>def get_operations(self) -&gt; List[Operation]:\n    \"\"\"Get operations for minimal strategy.\"\"\"\n    operations = []\n    if self.strip_html:\n        operations.append(StripHTML(to_markdown=self.to_markdown))\n    operations.append(NormalizeWhitespace())\n    return operations\n</code></pre>"},{"location":"api-reference/strategy/#operations","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> </ul>"},{"location":"api-reference/strategy/#example","title":"Example","text":"<pre><code>from prompt_refiner.strategy import MinimalStrategy\n\n# Create strategy and refiner\nrefiner = MinimalStrategy().create_refiner()\ncleaned = refiner.run(\"&lt;div&gt;  Your HTML content  &lt;/div&gt;\")\n# Output: \"Your HTML content\"\n\n# With Markdown conversion\nrefiner = MinimalStrategy(to_markdown=True).create_refiner()\ncleaned = refiner.run(\"&lt;strong&gt;bold&lt;/strong&gt; text\")\n# Output: \"**bold** text\"\n\n# Extend with additional operations\nfrom prompt_refiner import RedactPII\nrefiner = MinimalStrategy().create_refiner()\nrefiner.pipe(RedactPII(redact_types={\"email\"}))\n</code></pre>"},{"location":"api-reference/strategy/#standardstrategy","title":"StandardStrategy","text":"<p>Enhanced cleaning with deduplication for RAG contexts with potential duplicates.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy","title":"prompt_refiner.strategy.StandardStrategy","text":"<pre><code>StandardStrategy(\n    strip_html=True,\n    to_markdown=False,\n    similarity_threshold=0.8,\n    dedup_method=\"jaccard\",\n)\n</code></pre> <p>               Bases: <code>BaseStrategy</code></p> <p>Standard strategy: Cleaning plus deduplication.</p> <p>Operations: - StripHTML: Remove HTML tags - NormalizeWhitespace: Collapse excessive whitespace - Deduplicate: Remove similar content (sentence-level, 0.8 threshold)</p> <p>Characteristics: - Token reduction: ~4.8% - Quality: 98.4% (cosine similarity) - Use case: RAG contexts with potential duplicates - Latency: 0.25ms per 1k tokens</p> <p>Initialize standard strategy.</p> <p>Parameters:</p> Name Type Description Default <code>strip_html</code> <code>bool</code> <p>Whether to strip HTML tags (default: True)</p> <code>True</code> <code>to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> <code>similarity_threshold</code> <code>float</code> <p>Threshold for deduplication (default: 0.8)</p> <code>0.8</code> <code>dedup_method</code> <code>Literal['jaccard', 'levenshtein']</code> <p>Deduplication method: \"jaccard\" or \"levenshtein\" (default: \"jaccard\")</p> <code>'jaccard'</code> Source code in <code>src/prompt_refiner/strategy/standard.py</code> <pre><code>def __init__(\n    self,\n    strip_html: bool = True,\n    to_markdown: bool = False,\n    similarity_threshold: float = 0.8,\n    dedup_method: Literal[\"jaccard\", \"levenshtein\"] = \"jaccard\",\n):\n    \"\"\"\n    Initialize standard strategy.\n\n    Args:\n        strip_html: Whether to strip HTML tags (default: True)\n        to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n        similarity_threshold: Threshold for deduplication (default: 0.8)\n        dedup_method: Deduplication method: \"jaccard\" or \"levenshtein\" (default: \"jaccard\")\n    \"\"\"\n    self.strip_html = strip_html\n    self.to_markdown = to_markdown\n    self.similarity_threshold = similarity_threshold\n    self.dedup_method = dedup_method\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy.get_operations","title":"get_operations","text":"<pre><code>get_operations()\n</code></pre> <p>Get operations for standard strategy.</p> Source code in <code>src/prompt_refiner/strategy/standard.py</code> <pre><code>def get_operations(self) -&gt; List[Operation]:\n    \"\"\"Get operations for standard strategy.\"\"\"\n    operations = []\n    if self.strip_html:\n        operations.append(StripHTML(to_markdown=self.to_markdown))\n    operations.append(NormalizeWhitespace())\n    operations.append(\n        Deduplicate(\n            similarity_threshold=self.similarity_threshold,\n            method=self.dedup_method,\n            granularity=\"sentence\",\n        )\n    )\n    return operations\n</code></pre>"},{"location":"api-reference/strategy/#operations_1","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> <li><code>Deduplicate()</code> - Remove similar content (sentence-level, 0.8 threshold)</li> </ul>"},{"location":"api-reference/strategy/#example_1","title":"Example","text":"<pre><code>from prompt_refiner.strategy import StandardStrategy\n\n# Create strategy with defaults\nrefiner = StandardStrategy().create_refiner()\ntext = \"&lt;div&gt;Hello world. Hello world. Goodbye world.&lt;/div&gt;\"\ncleaned = refiner.run(text)\n# Output: \"Hello world. Goodbye world.\"  (duplicate removed)\n\n# Custom similarity threshold\nrefiner = StandardStrategy(similarity_threshold=0.7).create_refiner()\n\n# Alternative deduplication method\nrefiner = StandardStrategy(dedup_method=\"levenshtein\").create_refiner()\n</code></pre>"},{"location":"api-reference/strategy/#aggressivestrategy","title":"AggressiveStrategy","text":"<p>Maximum token reduction with deduplication and truncation for cost optimization.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy","title":"prompt_refiner.strategy.AggressiveStrategy","text":"<pre><code>AggressiveStrategy(\n    max_tokens=150,\n    strip_html=True,\n    to_markdown=False,\n    similarity_threshold=0.7,\n    dedup_method=\"jaccard\",\n    truncate_strategy=\"head\",\n)\n</code></pre> <p>               Bases: <code>BaseStrategy</code></p> <p>Aggressive strategy: Maximum token reduction with truncation.</p> <p>Operations: - StripHTML: Remove HTML tags - NormalizeWhitespace: Collapse excessive whitespace - Deduplicate: Remove similar content (sentence-level, 0.7 threshold) - TruncateTokens: Limit to max_tokens (default: 150)</p> <p>Characteristics: - Token reduction: ~15% (up to 74% on long contexts) - Quality: 96.4% (cosine similarity) - Use case: Cost optimization, long contexts, lenient quality requirements - Latency: 0.25ms per 1k tokens</p> <p>Initialize aggressive strategy.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum tokens to keep (default: 150)</p> <code>150</code> <code>strip_html</code> <code>bool</code> <p>Whether to strip HTML tags (default: True)</p> <code>True</code> <code>to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> <code>similarity_threshold</code> <code>float</code> <p>Threshold for deduplication (default: 0.7)</p> <code>0.7</code> <code>dedup_method</code> <code>Literal['jaccard', 'levenshtein']</code> <p>Deduplication method: \"jaccard\" or \"levenshtein\" (default: \"jaccard\")</p> <code>'jaccard'</code> <code>truncate_strategy</code> <code>Literal['head', 'tail', 'middle_out']</code> <p>\"head\", \"tail\", or \"middle_out\" (default: \"head\")</p> <code>'head'</code> Source code in <code>src/prompt_refiner/strategy/aggressive.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int = 150,\n    strip_html: bool = True,\n    to_markdown: bool = False,\n    similarity_threshold: float = 0.7,\n    dedup_method: Literal[\"jaccard\", \"levenshtein\"] = \"jaccard\",\n    truncate_strategy: Literal[\"head\", \"tail\", \"middle_out\"] = \"head\",\n):\n    \"\"\"\n    Initialize aggressive strategy.\n\n    Args:\n        max_tokens: Maximum tokens to keep (default: 150)\n        strip_html: Whether to strip HTML tags (default: True)\n        to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n        similarity_threshold: Threshold for deduplication (default: 0.7)\n        dedup_method: Deduplication method: \"jaccard\" or \"levenshtein\" (default: \"jaccard\")\n        truncate_strategy: \"head\", \"tail\", or \"middle_out\" (default: \"head\")\n    \"\"\"\n    self.max_tokens = max_tokens\n    self.strip_html = strip_html\n    self.to_markdown = to_markdown\n    self.similarity_threshold = similarity_threshold\n    self.dedup_method = dedup_method\n    self.truncate_strategy = truncate_strategy\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy.get_operations","title":"get_operations","text":"<pre><code>get_operations()\n</code></pre> <p>Get operations for aggressive strategy.</p> Source code in <code>src/prompt_refiner/strategy/aggressive.py</code> <pre><code>def get_operations(self) -&gt; List[Operation]:\n    \"\"\"Get operations for aggressive strategy.\"\"\"\n    operations = []\n    if self.strip_html:\n        operations.append(StripHTML(to_markdown=self.to_markdown))\n    operations.append(NormalizeWhitespace())\n    operations.append(\n        Deduplicate(\n            similarity_threshold=self.similarity_threshold,\n            method=self.dedup_method,\n            granularity=\"sentence\",\n        )\n    )\n    operations.append(\n        TruncateTokens(max_tokens=self.max_tokens, strategy=self.truncate_strategy)\n    )\n    return operations\n</code></pre>"},{"location":"api-reference/strategy/#operations_2","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> <li><code>Deduplicate()</code> - Remove similar content (sentence-level, 0.7 threshold)</li> <li><code>TruncateTokens()</code> - Limit to max_tokens (default: 150)</li> </ul>"},{"location":"api-reference/strategy/#example_2","title":"Example","text":"<pre><code>from prompt_refiner.strategy import AggressiveStrategy\n\n# Create strategy with default max_tokens=150\nrefiner = AggressiveStrategy().create_refiner()\nlong_text = \"word \" * 100  # 100 words\ncleaned = refiner.run(long_text)\n# Output: Truncated to ~150 tokens with duplicates removed\n\n# Custom max_tokens and truncation strategy\nrefiner = AggressiveStrategy(\n    max_tokens=200,\n    truncate_strategy=\"tail\"  # Keep last 200 tokens\n).create_refiner()\n\n# More aggressive deduplication\nrefiner = AggressiveStrategy(\n    max_tokens=100,\n    similarity_threshold=0.6  # More aggressive duplicate detection\n).create_refiner()\n</code></pre>"},{"location":"api-reference/strategy/#basestrategy","title":"BaseStrategy","text":"<p>Abstract base class for creating custom strategies.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.BaseStrategy","title":"prompt_refiner.strategy.BaseStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all refining strategies.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.BaseStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#prompt_refiner.strategy.BaseStrategy.get_operations","title":"get_operations  <code>abstractmethod</code>","text":"<pre><code>get_operations()\n</code></pre> <p>Get the list of operations for this strategy.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Operation instances to be applied in sequence</p> Source code in <code>src/prompt_refiner/strategy/base.py</code> <pre><code>@abstractmethod\ndef get_operations(self) -&gt; list:\n    \"\"\"\n    Get the list of operations for this strategy.\n\n    Returns:\n        List of Operation instances to be applied in sequence\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.BaseStrategy.create_refiner","title":"create_refiner","text":"<pre><code>create_refiner()\n</code></pre> <p>Create a Refiner instance with this strategy's operations.</p> <p>Returns:</p> Type Description <code>Refiner</code> <p>Configured Refiner instance</p> Source code in <code>src/prompt_refiner/strategy/base.py</code> <pre><code>def create_refiner(self) -&gt; Refiner:\n    \"\"\"\n    Create a Refiner instance with this strategy's operations.\n\n    Returns:\n        Configured Refiner instance\n    \"\"\"\n    refiner = Refiner()\n    for operation in self.get_operations():\n        refiner.pipe(operation)\n    return refiner\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.BaseStrategy.__call__","title":"__call__","text":"<pre><code>__call__(text)\n</code></pre> <p>Apply strategy directly to text (convenience method).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>Processed text</p> Source code in <code>src/prompt_refiner/strategy/base.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    \"\"\"\n    Apply strategy directly to text (convenience method).\n\n    Args:\n        text: Input text to process\n\n    Returns:\n        Processed text\n    \"\"\"\n    return self.create_refiner().run(text)\n</code></pre>"},{"location":"api-reference/strategy/#creating-custom-strategies","title":"Creating Custom Strategies","text":"<pre><code>from prompt_refiner.strategy import BaseStrategy\nfrom prompt_refiner import StripHTML, NormalizeWhitespace, RedactPII\n\nclass CustomStrategy(BaseStrategy):\n    def __init__(self, redact_pii: bool = True):\n        self.redact_pii = redact_pii\n\n    def get_operations(self):\n        operations = [StripHTML(), NormalizeWhitespace()]\n        if self.redact_pii:\n            operations.append(RedactPII(redact_types={\"email\", \"phone\"}))\n        return operations\n\n# Use custom strategy\nrefiner = CustomStrategy(redact_pii=True).create_refiner()\n</code></pre>"},{"location":"api-reference/strategy/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api-reference/strategy/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner.strategy import MinimalStrategy, StandardStrategy, AggressiveStrategy\n\n# Quick start with minimal\nrefiner = MinimalStrategy().create_refiner()\ncleaned = refiner.run(text)\n\n# Standard for RAG with duplicates\nrefiner = StandardStrategy().create_refiner()\ncleaned = refiner.run(rag_context)\n\n# Aggressive for cost optimization\nrefiner = AggressiveStrategy(max_tokens=200).create_refiner()\ncleaned = refiner.run(long_context)\n</code></pre>"},{"location":"api-reference/strategy/#composition-with-additional-operations","title":"Composition with Additional Operations","text":"<p>Strategies return <code>Refiner</code> instances, so you can extend them with additional operations:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy\nfrom prompt_refiner import RedactPII, Deduplicate\n\n# Start with minimal, add PII redaction\nrefiner = MinimalStrategy().create_refiner()\nrefiner.pipe(RedactPII(redact_types={\"email\"}))\n\n# Start with standard, add more aggressive deduplication\nfrom prompt_refiner.strategy import StandardStrategy\nrefiner = StandardStrategy().create_refiner()\nrefiner.pipe(Deduplicate(similarity_threshold=0.6))  # More aggressive\n</code></pre>"},{"location":"api-reference/strategy/#direct-strategy-calling","title":"Direct Strategy Calling","text":"<p>Strategies also support direct calling for quick one-off processing:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy\n\nstrategy = MinimalStrategy()\ncleaned = strategy(text)  # Equivalent to: strategy.create_refiner().run(text)\n</code></pre>"},{"location":"api-reference/strategy/#choosing-a-strategy","title":"Choosing a Strategy","text":""},{"location":"api-reference/strategy/#minimal-strategy","title":"Minimal Strategy","text":"<p>\u2705 Use when: - Quality is paramount - Minimal risk tolerance - Processing structured content - First time optimizing prompts</p> <p>\u274c Avoid when: - Budget constraints are tight - Dealing with very long contexts - Content has significant duplication</p>"},{"location":"api-reference/strategy/#standard-strategy","title":"Standard Strategy","text":"<p>\u2705 Use when: - RAG contexts with potential duplicates - Balanced quality and savings needed - Processing web-scraped content - General-purpose optimization</p> <p>\u274c Avoid when: - Context is already clean and unique - Maximum quality preservation required - Very tight token budgets</p>"},{"location":"api-reference/strategy/#aggressive-strategy","title":"Aggressive Strategy","text":"<p>\u2705 Use when: - Cost optimization is priority - Token budgets are tight - Processing very long contexts - Quality tolerance is lenient</p> <p>\u274c Avoid when: - Quality cannot be compromised - Context is already short - Truncation would remove critical info</p>"},{"location":"api-reference/strategy/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api-reference/strategy/#minimalstrategy-parameters","title":"MinimalStrategy Parameters","text":"Parameter Type Default Description <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping"},{"location":"api-reference/strategy/#standardstrategy-parameters","title":"StandardStrategy Parameters","text":"Parameter Type Default Description <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping <code>similarity_threshold</code> <code>float</code> <code>0.8</code> Threshold for deduplication (0.0-1.0) <code>dedup_method</code> <code>Literal[\"jaccard\", \"levenshtein\"]</code> <code>\"jaccard\"</code> Deduplication algorithm"},{"location":"api-reference/strategy/#aggressivestrategy-parameters","title":"AggressiveStrategy Parameters","text":"Parameter Type Default Description <code>max_tokens</code> <code>int</code> <code>150</code> Maximum tokens to keep <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping <code>similarity_threshold</code> <code>float</code> <code>0.7</code> Threshold for deduplication (0.0-1.0) <code>dedup_method</code> <code>Literal[\"jaccard\", \"levenshtein\"]</code> <code>\"jaccard\"</code> Deduplication algorithm <code>truncate_strategy</code> <code>Literal[\"head\", \"tail\", \"middle_out\"]</code> <code>\"head\"</code> Which part of text to keep"},{"location":"api-reference/strategy/#see-also","title":"See Also","text":"<ul> <li>Examples - Comprehensive examples</li> <li>Benchmark Results - Performance and quality metrics</li> <li>Refiner API - Pipeline composition</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for each module in Prompt Refiner.</p>"},{"location":"examples/#by-module","title":"By Module","text":""},{"location":"examples/#cleaner-examples","title":"Cleaner Examples","text":"<ul> <li>HTML Cleaning - Strip HTML tags and convert to Markdown</li> <li>JSON Cleaning - Strip nulls/empties from JSON for RAG APIs</li> <li>See more in: Cleaner Module</li> </ul>"},{"location":"examples/#compressor-examples","title":"Compressor Examples","text":"<ul> <li>Deduplication - Remove duplicate content from RAG results</li> <li>See more in: Compressor Module</li> </ul>"},{"location":"examples/#scrubber-examples","title":"Scrubber Examples","text":"<ul> <li>PII Redaction - Redact sensitive information</li> <li>See more in: Scrubber Module</li> </ul>"},{"location":"examples/#analyzer-examples","title":"Analyzer Examples","text":"<ul> <li>Token Analysis - Calculate token savings and ROI</li> <li>See more in: Analyzer Module</li> </ul>"},{"location":"examples/#packer-examples-advanced","title":"Packer Examples (Advanced)","text":"<ul> <li>Context Budget Management - RAG applications, chatbots, and conversation history</li> <li>See more in: Packer Module</li> </ul>"},{"location":"examples/#complete-examples","title":"Complete Examples","text":"<ul> <li>Complete Pipeline - Full optimization with all modules</li> </ul>"},{"location":"examples/#running-examples-locally","title":"Running Examples Locally","text":"<p>All examples are available in the <code>examples/</code> directory:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run an example\npython examples/packer/messages_packer.py\n</code></pre>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Getting Started Guide</li> <li>API Reference</li> <li>Report Issues</li> </ul>"},{"location":"examples/complete-pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive example using all 5 modules together.</p>"},{"location":"examples/complete-pipeline/#full-optimization-pipeline","title":"Full Optimization Pipeline","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\n# Messy input with HTML, PII, duplicates\nmessy_input = \"\"\"\n&lt;div&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;We provide excellent service   with   lots   of   spaces.&lt;/p&gt;\n    &lt;p&gt;Our IP address is 192.168.1.1 for reference.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Initialize counter\ncounter = CountTokens(original_text=messy_input)\n\n# Build complete pipeline using pipe operator (recommended)\npipeline = (\n    # Clean dirty data\n    StripHTML()\n    | FixUnicode()\n    | NormalizeWhitespace()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=50, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\", \"ip\"})\n    # Analyze\n    | counter\n)\n\n# Run pipeline\nresult = pipeline.run(messy_input)\n\nprint(\"Optimized result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(counter.format_stats())\n</code></pre> <p>Alternative: Fluent API</p> <p>You can also use <code>.pipe()</code> method chaining: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n    # ... continue with other operations\n)\n</code></pre></p>"},{"location":"examples/complete-pipeline/#related","title":"Related","text":"<ul> <li>Pipeline Basics</li> <li>All Modules Overview</li> </ul>"},{"location":"examples/deduplication/","title":"Deduplication Example","text":"<p>Remove duplicate content from RAG retrieval results.</p>"},{"location":"examples/deduplication/#scenario","title":"Scenario","text":"<p>Your RAG system retrieved multiple similar chunks that contain overlapping information.</p>"},{"location":"examples/deduplication/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# RAG results with duplicates\nrag_results = \"\"\"\nPython is a high-level programming language.\n\nPython is a high level programming language.\n\nPython supports multiple programming paradigms.\n\"\"\"\n\npipeline = Deduplicate(similarity_threshold=0.85)\ndeduplicated = pipeline.run(rag_results)\n\nprint(deduplicated)\n# Output: Only unique paragraphs remain\n</code></pre>"},{"location":"examples/deduplication/#adjusting-sensitivity","title":"Adjusting Sensitivity","text":"<pre><code># More aggressive (70% similarity)\npipeline = Deduplicate(similarity_threshold=0.70)\n\n# Sentence-level deduplication\npipeline = Deduplicate(granularity=\"sentence\")\n</code></pre>"},{"location":"examples/deduplication/#performance-considerations","title":"Performance Considerations","text":"<p>When working with large RAG contexts, keep these performance tips in mind:</p>"},{"location":"examples/deduplication/#choosing-a-similarity-method","title":"Choosing a Similarity Method","text":"<pre><code># Fast: Jaccard (word-based) - recommended for most use cases\npipeline = Deduplicate(method=\"jaccard\")\n\n# Precise but slower: Levenshtein (character-based)\n# Only use when you need character-level accuracy\npipeline = Deduplicate(method=\"levenshtein\")\n</code></pre>"},{"location":"examples/deduplication/#scaling-with-input-size","title":"Scaling with Input Size","text":"<p>The deduplication algorithm compares each chunk against all previous chunks (O(n\u00b2)):</p> <ul> <li>10-50 chunks: Fast with either method (typical RAG use case)</li> <li>50-200 chunks: Use Jaccard for better performance</li> <li>200+ chunks: Use <code>granularity=\"paragraph\"</code> to reduce chunk count</li> </ul> <pre><code># For large documents: use paragraph granularity\npipeline = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\"  # Fewer chunks = faster\n)\n</code></pre>"},{"location":"examples/deduplication/#full-example","title":"Full Example","text":"<p>See: <code>examples/compressor/deduplication.py</code></p>"},{"location":"examples/deduplication/#related","title":"Related","text":"<ul> <li>Deduplicate API Reference</li> <li>Compressor Module Guide</li> </ul>"},{"location":"examples/html-cleaning/","title":"HTML Cleaning Example","text":"<p>Clean HTML content from web scraping or user input.</p>"},{"location":"examples/html-cleaning/#scenario","title":"Scenario","text":"<p>You've scraped content from a website and need to clean it before sending to an LLM API.</p>"},{"location":"examples/html-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\nhtml_content = \"\"\"\n&lt;div class=\"article\"&gt;\n    &lt;h1&gt;Understanding &lt;strong&gt;LLMs&lt;/strong&gt;&lt;/h1&gt;\n    &lt;p&gt;Large Language Models are powerful &lt;em&gt;AI systems&lt;/em&gt;.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Remove all HTML and normalize whitespace\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\ncleaned = pipeline.run(html_content)\nprint(cleaned)\n# Output: \"Understanding LLMs Large Language Models are powerful AI systems.\"\n</code></pre>"},{"location":"examples/html-cleaning/#converting-to-markdown","title":"Converting to Markdown","text":"<pre><code># Convert HTML to Markdown instead of removing\npipeline = (\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n)\n\nmarkdown = pipeline.run(html_content)\nprint(markdown)\n# Output:\n# # Understanding **LLMs**\n#\n# Large Language Models are powerful *AI systems*.\n</code></pre>"},{"location":"examples/html-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/html_cleaning.py</code></p> <pre><code>python examples/cleaner/html_cleaning.py\n</code></pre>"},{"location":"examples/html-cleaning/#related","title":"Related","text":"<ul> <li>StripHTML API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/json-cleaning/","title":"JSON Cleaning Example","text":"<p>Clean and compress JSON from API responses before sending to LLM.</p>"},{"location":"examples/json-cleaning/#scenario","title":"Scenario","text":"<p>You're building a RAG application that fetches documents from an API. The API responses contain many null values and empty fields that waste tokens. You need to compress the JSON before including it in your LLM prompt.</p>"},{"location":"examples/json-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import JsonCleaner\n\napi_response = \"\"\"\n{\n    \"documents\": [\n        {\n            \"id\": 1,\n            \"title\": \"Introduction to LLMs\",\n            \"content\": \"Large Language Models are powerful AI systems...\",\n            \"metadata\": {\n                \"author\": \"Alice\",\n                \"deprecated\": null,\n                \"tags\": []\n            }\n        }\n    ],\n    \"next_page\": null,\n    \"filters\": {}\n}\n\"\"\"\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ncompressed = cleaner.run(api_response)\nprint(compressed)\n# Output: {\"documents\":[{\"id\":1,\"title\":\"Introduction to LLMs\",\"content\":\"Large Language Models are powerful AI systems...\",\"metadata\":{\"author\":\"Alice\"}}]}\n</code></pre> <p>Token savings: 61% reduction (791 \u2192 316 characters)</p>"},{"location":"examples/json-cleaning/#only-minify-keep-all-data","title":"Only Minify (Keep All Data)","text":"<pre><code># Just remove whitespace, keep all data\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nminified = cleaner.run(api_response)\n# Output: {\"documents\":[...],\"next_page\":null,\"filters\":{}}\n</code></pre>"},{"location":"examples/json-cleaning/#rag-pipeline","title":"RAG Pipeline","text":"<pre><code>from prompt_refiner import JsonCleaner, TruncateTokens\n\n# Compress JSON and truncate if still too long\nrag_pipeline = (\n    JsonCleaner(strip_nulls=True, strip_empty=True)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n)\n\ncompressed = rag_pipeline.run(large_api_response)\n</code></pre>"},{"location":"examples/json-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/json_cleaning.py</code></p> <pre><code>python examples/cleaner/json_cleaning.py\n</code></pre>"},{"location":"examples/json-cleaning/#related","title":"Related","text":"<ul> <li>JsonCleaner API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/packer/","title":"Packer Examples","text":"<p>Advanced examples for managing context budgets with MessagesPacker and TextPacker.</p> <p>When to Use Packer</p> <p>Packer is ideal for:</p> <ul> <li>RAG Applications: Pack multiple retrieved documents within token budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"examples/packer/#example-1-basic-rag-with-messagespacker","title":"Example 1: Basic RAG with MessagesPacker","text":"<p>Pack RAG documents for chat APIs with priority-based selection:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_MEDIUM,\n)\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on the provided context only.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with different priorities\npacker.add(\n    \"Product X costs $99 and includes 1-year warranty.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH  # Most relevant document\n)\n\npacker.add(\n    \"We offer free shipping on orders over $50.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM  # Less relevant document\n)\n\npacker.add(\n    \"Customer reviews rate Product X 4.5/5 stars.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM\n)\n\n# User query (must include)\npacker.add(\n    \"What is the price of Product X?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into messages format\nmessages = packer.pack()\n\n# Use directly with OpenAI\n# response = client.chat.completions.create(\n#     model=\"gpt-4\",\n#     messages=messages\n# )\n\nprint(f\"Packed {len(messages)} messages\")\nfor msg in messages:\n    print(f\"{msg['role']}: {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-2-rag-with-dirty-html-jit-refinement","title":"Example 2: RAG with Dirty HTML (JIT Refinement)","text":"<p>Clean web-scraped RAG documents on-the-fly:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    StripHTML,\n    NormalizeWhitespace\n)\n\npacker = MessagesPacker(max_tokens=800)\n\n# System prompt\npacker.add(\n    \"You are a helpful product assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents from web scraping (with HTML)\ndocs = [\n    \"&lt;div class='product'&gt;&lt;h2&gt;Product Features&lt;/h2&gt;&lt;p&gt;  Waterproof   design  &lt;/p&gt;&lt;/div&gt;\",\n    \"&lt;html&gt;&lt;body&gt;   &lt;p&gt;Available in  &lt;b&gt;5 colors&lt;/b&gt;:  red, blue...&lt;/p&gt;  &lt;/body&gt;&lt;/html&gt;\",\n    \"&lt;article&gt;   Battery life:  &lt;strong&gt;  48 hours  &lt;/strong&gt;  continuous use  &lt;/article&gt;\"\n]\n\n# Clean each document before adding (JIT refinement)\nfor doc in docs:\n    packer.add(\n        doc,\n        role=\"system\",\n        priority=PRIORITY_HIGH,\n        refine_with=[StripHTML(), NormalizeWhitespace()]  # Clean on-the-fly!\n    )\n\n# User query\npacker.add(\n    \"What are the key features?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# All HTML is automatically cleaned before packing\nprint(\"Cleaned messages:\")\nfor msg in messages:\n    if msg['role'] == 'system' and 'Waterproof' in msg['content']:\n        print(f\"Before: {docs[0][:50]}...\")\n        print(f\"After:  {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-3-chatbot-with-conversation-history","title":"Example 3: Chatbot with Conversation History","text":"<p>Manage conversation history with priorities - old messages can be dropped:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_LOW,\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (highest priority)\npacker.add(\n    \"You are a helpful customer support agent.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG: Current relevant documentation\npacker.add(\n    \"Return policy: 30-day money-back guarantee for all products.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH\n)\n\n# Old conversation history (can be dropped if budget is tight)\nold_conversation = [\n    {\"role\": \"user\", \"content\": \"What are your business hours?\"},\n    {\"role\": \"assistant\", \"content\": \"We're open 9 AM - 5 PM EST, Monday-Friday.\"},\n    {\"role\": \"user\", \"content\": \"Do you ship internationally?\"},\n    {\"role\": \"assistant\", \"content\": \"Yes, we ship to over 50 countries worldwide.\"}\n]\n\npacker.add_messages(old_conversation, priority=PRIORITY_LOW)\n\n# Current user query (highest priority)\npacker.add(\n    \"What is your return policy?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# If budget is tight, old history is dropped, but system prompt + current query are kept\nprint(f\"Packed {len(messages)} messages (old history may be dropped)\")\n</code></pre>"},{"location":"examples/packer/#example-4-textpacker-for-base-models","title":"Example 4: TextPacker for Base Models","text":"<p>Use TextPacker with Llama or GPT-3 base models:</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\n# Use MARKDOWN format for better structure\npacker = TextPacker(\n    max_tokens=600,\n    text_format=TextFormat.MARKDOWN\n)\n\n# System instructions\npacker.add(\n    \"You are a QA assistant. Answer based on the context provided.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents (no role = treated as context)\npacker.add(\n    \"Prompt-refiner is a Python library for optimizing LLM inputs.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"It reduces token usage by 4-15% through cleaning and compression.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"The library has zero dependencies by default.\",\n    priority=PRIORITY_HIGH\n)\n\n# User query\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into formatted text\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# ### INSTRUCTIONS:\n# You are a QA assistant. Answer based on the context provided.\n#\n# ### CONTEXT:\n# - Prompt-refiner is a Python library for optimizing LLM inputs.\n# - It reduces token usage by 4-15% through cleaning and compression.\n# - The library has zero dependencies by default.\n#\n# ### INPUT:\n# What is prompt-refiner?\n\n# Use with completion API\n# response = client.completions.create(\n#     model=\"llama-2-70b\",\n#     prompt=prompt\n# )\n</code></pre>"},{"location":"examples/packer/#example-5-textpacker-with-xml-format","title":"Example 5: TextPacker with XML Format","text":"<p>Use XML format (Anthropic best practice for Claude base models):</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\npacker = TextPacker(\n    max_tokens=500,\n    text_format=TextFormat.XML\n)\n\npacker.add(\n    \"You are a code review assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"Code snippet: def hello(): return 'world'\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"Please review this code for best practices.\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# &lt;system&gt;\n# You are a code review assistant.\n# &lt;/system&gt;\n#\n# &lt;context&gt;\n# Code snippet: def hello(): return 'world'\n# &lt;/context&gt;\n#\n# &lt;user&gt;\n# Please review this code for best practices.\n# &lt;/user&gt;\n</code></pre>"},{"location":"examples/packer/#example-6-precise-mode-for-maximum-token-utilization","title":"Example 6: Precise Mode for Maximum Token Utilization","text":"<p>Use precise mode with tiktoken for 100% token budget utilization:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER\n\n# Install tiktoken: pip install llm-prompt-refiner[token]\n\n# Estimation mode (default): 10% safety buffer\npacker_estimate = MessagesPacker(max_tokens=1000)\nprint(f\"Estimation mode: {packer_estimate.effective_max_tokens} effective tokens\")\n# Output: Estimation mode: 900 effective tokens\n\n# Precise mode: 100% budget utilization (no safety buffer)\npacker_precise = MessagesPacker(max_tokens=1000, model=\"gpt-4\")\nprint(f\"Precise mode: {packer_precise.effective_max_tokens} effective tokens\")\n# Output: Precise mode: 997 effective tokens (1000 - 3 request overhead)\n\n# Use precise mode for production to maximize token capacity\npacker_precise.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker_precise.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nmessages = packer_precise.pack()\n</code></pre>"},{"location":"examples/packer/#example-7-inspection-and-debugging","title":"Example 7: Inspection and Debugging","text":"<p>Inspect items before packing to understand token distribution:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_HIGH, PRIORITY_USER\n\npacker = MessagesPacker(max_tokens=500)\n\npacker.add(\"System prompt here\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Document 1\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"Document 2\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\n\n# Inspect items before packing\nitems = packer.get_items()\n\nprint(\"Items before packing:\")\nfor i, item in enumerate(items):\n    print(f\"{i+1}. Priority: {item['priority']}, Tokens: {item['tokens']}, Role: {item['role']}\")\n\n# Pack and see which items fit\nmessages = packer.pack()\nprint(f\"\\nPacked {len(messages)}/{len(items)} items\")\n</code></pre>"},{"location":"examples/packer/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Choose the Right Packer:</li> <li><code>MessagesPacker</code> for chat APIs (OpenAI, Anthropic)</li> <li> <p><code>TextPacker</code> for completion APIs (Llama Base, GPT-3)</p> </li> <li> <p>Set Priorities Correctly:</p> </li> <li><code>PRIORITY_SYSTEM</code> (0): System prompts, absolute must-have</li> <li><code>PRIORITY_USER</code> (10): User queries, critical</li> <li><code>PRIORITY_HIGH</code> (20): Core RAG documents</li> <li><code>PRIORITY_MEDIUM</code> (30): Supporting context</li> <li> <p><code>PRIORITY_LOW</code> (40): Old conversation history</p> </li> <li> <p>Use JIT Refinement:</p> </li> <li>Clean dirty documents with <code>refine_with</code> parameter</li> <li> <p>Chain multiple operations: <code>refine_with=[StripHTML(), NormalizeWhitespace()]</code></p> </li> <li> <p>Optimize for Production:</p> </li> <li>Use precise mode with <code>model</code> parameter for 100% token utilization</li> <li>Choose appropriate text format for base models (MARKDOWN recommended)</li> </ol>"},{"location":"examples/packer/#related-documentation","title":"Related Documentation","text":"<ul> <li>Packer Module Guide</li> <li>Packer API Reference</li> <li>Getting Started</li> </ul>"},{"location":"examples/pii-redaction/","title":"PII Redaction Example","text":"<p>Automatically redact sensitive information before sending to APIs.</p>"},{"location":"examples/pii-redaction/#scenario","title":"Scenario","text":"<p>User input contains personal information that should not be sent to external LLM APIs.</p>"},{"location":"examples/pii-redaction/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import RedactPII\n\nuser_input = \"\"\"\nPlease contact me at john.doe@example.com or call 555-123-4567.\nMy account number is EMP-12345.\n\"\"\"\n\npipeline = RedactPII()\nsecure = pipeline.run(user_input)\n\nprint(secure)\n# Output:\n# Please contact me at [EMAIL] or call [PHONE].\n# My account number is EMP-12345.\n</code></pre>"},{"location":"examples/pii-redaction/#custom-patterns","title":"Custom Patterns","text":"<pre><code>pipeline = RedactPII(\n    redact_types={\"email\", \"phone\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n\nsecure = pipeline.run(user_input)\n# Now EMP-12345 is also redacted as [EMPLOYEE_ID]\n</code></pre>"},{"location":"examples/pii-redaction/#full-example","title":"Full Example","text":"<p>See: <code>examples/scrubber/pii_redaction.py</code></p>"},{"location":"examples/pii-redaction/#related","title":"Related","text":"<ul> <li>RedactPII API Reference</li> <li>Scrubber Module Guide</li> </ul>"},{"location":"examples/token-analysis/","title":"Token Analysis Example","text":"<p>Measure optimization impact and calculate cost savings.</p>"},{"location":"examples/token-analysis/#scenario","title":"Scenario","text":"<p>You want to demonstrate the value of prompt optimization.</p>"},{"location":"examples/token-analysis/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   from   HTML  &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | counter\n)\n\nresult = pipeline.run(original_text)\n\n# Show statistics\nprint(counter.format_stats())\n# Output:\n# Original: 10 tokens\n# Cleaned: 4 tokens\n# Saved: 6 tokens (60.0%)\n</code></pre>"},{"location":"examples/token-analysis/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\n\n# GPT-4 pricing: $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\n\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings_per_request = original_cost - cleaned_cost\n\nprint(f\"Savings: ${savings_per_request:.4f} per request\")\n\n# Project annual savings\nrequests_per_day = 10000\nannual_savings = savings_per_request * requests_per_day * 365\nprint(f\"Annual savings: ${annual_savings:.2f}\")\n</code></pre>"},{"location":"examples/token-analysis/#full-example","title":"Full Example","text":"<p>See: <code>examples/analyzer/token_counting.py</code></p>"},{"location":"examples/token-analysis/#related","title":"Related","text":"<ul> <li>CountTokens API Reference</li> <li>Analyzer Module Guide</li> </ul>"},{"location":"modules/analyzer/","title":"Analyzer Module","text":"<p>Track optimization impact and demonstrate value with token counting and statistics.</p>"},{"location":"modules/analyzer/#counttokens-operation","title":"CountTokens Operation","text":"<p>Measure token usage before and after optimization.</p>"},{"location":"modules/analyzer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World&lt;/p&gt;\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"modules/analyzer/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\ncost_per_token = 0.03 / 1000  # GPT-4 pricing\nsavings = stats['saved'] * cost_per_token\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data from various sources.</p>"},{"location":"modules/cleaner/#overview","title":"Overview","text":"<p>When working with real-world text data, you often encounter:</p> <ul> <li>HTML tags from web scraping</li> <li>Excessive whitespace and formatting issues</li> <li>Problematic Unicode characters</li> <li>JSON with null values and empty containers</li> </ul> <p>The Cleaner module addresses these issues efficiently.</p>"},{"location":"modules/cleaner/#operations","title":"Operations","text":""},{"location":"modules/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags or convert them to Markdown.</p> <p>Use cases:</p> <ul> <li>Web scraping</li> <li>Email content processing</li> <li>User-generated HTML content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import StripHTML\n\n# Remove all HTML\ncleaner = StripHTML()\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\ncleaner = StripHTML(to_markdown=True)\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines.</p> <p>Use cases:</p> <ul> <li>Text from PDFs</li> <li>User input normalization</li> <li>Copy-pasted content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import NormalizeWhitespace\n\ncleaner = NormalizeWhitespace()\nresult = cleaner.run(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters.</p> <p>Use cases:</p> <ul> <li>Zero-width spaces from copy-paste</li> <li>Control characters</li> <li>Invisible characters causing issues</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import FixUnicode\n\ncleaner = FixUnicode()\nresult = cleaner.run(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#jsoncleaner","title":"JsonCleaner","text":"<p>Clean and minify JSON by removing null values and empty containers.</p> <p>Use cases:</p> <ul> <li>RAG API responses with null/empty fields</li> <li>Compressing JSON context before LLM input</li> <li>Normalizing inconsistent API data</li> <li>Token optimization for JSON-heavy prompts</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import JsonCleaner\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndirty_json = \"\"\"\n{\n    \"name\": \"Alice\",\n    \"age\": null,\n    \"address\": {},\n    \"tags\": []\n}\n\"\"\"\nresult = cleaner.run(dirty_json)\n# Output: {\"name\":\"Alice\"}\n\n# Only minify (keep all data)\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nresult = cleaner.run(dirty_json)\n# Output: {\"name\":\"Alice\",\"age\":null,\"address\":{},\"tags\":[]}\n</code></pre> <p>Token savings: 50-60% reduction in typical RAG API responses!</p> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/cleaner/#web-content-pipeline","title":"Web Content Pipeline","text":"<pre><code>from prompt_refiner import StripHTML, FixUnicode, NormalizeWhitespace\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import FixUnicode, NormalizeWhitespace\n\nnormalizer = (\n    FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#rag-context-compression","title":"RAG Context Compression","text":"<pre><code>from prompt_refiner import JsonCleaner, TruncateTokens\n\nrag_compressor = (\n    JsonCleaner(strip_nulls=True, strip_empty=True)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n)\n</code></pre>"},{"location":"modules/cleaner/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples</li> <li>Full API Reference</li> <li>Explore Other Modules</li> </ul>"},{"location":"modules/compressor/","title":"Compressor Module","text":"<p>Reduce text size while preserving meaning through smart truncation and deduplication.</p>"},{"location":"modules/compressor/#operations","title":"Operations","text":""},{"location":"modules/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Smart text truncation respecting sentence boundaries.</p> <pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\n\n# Keep last 100 tokens (for conversation history)\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\n\n# Keep beginning and end, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or similar content chunks.</p> <pre><code>from prompt_refiner import Deduplicate\n\n# Remove paragraphs with 85% similarity\ndeduper = Deduplicate(similarity_threshold=0.85)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(granularity=\"sentence\")\n</code></pre> <p>Performance Considerations:</p> <ul> <li>Method Choice: Use <code>jaccard</code> (default) for most cases - it's fast and works well with typical prompts. Only use <code>levenshtein</code> when you need character-level precision.</li> <li>Complexity: Deduplication uses O(n\u00b2) comparisons where n is the number of chunks. For 50 chunks, this is ~1,225 comparisons.</li> <li>Large Inputs: For 200+ chunks, use <code>granularity=\"paragraph\"</code> to reduce chunk count and speed up processing.</li> <li>Jaccard: O(m) per comparison - fast even with long chunks</li> <li>Levenshtein: O(m\u2081 \u00d7 m\u2082) per comparison - can be slow with chunks over 1000 characters</li> </ul> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"modules/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n</code></pre> <p>View Examples</p>"},{"location":"modules/overview/","title":"Modules Overview","text":"<p>Prompt Refiner is organized into 5 core modules plus measurement utilities.</p>"},{"location":"modules/overview/#the-5-core-modules","title":"The 5 Core Modules","text":""},{"location":"modules/overview/#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<p>The Cleaner module removes unwanted artifacts from your text.</p> <p>Operations:</p> <ul> <li>StripHTML - Remove or convert HTML tags</li> <li>NormalizeWhitespace - Collapse excessive whitespace</li> <li>FixUnicode - Remove problematic Unicode characters</li> <li>JsonCleaner - Strip nulls/empties from JSON, minify</li> </ul> <p>When to use:</p> <ul> <li>Processing web-scraped content</li> <li>Cleaning user-generated text</li> <li>Compressing JSON from RAG APIs</li> <li>Normalizing text from various sources</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<p>The Compressor module reduces token count while preserving meaning.</p> <p>Operations:</p> <ul> <li>TruncateTokens - Smart text truncation with sentence boundaries</li> <li>Deduplicate - Remove similar or duplicate content</li> </ul> <p>When to use:</p> <ul> <li>Fitting content within context windows</li> <li>Optimizing RAG retrieval results</li> <li>Reducing API costs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<p>The Scrubber module protects sensitive information.</p> <p>Operations:</p> <ul> <li>RedactPII - Automatically redact personally identifiable information</li> </ul> <p>When to use:</p> <ul> <li>Before sending data to external APIs</li> <li>Compliance with privacy regulations</li> <li>Protecting user data in logs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#4-packer-context-budget-management","title":"4. Packer - Context Budget Management","text":"<p>The Packer module manages context budgets with intelligent priority-based item selection.</p> <p>Operations:</p> <ul> <li>MessagesPacker - Pack items for chat completion APIs</li> <li>TextPacker - Pack items for text completion APIs</li> </ul> <p>When to use:</p> <ul> <li>RAG applications with multiple documents</li> <li>Chatbots with conversation history</li> <li>Managing context windows with size limits</li> <li>Combining system prompts, user input, and documents</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#5-strategy-preset-strategies","title":"5. Strategy - Preset Strategies","text":"<p>The Strategy module provides benchmark-tested preset strategies for quick setup.</p> <p>Strategies:</p> <ul> <li>MinimalStrategy - 4.3% reduction, 98.7% quality</li> <li>StandardStrategy - 4.8% reduction, 98.4% quality</li> <li>AggressiveStrategy - 15% reduction, 96.4% quality</li> </ul> <p>When to use:</p> <ul> <li>Quick setup without manual configuration</li> <li>Benchmark-tested optimization presets</li> <li>Extending with additional custom operations</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#measurement-utilities","title":"Measurement Utilities","text":""},{"location":"modules/overview/#analyzer-measure-impact","title":"Analyzer - Measure Impact","text":"<p>The Analyzer module measures optimization impact but does not transform prompts. Use it to track token savings and demonstrate ROI.</p> <p>Operations:</p> <ul> <li>CountTokens - Measure token savings and calculate ROI</li> </ul> <p>When to use:</p> <ul> <li>Demonstrating cost savings to stakeholders</li> <li>A/B testing optimization strategies</li> <li>Monitoring optimization impact over time</li> <li>Calculating ROI for prompt optimization</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#combining-modules","title":"Combining Modules","text":"<p>The real power comes from combining modules:</p>"},{"location":"modules/overview/#pipeline-example","title":"Pipeline Example","text":"<pre><code>from prompt_refiner import (\n    Refiner,\n    StripHTML, NormalizeWhitespace,  # Cleaner\n    TruncateTokens,                  # Compressor\n    RedactPII,                       # Scrubber\n    CountTokens                      # Analyzer\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    Refiner()\n    # Clean first\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    # Then compress\n    .pipe(TruncateTokens(max_tokens=1000))\n    # Secure\n    .pipe(RedactPII())\n    # Analyze\n    .pipe(counter)\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"modules/overview/#packer-example","title":"Packer Example","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    StripHTML\n)\n\n# Manage RAG context budget for chat APIs\npacker = MessagesPacker(max_tokens=1000)\n\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Clean documents before packing\nfor doc in retrieved_docs:\n    packer.add(\n        doc.content,\n        role=\"system\",\n        priority=PRIORITY_HIGH,\n        refine_with=StripHTML()\n    )\n\nmessages = packer.pack()  # Returns List[Dict] directly\n</code></pre>"},{"location":"modules/overview/#module-relationships","title":"Module Relationships","text":"<pre><code>graph LR\n    A[Raw Input] --&gt; B[Cleaner]\n    B --&gt; C[Compressor]\n    C --&gt; D[Scrubber]\n    D --&gt; E[Optimized Output]\n    E -.-&gt; F[Analyzer&lt;br/&gt;Measurement Only]\n\n    G[Multiple Items] --&gt; H[Packer]\n    H --&gt; I[Packed Context]\n</code></pre> <p>Note: Analyzer (dotted line) measures but doesn't transform the output.</p>"},{"location":"modules/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Clean before compressing, compress before redacting</li> <li>Use Packer for RAG: When managing multiple documents with priorities</li> <li>Test your pipeline: Different inputs may need different operations</li> <li>Measure, don't transform: Use CountTokens to track savings without changing output</li> <li>Start simple: Begin with one module and add more as needed</li> </ol>"},{"location":"modules/packer/","title":"Packer Module","text":"<p>Intelligently manage context budgets with smart priority-based packing for RAG applications and chatbots.</p>"},{"location":"modules/packer/#overview-v013","title":"Overview (v0.1.3+)","text":"<p>The Packer module provides two specialized packers following the Single Responsibility Principle:</p> <ul> <li><code>MessagesPacker</code>: For chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict]</code></li> <li><code>TextPacker</code>: For text completion APIs (Llama Base, GPT-3). Returns <code>str</code></li> </ul> <p>Key Features: - Smart priority-based selection (auto-prioritizes: system &gt; query &gt; context &gt; history) - Semantic roles for clear intent (ROLE_SYSTEM, ROLE_QUERY, ROLE_CONTEXT, ROLE_USER, ROLE_ASSISTANT) - JIT refinement with <code>refine_with</code> parameter - Automatic format overhead calculation</p>"},{"location":"modules/packer/#messagespacker","title":"MessagesPacker","text":"<p>Pack items into chat message format for chat completion APIs.</p>"},{"location":"modules/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=1000)\n\n# Add items with semantic roles (auto-prioritized)\npacker.add(\n    \"You are a helpful assistant.\",\n    role=ROLE_SYSTEM  # Auto: highest priority\n)\n\npacker.add(\n    \"Product documentation: Feature A, B, C...\",\n    role=ROLE_CONTEXT  # Auto: high priority\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=ROLE_QUERY  # Auto: critical priority\n)\n\n# Pack into messages format\nmessages = packer.pack()  # Returns List[Dict[str, str]]\n\n# Use directly with chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"modules/packer/#rag-conversation-history-example","title":"RAG + Conversation History Example","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    ROLE_SYSTEM,\n    ROLE_CONTEXT,\n    ROLE_QUERY,\n    ROLE_USER,\n    ROLE_ASSISTANT,\n    StripHTML\n)\n\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (auto: highest priority)\npacker.add(\n    \"Answer based on the provided context.\",\n    role=ROLE_SYSTEM\n)\n\n# RAG documents with JIT cleaning (auto: high priority)\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n\n# Old conversation history (auto: low priority, can be dropped)\nold_messages = [\n    {\"role\": ROLE_USER, \"content\": \"What is this library?\"},\n    {\"role\": ROLE_ASSISTANT, \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages)\n\n# Current query (auto: critical priority)\npacker.add(\n    \"How does it reduce costs?\",\n    role=ROLE_QUERY\n)\n\n# Pack into messages\nmessages = packer.pack()  # List[Dict[str, str]]\n</code></pre>"},{"location":"modules/packer/#textpacker","title":"TextPacker","text":"<p>Pack items into formatted text for text completion APIs (base models).</p>"},{"location":"modules/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, ROLE_SYSTEM, ROLE_CONTEXT, ROLE_QUERY\n\n# Create packer with MARKDOWN format\npacker = TextPacker(\n    max_tokens=1000,\n    text_format=TextFormat.MARKDOWN\n)\n\n# Add items with semantic roles (auto-prioritized)\npacker.add(\n    \"You are a helpful assistant.\",\n    role=ROLE_SYSTEM  # Auto: highest priority\n)\n\npacker.add(\n    \"Product documentation...\",\n    role=ROLE_CONTEXT  # Auto: high priority\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=ROLE_QUERY  # Auto: critical priority\n)\n\n# Pack into formatted text\nprompt = packer.pack()  # Returns str\n\n# Use with completion APIs\n# response = client.completions.create(prompt=prompt)\n</code></pre>"},{"location":"modules/packer/#text-formats","title":"Text Formats","text":"<p>RAW Format (default): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.RAW)\n# Output: Simple concatenation with separators\n</code></pre></p> <p>MARKDOWN Format (recommended for base models): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Document 1\n# - Document 2\n#\n# ### CONVERSATION:\n# User: Hello\n# Assistant: Hi\n#\n# ### INPUT:\n# Final query\n</code></pre></p> <p>XML Format (Anthropic best practice): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.XML)\n# Output: &lt;role&gt;content&lt;/role&gt; tags\n</code></pre></p>"},{"location":"modules/packer/#rag-example-with-grouped-sections","title":"RAG Example with Grouped Sections","text":"<pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    ROLE_SYSTEM,\n    ROLE_CONTEXT,\n    ROLE_QUERY,\n    StripHTML\n)\n\npacker = TextPacker(max_tokens=500, text_format=TextFormat.MARKDOWN)\n\n# System prompt (auto: highest priority)\npacker.add(\n    \"Answer based on context.\",\n    role=ROLE_SYSTEM\n)\n\n# RAG documents (auto: high priority)\npacker.add(\n    \"&lt;p&gt;Document 1...&lt;/p&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n\npacker.add(\n    \"Document 2...\",\n    role=ROLE_CONTEXT\n)\n\n# User query (auto: critical priority)\npacker.add(\n    \"What is the answer?\",\n    role=ROLE_QUERY\n)\n\nprompt = packer.pack()  # str\n</code></pre>"},{"location":"modules/packer/#semantic-roles-priorities","title":"Semantic Roles &amp; Priorities","text":"<p>Semantic Roles (Recommended): <pre><code>from prompt_refiner import (\n    ROLE_SYSTEM,      # \"system\" - System instructions (auto: PRIORITY_SYSTEM = 0)\n    ROLE_QUERY,       # \"query\" - Current user question (auto: PRIORITY_QUERY = 10)\n    ROLE_CONTEXT,     # \"context\" - RAG documents (auto: PRIORITY_HIGH = 20)\n    ROLE_USER,        # \"user\" - User messages in history (auto: PRIORITY_LOW = 40)\n    ROLE_ASSISTANT,   # \"assistant\" - Assistant messages in history (auto: PRIORITY_LOW = 40)\n)\n</code></pre></p> <p>Priority Constants (Optional): <pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_QUERY,    # 10 - Current user query (critical for response)\n    PRIORITY_HIGH,     # 20 - Important context (core RAG docs)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG docs)\n    PRIORITY_LOW,      # 40 - Optional content (old history)\n)\n</code></pre></p> <p>Use Semantic Roles</p> <p>Semantic roles auto-infer priorities, making code clearer. You usually don't need to specify priority manually!</p>"},{"location":"modules/packer/#common-features","title":"Common Features","text":""},{"location":"modules/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Apply operations before adding items:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, ROLE_CONTEXT\n\npacker.add(\n    \"&lt;div&gt;  Messy   HTML  &lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"modules/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_QUERY\n\nmessages = (\n    MessagesPacker(max_tokens=500)\n    .add(\"System prompt\", role=ROLE_SYSTEM)\n    .add(\"User query\", role=ROLE_QUERY)\n    .pack()\n)\n</code></pre>"},{"location":"modules/packer/#inspection","title":"Inspection","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_SYSTEM, ROLE_QUERY\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"Item 1\", role=ROLE_SYSTEM)\npacker.add(\"Item 2\", role=ROLE_QUERY)\n\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Tokens: {item['tokens']}\")\n</code></pre>"},{"location":"modules/packer/#reset","title":"Reset","text":"<pre><code>from prompt_refiner import MessagesPacker, ROLE_CONTEXT\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"First batch\", role=ROLE_CONTEXT)\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=ROLE_CONTEXT)\nmessages2 = packer.pack()\n</code></pre>"},{"location":"modules/packer/#how-it-works","title":"How It Works","text":"<ol> <li>Add items with priorities, roles, and optional JIT refinement</li> <li>Sort by priority (lower number = higher priority)</li> <li>Greedy packing - select items that fit within budget</li> <li>Restore insertion order for natural reading flow</li> <li>Format output:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code></li> <li>TextPacker: Returns <code>str</code> (formatted based on text_format)</li> </ol>"},{"location":"modules/packer/#token-overhead-optimization","title":"Token Overhead Optimization","text":""},{"location":"modules/packer/#messagespacker_1","title":"MessagesPacker","text":"<ul> <li>Pre-calculates ChatML format overhead (~4 tokens per message)</li> <li>100% token budget utilization in precise mode</li> </ul>"},{"location":"modules/packer/#textpacker-markdown","title":"TextPacker (MARKDOWN)","text":"<ul> <li>\"Entrance fee\" strategy: Pre-reserves 30 tokens for section headers</li> <li>Marginal costs: Only counts bullet points and newlines per item</li> <li>Result: Fits more documents compared to per-item header calculation</li> </ul>"},{"location":"modules/packer/#use-cases","title":"Use Cases","text":"<ul> <li>RAG Applications: Pack retrieved documents into context budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"modules/packer/#new-in-v013","title":"New in v0.1.3","text":"<p>The Packer module now provides two specialized packers:</p> <pre><code>from prompt_refiner import MessagesPacker, TextPacker\n\n# For chat APIs (OpenAI, Anthropic)\nmessages_packer = MessagesPacker(max_tokens=1000)\nmessages = messages_packer.pack()  # List[Dict[str, str]]\n\n# For completion APIs (Llama Base, GPT-3)\ntext_packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\ntext = text_packer.pack()  # str\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/scrubber/","title":"Scrubber Module","text":"<p>Protect sensitive information with automatic PII redaction.</p>"},{"location":"modules/scrubber/#redactpii-operation","title":"RedactPII Operation","text":"<p>Automatically redact personally identifiable information using regex patterns.</p>"},{"location":"modules/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code> - Email addresses</li> <li><code>phone</code> - Phone numbers</li> <li><code>ip</code> - IP addresses</li> <li><code>credit_card</code> - Credit card numbers</li> <li><code>ssn</code> - Social Security Numbers</li> <li><code>url</code> - URLs</li> </ul>"},{"location":"modules/scrubber/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact john@example.com or 555-123-4567\")\n# Output: \"Contact [EMAIL] or [PHONE]\"\n\n# Redact specific types\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\n</code></pre>"},{"location":"modules/scrubber/#custom-patterns","title":"Custom Patterns","text":"<pre><code>redactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"user-guide/custom-operations/","title":"Custom Operations","text":"<p>Create your own operations to extend Prompt Refiner.</p>"},{"location":"user-guide/custom-operations/#creating-a-custom-operation","title":"Creating a Custom Operation","text":"<p>All operations inherit from the <code>Operation</code> base class and implement the <code>process</code> method:</p> <pre><code>from prompt_refiner import Operation\n\nclass RemoveEmojis(Operation):\n    \"\"\"Remove emoji characters from text.\"\"\"\n\n    def process(self, text: str) -&gt; str:\n        import re\n        # Simple emoji removal pattern\n        emoji_pattern = re.compile(\n            \"[\"\n            \"\\U0001F600-\\U0001F64F\"  # emoticons\n            \"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n            \"]+\", flags=re.UNICODE\n        )\n        return emoji_pattern.sub(\"\", text)\n</code></pre>"},{"location":"user-guide/custom-operations/#using-your-custom-operation","title":"Using Your Custom Operation","text":"<p>Use it like any built-in operation:</p> <pre><code>from prompt_refiner import Refiner, NormalizeWhitespace\n\npipeline = (\n    Refiner()\n    .pipe(RemoveEmojis())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = pipeline.run(\"Hello \ud83d\ude00 World \ud83c\udf0d!\")\n# Output: \"Hello World !\"\n</code></pre>"},{"location":"user-guide/custom-operations/#more-examples","title":"More Examples","text":""},{"location":"user-guide/custom-operations/#remove-urls","title":"Remove URLs","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveURLs(Operation):\n    def process(self, text: str) -&gt; str:\n        url_pattern = r'https?://\\S+|www\\.\\S+'\n        return re.sub(url_pattern, '[URL]', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#lowercase-text","title":"Lowercase Text","text":"<pre><code>from prompt_refiner import Operation\n\nclass Lowercase(Operation):\n    def process(self, text: str) -&gt; str:\n        return text.lower()\n</code></pre>"},{"location":"user-guide/custom-operations/#remove-numbers","title":"Remove Numbers","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveNumbers(Operation):\n    def process(self, text: str) -&gt; str:\n        return re.sub(r'\\d+', '', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#guidelines","title":"Guidelines","text":"<ol> <li>Single responsibility - Each operation should do one thing well</li> <li>Immutable - Don't modify the input, return a new string</li> <li>Deterministic - Same input should always produce same output</li> <li>Document - Add docstrings explaining what it does</li> </ol>"},{"location":"user-guide/custom-operations/#contributing","title":"Contributing","text":"<p>Have a useful operation? Consider contributing it to Prompt Refiner!</p> <p>See contributing guide \u2192</p>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Learn how to use Prompt Refiner effectively to optimize your LLM inputs.</p>"},{"location":"user-guide/overview/#what-is-prompt-refiner","title":"What is Prompt Refiner?","text":"<p>Prompt Refiner is a library for cleaning and optimizing text before sending it to LLM APIs. It helps you:</p> <ul> <li>Save money by reducing token usage</li> <li>Improve quality by cleaning and normalizing text</li> <li>Enhance security by redacting PII</li> <li>Track value by measuring optimization impact</li> </ul>"},{"location":"user-guide/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/overview/#operations","title":"Operations","text":"<p>An Operation is a single transformation that processes text:</p> <pre><code>from prompt_refiner import StripHTML\n\noperation = StripHTML()\nresult = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n# Output: \"Hello\"\n</code></pre> <p>All operations implement the same interface: <code>process(text: str) -&gt; str</code></p>"},{"location":"user-guide/overview/#pipelines","title":"Pipelines","text":"<p>A Pipeline chains multiple operations together:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Using the pipe operator (recommended)\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\nresult = pipeline.run(\"&lt;p&gt;Hello    World&lt;/p&gt;\")\n# Output: \"Hello World\"\n</code></pre> <p>Alternatively, use the fluent API: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"user-guide/overview/#the-4-modules","title":"The 4 Modules","text":"<ul> <li>Cleaner - Clean dirty data</li> <li>Compressor - Reduce size</li> <li>Scrubber - Security &amp; privacy</li> <li>Analyzer - Track metrics</li> </ul>"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about pipelines</li> <li>Create custom operations</li> <li>Browse examples</li> </ul>"},{"location":"user-guide/pipelines/","title":"Pipeline Basics","text":"<p>Learn how to build effective pipelines with Prompt Refiner.</p>"},{"location":"user-guide/pipelines/#two-ways-to-build-pipelines","title":"Two Ways to Build Pipelines","text":"<p>Prompt Refiner supports two syntax options for building pipelines:</p>"},{"location":"user-guide/pipelines/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<p>The pipe operator (<code>|</code>) provides a clean, Pythonic syntax similar to LangChain:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More concise - no need to import or instantiate <code>Refiner()</code> - Familiar to LangChain, LangGraph, and modern Python framework users - Cleaner visual appearance</p>"},{"location":"user-guide/pipelines/#fluent-api","title":"Fluent API","text":"<p>The fluent API uses method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(TruncateTokens(max_tokens=1000))\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More explicit - clear that you're creating a Refiner pipeline - Traditional method chaining pattern</p> <p>Choose One Style</p> <p>Pick one syntax style per project and use it consistently. Both work identically under the hood. Don't mix styles in the same pipeline.</p>"},{"location":"user-guide/pipelines/#the-pipeline-pattern","title":"The Pipeline Pattern","text":"<p>A pipeline chains operations in sequence:</p> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre> <p>All operations process the text in order, with each operation's output becoming the next operation's input.</p>"},{"location":"user-guide/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<ol> <li>Text enters the pipeline</li> <li>Each operation processes it in order</li> <li>Output of one operation becomes input of the next</li> <li>Final result is returned</li> </ol> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre>"},{"location":"user-guide/pipelines/#order-matters","title":"Order Matters","text":"<p>Operations run in the order you add them:</p> <pre><code># \u2705 Correct: Clean HTML first, then normalize\npipeline = StripHTML() | NormalizeWhitespace()\n\n# \u274c Wrong order - normalizes first, HTML remains\npipeline = NormalizeWhitespace() | StripHTML()\n</code></pre>"},{"location":"user-guide/pipelines/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/pipelines/#1-clean-before-compressing","title":"1. Clean Before Compressing","text":"<pre><code>pipeline = (\n    StripHTML()                  # Clean first\n    | NormalizeWhitespace()\n    | TruncateTokens()           # Then compress\n)\n</code></pre>"},{"location":"user-guide/pipelines/#2-compress-before-redacting","title":"2. Compress Before Redacting","text":"<pre><code>pipeline = (\n    TruncateTokens()  # Compress first\n    | RedactPII()     # Then redact\n)\n</code></pre>"},{"location":"user-guide/pipelines/#3-analyze-last","title":"3. Analyze Last","text":"<pre><code>counter = CountTokens(original_text=text)\npipeline = (\n    StripHTML()\n    | TruncateTokens()\n    | counter  # Analyze at the end\n)\n</code></pre>"},{"location":"user-guide/pipelines/#multiple-pipelines","title":"Multiple Pipelines","text":"<p>Create different pipelines for different use cases:</p> <pre><code># Pipeline for web content\nweb_pipeline = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n\n# Pipeline for RAG\nrag_pipeline = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n\n# Pipeline for secure processing\nsecure_pipeline = RedactPII()\n\n# Use them\ncleaned_web = web_pipeline.run(html_content)\noptimized_rag = rag_pipeline.run(rag_context)\nsafe_text = secure_pipeline.run(user_input)\n</code></pre> <p>Learn about custom operations \u2192</p>"}]}