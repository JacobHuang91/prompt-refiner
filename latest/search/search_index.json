{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prompt Refiner","text":"<p>A lightweight Python library for optimizing and cleaning LLM inputs. Reduce token usage, improve prompt quality, and lower API costs.</p>"},{"location":"#overview","title":"Overview","text":"<p>Prompt Refiner helps you clean and optimize prompts before sending them to LLM APIs. By removing unnecessary whitespace, duplicate characters, and other inefficiencies, you can:</p> <ul> <li>Reduce token usage and API costs - Remove unnecessary characters and content</li> <li>Improve prompt quality - Clean HTML, fix Unicode issues, normalize whitespace</li> <li>Enhance security - Redact PII automatically before sending to APIs</li> <li>Track optimization value - Measure token savings and cost reductions</li> </ul> <p>Proven Effectiveness</p> <p>Benchmarked on 30 real-world test cases, Prompt Refiner achieves 4-15% token reduction while maintaining 96-99% quality. Aggressive optimization can save up to ~$54/month on GPT-4 at scale (1M tokens/month).</p> <p>Processing overhead is &lt; 0.5ms per 1k tokens - negligible compared to network and LLM latency.</p> <p>See benchmark results \u2192</p>"},{"location":"#status","title":"Status","text":"<p>Early Development</p> <p>This project is in early development. Features are being added iteratively.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Build custom cleaning pipelines with the pipe operator:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\n# Define a cleaning pipeline\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000, strategy=\"middle_out\")\n)\n\nraw_input = \"&lt;div&gt;  User input with &lt;b&gt;lots&lt;/b&gt; of   spaces... &lt;/div&gt;\"\nclean_prompt = pipeline.run(raw_input)\n# Output: \"User input with lots of spaces...\"\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"#4-core-modules","title":"4 Core Modules","text":"<p>Prompt Refiner is organized into 4 specialized modules:</p>"},{"location":"#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<ul> <li>StripHTML() - Remove HTML tags, convert to Markdown</li> <li>NormalizeWhitespace() - Collapse excessive whitespace</li> <li>FixUnicode() - Remove zero-width spaces and problematic Unicode</li> </ul> <p>Learn more about Cleaner \u2192</p>"},{"location":"#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<ul> <li>TruncateTokens() - Smart truncation with sentence boundaries<ul> <li>Strategies: <code>\"head\"</code>, <code>\"tail\"</code>, <code>\"middle_out\"</code></li> </ul> </li> <li>Deduplicate() - Remove similar content (great for RAG)</li> </ul> <p>Learn more about Compressor \u2192</p>"},{"location":"#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<ul> <li>RedactPII() - Automatically redact emails, phones, IPs, credit cards, URLs, SSNs</li> </ul> <p>Learn more about Scrubber \u2192</p>"},{"location":"#4-analyzer-show-value","title":"4. Analyzer - Show Value","text":"<ul> <li>CountTokens() - Track token savings and optimization impact</li> </ul> <p>Learn more about Analyzer \u2192</p>"},{"location":"#complete-example","title":"Complete Example","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\noriginal_text = \"\"\"Your messy input here...\"\"\"\n\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    # Clean\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n    | FixUnicode()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\"})\n    # Analyze\n    | counter\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())  # Shows token savings\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Get Started</p> <p>Install Prompt Refiner and build your first pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting Started</p> </li> <li> <p>API Reference</p> <p>Complete API documentation for all operations and modules</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>Examples</p> <p>Browse practical examples for each module</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>Contributing</p> <p>Learn how to contribute to the project</p> <p>:octicons-arrow-right-24: Contributing Guide</p> </li> </ul>"},{"location":"benchmark/","title":"Benchmark Results","text":"<p>Prompt Refiner's effectiveness has been validated through comprehensive testing covering both quality &amp; cost savings and performance &amp; latency.</p>"},{"location":"benchmark/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"benchmark/#quality-cost-benchmark","title":"\ud83c\udfaf Quality &amp; Cost Benchmark","text":"<p>Comprehensive A/B testing on 30 real-world test cases measuring token reduction and response quality.</p> <p>Jump to Quality Benchmark \u2192</p>"},{"location":"benchmark/#latency-benchmark","title":"\u26a1 Latency Benchmark","text":"<p>Performance testing measuring processing overhead of refining operations.</p> <p>Jump to Latency Benchmark \u2192</p>"},{"location":"benchmark/#quality-cost-results","title":"Quality &amp; Cost Results","text":"<p>The benchmark measures two critical factors:</p> <ul> <li>Token Reduction - How much we can reduce prompt size (cost savings)</li> <li>Response Quality - Whether responses remain semantically equivalent</li> </ul> <p>Quality is evaluated using two methods: 1. Cosine Similarity - Semantic similarity of response embeddings (0-1 scale) 2. LLM Judge - GPT-4 evaluation of response equivalence</p>"},{"location":"benchmark/#results-summary","title":"Results Summary","text":"<p>We tested 3 optimization strategies on 30 test cases (15 SQuAD Q&amp;A pairs + 15 RAG scenarios):</p> Strategy Token Reduction Quality (Cosine) Judge Approval Overall Equivalent Minimal 4.3% 0.987 86.7% 86.7% Standard 4.8% 0.984 90.0% 86.7% Aggressive 15.0% 0.964 80.0% 66.7%"},{"location":"benchmark/#strategy-definitions","title":"Strategy Definitions","text":"<p>Minimal (Conservative cleaning): <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p> <p>Standard (Recommended for most use cases): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p> <p>Aggressive (Maximum savings): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150, strategy=\"head\")\n)\n</code></pre></p>"},{"location":"benchmark/#key-findings","title":"Key Findings","text":""},{"location":"benchmark/#standard-strategy-best-balance","title":"\ud83c\udfaf Standard Strategy: Best Balance","text":"<p>The Standard strategy offers the best balance: - 4.8% token reduction with minimal quality impact - 90% judge approval - highest among all strategies - 0.984 cosine similarity - nearly perfect semantic preservation</p>"},{"location":"benchmark/#cost-savings","title":"\ud83d\udcb0 Cost Savings","text":"<p>Real-world cost savings for production applications:</p> GPT-4 TurboGPT-4 <p>Input cost: $0.01 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $4.30 $4.80 $15.00 1M tokens/month $43 $48 $150 10M tokens/month $430 $480 $1,500 <p>Input cost: $0.03 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $13 $14 $45 1M tokens/month $129 $144 $450 10M tokens/month $1,290 $1,440 $4,500"},{"location":"benchmark/#performance-by-scenario","title":"\ud83d\udcca Performance by Scenario","text":"<p>RAG Scenarios (with duplicates and HTML): - Minimal: 17% reduction on average - Standard: 31% reduction on average - Aggressive: 49% reduction on complex documents</p> <p>SQuAD Q&amp;A (clean academic text): - All strategies: 2-5% reduction (less messy data = less to clean)</p> <p>Key Insight</p> <p>Token savings scale with input messiness. RAG contexts with HTML, duplicates, and whitespace see 3-10x more reduction than clean text.</p>"},{"location":"benchmark/#visualizations","title":"Visualizations","text":""},{"location":"benchmark/#token-reduction-vs-quality","title":"Token Reduction vs Quality","text":"<p>The scatter plot shows each strategy's position in the cost-quality tradeoff space. Standard strategy achieves near-optimal quality while maintaining solid savings.</p>"},{"location":"benchmark/#test-dataset","title":"Test Dataset","text":"<p>The benchmark uses 30 carefully curated test cases:</p>"},{"location":"benchmark/#squad-samples-15-cases","title":"SQuAD Samples (15 cases)","text":"<p>Question-answer pairs with context covering: - History (\"When did Beyonce start becoming popular?\") - Science (\"What is DNA?\") - Geography, literature, technology</p>"},{"location":"benchmark/#rag-scenarios-15-cases","title":"RAG Scenarios (15 cases)","text":"<p>Realistic retrieval-augmented generation use cases: - E-commerce product catalogs with HTML - Documentation with excessive whitespace - Customer support tickets with duplicates - Code search results - Recipe collections</p>"},{"location":"benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>Want to validate these results yourself?</p>"},{"location":"benchmark/#prerequisites","title":"Prerequisites","text":"<pre><code># Install benchmark dependencies\nuv pip install -e \".[benchmark]\"\n\n# Set up OpenAI API key\ncd benchmark/custom\ncp .env.example .env\n# Edit .env and add your OPENAI_API_KEY\n</code></pre>"},{"location":"benchmark/#run-the-benchmark","title":"Run the Benchmark","text":"<pre><code>cd benchmark/custom\npython benchmark.py\n</code></pre> <p>This will: 1. Test 30 cases with 3 strategies (90 total comparisons) 2. Generate detailed report with visualizations 3. Save results to <code>./results/</code> directory</p> <p>Estimated cost: ~$2-5 per full run (using gpt-4o-mini)</p>"},{"location":"benchmark/#advanced-options","title":"Advanced Options","text":"<pre><code># Use a different model\npython benchmark.py --model gpt-4o\n\n# Test specific strategies only\npython benchmark.py --strategies minimal standard\n\n# Use fewer test cases (faster, cheaper)\npython benchmark.py --limit 10\n</code></pre>"},{"location":"benchmark/#recommendations","title":"Recommendations","text":"<p>Based on benchmark results:</p>"},{"location":"benchmark/#for-production-rag-applications","title":"For Production RAG Applications","text":"<p>Use Standard strategy - Best balance of savings and quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p>"},{"location":"benchmark/#for-high-volume-cost-sensitive-applications","title":"For High-Volume, Cost-Sensitive Applications","text":"<p>Consider Aggressive strategy if 15% cost reduction outweighs slightly lower quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150)\n)\n</code></pre></p>"},{"location":"benchmark/#for-quality-critical-applications","title":"For Quality-Critical Applications","text":"<p>Use Minimal strategy for maximum quality preservation <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p>"},{"location":"benchmark/#latency-performance","title":"Latency &amp; Performance","text":"<p>\"What's the latency overhead?\" - Negligible. Prompt Refiner adds &lt; 0.5ms per 1k tokens of overhead.</p>"},{"location":"benchmark/#performance-results","title":"Performance Results","text":"Strategy @ 1k tokens @ 10k tokens @ 50k tokens Overhead per 1k tokens Minimal (HTML + Whitespace) 0.05ms 0.48ms 2.39ms 0.05ms Standard (+ Deduplicate) 0.26ms 2.47ms 12.27ms 0.25ms Aggressive (+ Truncate) 0.26ms 2.46ms 12.38ms 0.25ms"},{"location":"benchmark/#key-performance-insights","title":"Key Performance Insights","text":"<ul> <li>\u26a1 Minimal strategy: Only 0.05ms per 1k tokens (faster than a network packet)</li> <li>\ud83c\udfaf Standard strategy: 0.25ms per 1k tokens - adds ~2.5ms to a 10k token prompt</li> <li>\ud83d\udcca Context: Network + LLM TTFT is typically 600ms+, refining adds &lt; 0.5% overhead</li> <li>\ud83d\ude80 Individual operations (HTML, whitespace) are &lt; 0.5ms per 1k tokens</li> </ul>"},{"location":"benchmark/#real-world-impact","title":"Real-World Impact","text":"<pre><code>10k token RAG context refining: ~2.5ms overhead\nNetwork latency: ~100ms\nLLM Processing (TTFT): ~500ms+\nTotal overhead: &lt; 0.5% of request time\n</code></pre> <p>Performance Takeaway</p> <p>Refining overhead is negligible compared to network + LLM latency (600ms+). Standard refining adds ~2.5ms overhead - less than 0.5% of total request time.</p>"},{"location":"benchmark/#running-the-latency-benchmark","title":"Running the Latency Benchmark","text":"<p>The latency benchmark requires no API keys and runs completely offline:</p> <pre><code>cd benchmark/latency\npython benchmark.py\n</code></pre> <p>This will: 1. Test individual operations at multiple scales (1k, 10k, 50k tokens) 2. Test complete strategies (Minimal, Standard, Aggressive) 3. Report average, median, and P95 latency metrics 4. Show per-1k-token normalized overhead</p> <p>Cost: $0 (runs locally, no API calls)</p> <p>Duration: ~30-60 seconds</p>"},{"location":"benchmark/#learn-more","title":"Learn More","text":"<ul> <li>View Quality Benchmark Documentation</li> <li>View Latency Benchmark Documentation</li> <li>Browse Test Cases</li> <li>Examine Raw Results</li> </ul>"},{"location":"benchmark/#contributing","title":"Contributing","text":"<p>Have ideas to improve the benchmark? We welcome: - New test cases (especially domain-specific scenarios) - Additional evaluation metrics - Alternative refining strategies - Multi-model comparisons</p> <p>Open an issue or submit a PR!</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Prompt Refiner!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>uv package manager</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run tests\nmake test\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>prompt-refiner/\n\u251c\u2500\u2500 src/prompt_refiner/     # Source code\n\u2502   \u251c\u2500\u2500 cleaner/           # Cleaner module\n\u2502   \u251c\u2500\u2500 compressor/        # Compressor module\n\u2502   \u251c\u2500\u2500 scrubber/          # Scrubber module\n\u2502   \u2514\u2500\u2500 analyzer/          # Analyzer module\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 examples/              # Example scripts\n\u2514\u2500\u2500 docs/                  # Documentation\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following existing patterns</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_cleaner.py -v\n</code></pre>"},{"location":"contributing/#4-format-code","title":"4. Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"contributing/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write clear docstrings (Google style)</li> <li>Keep functions small and focused</li> </ul>"},{"location":"contributing/#example","title":"Example","text":"<pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text\n    \"\"\"\n    return text.strip()\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Aim for high test coverage</li> <li>Test edge cases</li> </ul> <pre><code>def test_strip_html():\n    operation = StripHTML()\n    result = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n    assert result == \"Hello\"\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update relevant documentation files</li> <li>Add examples for new features</li> <li>Keep API reference up to date (auto-generated from docstrings)</li> </ul>"},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code>make docs-serve\n</code></pre> <p>Then visit http://127.0.0.1:8000</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with Prompt Refiner in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"Using uv (recommended)Using pip <pre><code>uv pip install prompt-refiner\n</code></pre> <pre><code>pip install prompt-refiner\n</code></pre>"},{"location":"getting-started/#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's create a simple pipeline to clean HTML and normalize whitespace:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Process some text\nraw_input = \"\"\"\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Welcome&lt;/h1&gt;\n        &lt;p&gt;This  has    excessive   spaces.&lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nclean_output = pipeline.run(raw_input)\nprint(clean_output)\n# Output: \"Welcome This has excessive spaces.\"\n</code></pre>"},{"location":"getting-started/#understanding-the-pipeline-pattern","title":"Understanding the Pipeline Pattern","text":"<p>Prompt Refiner uses a pipeline pattern where you chain operations together:</p> <ol> <li>Create operations - Initialize the operations you need</li> <li>Chain with <code>|</code> operator - Combine operations in order</li> <li>Run with <code>.run()</code> - Execute the pipeline on your text</li> </ol> <pre><code>pipeline = (\n    Operation1()            # 1. Create operations\n    | Operation2()          # 2. Chain with | operator\n    | Operation3()\n)\n\nresult = pipeline.run(text)  # 3. Run\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use the traditional fluent API with <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(Operation3())\n)\n</code></pre></p> <p>Order Matters</p> <p>Operations run in the order you add them. For example, you should typically clean HTML before normalizing whitespace.</p>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#pattern-1-web-content-cleaning","title":"Pattern 1: Web Content Cleaning","text":"<p>Clean content scraped from the web:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)  # Convert to Markdown\n    | FixUnicode()               # Fix Unicode issues\n    | NormalizeWhitespace()      # Normalize spaces\n)\n</code></pre>"},{"location":"getting-started/#pattern-2-rag-context-optimization","title":"Pattern 2: RAG Context Optimization","text":"<p>Optimize retrieved context for RAG applications:</p> <pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate(similarity_threshold=0.85)  # Remove duplicates\n    | TruncateTokens(max_tokens=2000)       # Fit in context window\n)\n</code></pre>"},{"location":"getting-started/#pattern-3-secure-pii-handling","title":"Pattern 3: Secure PII Handling","text":"<p>Redact sensitive information before sending to APIs:</p> <pre><code>from prompt_refiner import RedactPII\n\nsecure_pipeline = RedactPII(redact_types={\"email\", \"phone\", \"ssn\"})\n</code></pre>"},{"location":"getting-started/#pattern-4-full-optimization-with-tracking","title":"Pattern 4: Full Optimization with Tracking","text":"<p>Complete optimization with metrics:</p> <pre><code>from prompt_refiner import (\n    StripHTML, NormalizeWhitespace,\n    TruncateTokens, RedactPII, CountTokens\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\nfull_pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n    | RedactPII()\n    | counter\n)\n\nresult = full_pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"getting-started/#proven-results","title":"Proven Results","text":"<p>Curious about the real-world effectiveness? Check out our comprehensive benchmark results:</p> <p>Benchmark Highlights</p> <ul> <li>4-15% token reduction across 30 test cases</li> <li>96-99% quality preservation (cosine similarity + LLM judge)</li> <li>Real cost savings: $48-$150/month per 1M tokens</li> </ul> <p>View Full Benchmark \u2192</p>"},{"location":"getting-started/#exploring-modules","title":"Exploring Modules","text":"<p>Prompt Refiner has 4 specialized modules:</p> <ul> <li>Cleaner - Clean dirty data (HTML, whitespace, Unicode)</li> <li>Compressor - Reduce size (truncation, deduplication)</li> <li>Scrubber - Security and privacy (PII redaction)</li> <li>Analyzer - Metrics and analysis (token counting)</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn the Modules</p> <p>Deep dive into each of the 4 core modules</p> <p>:octicons-arrow-right-24: Modules Overview</p> </li> <li> <p>Browse Examples</p> <p>See practical examples for each operation</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for all Prompt Refiner classes and operations.</p> <p>This section contains auto-generated documentation from the codebase docstrings. All operations inherit from the base <code>Operation</code> class and implement a <code>process(text: str) -&gt; str</code> method.</p>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":"<ul> <li> <p>:material-pipe:{ .lg .middle } Refiner</p> <p>Pipeline builder for chaining operations</p> <p>:octicons-arrow-right-24: Refiner API</p> </li> <li> <p>:material-broom:{ .lg .middle } Cleaner</p> <p>Operations for cleaning dirty data</p> <p>:octicons-arrow-right-24: Cleaner API</p> </li> <li> <p>:material-compress:{ .lg .middle } Compressor</p> <p>Operations for reducing size</p> <p>:octicons-arrow-right-24: Compressor API</p> </li> <li> <p>:material-shield-lock:{ .lg .middle } Scrubber</p> <p>Operations for security and privacy</p> <p>:octicons-arrow-right-24: Scrubber API</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Analyzer</p> <p>Operations for metrics and analysis</p> <p>:octicons-arrow-right-24: Analyzer API</p> </li> </ul>"},{"location":"api-reference/#operation-base-class","title":"Operation Base Class","text":"<p>All operations in Prompt Refiner inherit from the <code>Operation</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Operation(ABC):\n    @abstractmethod\n    def process(self, text: str) -&gt; str:\n        \"\"\"Process the input text and return the result.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#usage-pattern","title":"Usage Pattern","text":"<p>Operations are used within a <code>Refiner</code> pipeline:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = refiner.run(\"Your text here...\")\n</code></pre>"},{"location":"api-reference/#module-organization","title":"Module Organization","text":"<ul> <li>Refiner - Core pipeline builder class</li> <li>Cleaner - <code>StripHTML</code>, <code>NormalizeWhitespace</code>, <code>FixUnicode</code></li> <li>Compressor - <code>TruncateTokens</code>, <code>Deduplicate</code></li> <li>Scrubber - <code>RedactPII</code></li> <li>Analyzer - <code>CountTokens</code></li> </ul>"},{"location":"api-reference/analyzer/","title":"Analyzer Module","text":"<p>The Analyzer module provides operations for measuring optimization impact and tracking metrics.</p>"},{"location":"api-reference/analyzer/#counttokens","title":"CountTokens","text":"<p>Count tokens and provide before/after statistics to demonstrate optimization value.</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens","title":"prompt_refiner.analyzer.CountTokens","text":"<pre><code>CountTokens(original_text=None)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Count tokens and provide statistics before/after processing.</p> <p>Initialize the token counter.</p> <p>Parameters:</p> Name Type Description Default <code>original_text</code> <code>Optional[str]</code> <p>Optional original text to compare against</p> <code>None</code> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def __init__(self, original_text: Optional[str] = None):\n    \"\"\"\n    Initialize the token counter.\n\n    Args:\n        original_text: Optional original text to compare against\n    \"\"\"\n    self.original_text = original_text\n    self._stats: Optional[dict] = None\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens-functions","title":"Functions","text":""},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Count tokens in the text and store statistics.</p> <p>This operation doesn't modify the text, it just analyzes it.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>The same text (unchanged)</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Count tokens in the text and store statistics.\n\n    This operation doesn't modify the text, it just analyzes it.\n\n    Args:\n        text: The input text\n\n    Returns:\n        The same text (unchanged)\n    \"\"\"\n    current_tokens = self._estimate_tokens(text)\n\n    if self.original_text is not None:\n        original_tokens = self._estimate_tokens(self.original_text)\n        saved_tokens = original_tokens - current_tokens\n        saving_percent = (saved_tokens / original_tokens * 100) if original_tokens &gt; 0 else 0\n\n        self._stats = {\n            \"original\": original_tokens,\n            \"cleaned\": current_tokens,\n            \"saved\": saved_tokens,\n            \"saving_percent\": f\"{saving_percent:.1f}%\",\n        }\n    else:\n        self._stats = {\n            \"tokens\": current_tokens,\n        }\n\n    return text\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.get_stats","title":"get_stats","text":"<pre><code>get_stats()\n</code></pre> <p>Get token statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing token statistics</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get token statistics.\n\n    Returns:\n        Dictionary containing token statistics\n    \"\"\"\n    return self._stats or {}\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.CountTokens.format_stats","title":"format_stats","text":"<pre><code>format_stats()\n</code></pre> <p>Format statistics as a human-readable string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted statistics string</p> Source code in <code>src/prompt_refiner/analyzer/counter.py</code> <pre><code>def format_stats(self) -&gt; str:\n    \"\"\"\n    Format statistics as a human-readable string.\n\n    Returns:\n        Formatted statistics string\n    \"\"\"\n    if not self._stats:\n        return \"No statistics available\"\n\n    if \"original\" in self._stats:\n        return (\n            f\"Original: {self._stats['original']} tokens\\n\"\n            f\"Cleaned: {self._stats['cleaned']} tokens\\n\"\n            f\"Saved: {self._stats['saved']} tokens ({self._stats['saving_percent']})\"\n        )\n    else:\n        return f\"Tokens: {self._stats['tokens']}\"\n</code></pre>"},{"location":"api-reference/analyzer/#token-estimation","title":"Token Estimation","text":"<p>Token Estimation Method</p> <p>CountTokens uses a simple approximation: ~1 token per word. This is fast but not as accurate as actual tokenizer libraries like <code>tiktoken</code>.</p> <p>For more accurate token counting in production, consider:</p> <ul> <li>Using <code>tiktoken</code> for OpenAI models</li> <li>Using model-specific tokenizers</li> <li>Treating this as an approximation for optimization tracking</li> </ul>"},{"location":"api-reference/analyzer/#examples","title":"Examples","text":""},{"location":"api-reference/analyzer/#basic-token-counting","title":"Basic Token Counting","text":"<pre><code>from prompt_refiner import CountTokens\n\ncounter = CountTokens()\ncounter.process(\"Hello World\")\n\nstats = counter.get_stats()\nprint(stats)\n# {'tokens': 2}\n</code></pre>"},{"location":"api-reference/analyzer/#beforeafter-comparison","title":"Before/After Comparison","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\n# Build pipeline with counter at the end\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\n\n# Get statistics\nstats = counter.get_stats()\nprint(stats)\n# {\n#   'original': 6,\n#   'cleaned': 2,\n#   'saved': 4,\n#   'saving_percent': '66.7%'\n# }\n\n# Formatted output\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"api-reference/analyzer/#cost-calculation-example","title":"Cost Calculation Example","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"\"\"Your long text here...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nstats = counter.get_stats()\n\n# Calculate cost savings\n# Example: GPT-4 pricing - $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings = original_cost - cleaned_cost\n\nprint(f\"Original cost: ${original_cost:.4f}\")\nprint(f\"Cleaned cost: ${cleaned_cost:.4f}\")\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre>"},{"location":"api-reference/analyzer/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/analyzer/#roi-demonstration","title":"ROI Demonstration","text":"<pre><code>from prompt_refiner import (\n    Refiner, StripHTML, NormalizeWhitespace,\n    Deduplicate, TruncateTokens, CountTokens\n)\n\noriginal_text = \"\"\"Your messy input...\"\"\"\ncounter = CountTokens(original_text=original_text)\n\nfull_optimization = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=1000))\n    .pipe(counter)\n)\n\nresult = full_optimization.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#ab-testing-different-strategies","title":"A/B Testing Different Strategies","text":"<pre><code>from prompt_refiner import Refiner, TruncateTokens, Deduplicate, CountTokens\n\noriginal_text = \"\"\"Your text...\"\"\"\n\n# Strategy A: Just truncate\ncounter_a = CountTokens(original_text=original_text)\nstrategy_a = (\n    Refiner()\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_a)\n)\nstrategy_a.run(original_text)\n\n# Strategy B: Deduplicate then truncate\ncounter_b = CountTokens(original_text=original_text)\nstrategy_b = (\n    Refiner()\n    .pipe(Deduplicate())\n    .pipe(TruncateTokens(max_tokens=500))\n    .pipe(counter_b)\n)\nstrategy_b.run(original_text)\n\nprint(\"Strategy A:\", counter_a.format_stats())\nprint(\"Strategy B:\", counter_b.format_stats())\n</code></pre>"},{"location":"api-reference/analyzer/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nfrom prompt_refiner import Refiner, StripHTML, CountTokens\n\nlogger = logging.getLogger(__name__)\n\ndef process_user_input(text):\n    counter = CountTokens(original_text=text)\n\n    refiner = (\n        Refiner()\n        .pipe(StripHTML())\n        .pipe(counter)\n    )\n\n    result = refiner.run(text)\n    stats = counter.get_stats()\n\n    # Log optimization impact\n    logger.info(\n        f\"Processed input: \"\n        f\"original={stats['original']} tokens, \"\n        f\"cleaned={stats['cleaned']} tokens, \"\n        f\"saved={stats['saved']} tokens ({stats['saving_percent']})\"\n    )\n\n    return result\n</code></pre>"},{"location":"api-reference/analyzer/#tips","title":"Tips","text":"<p>Always Use with Original Text</p> <p>To see before/after comparisons, always initialize <code>CountTokens</code> with the original text:</p> <pre><code>counter = CountTokens(original_text=original_text)\n</code></pre> <p>Otherwise, you'll only get the final token count.</p> <p>Place at End of Pipeline</p> <p>For accurate \"after\" measurements, place <code>CountTokens</code> as the last operation in your pipeline:</p> <pre><code>refiner = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(CountTokens(original_text=text))  # Last!\n)\n</code></pre>"},{"location":"api-reference/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data, including HTML removal, whitespace normalization, and Unicode fixing.</p>"},{"location":"api-reference/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML","title":"prompt_refiner.cleaner.StripHTML","text":"<pre><code>StripHTML(preserve_tags=None, to_markdown=False)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p> <p>Initialize the HTML stripper.</p> <p>Parameters:</p> Name Type Description Default <code>preserve_tags</code> <code>Optional[Set[str]]</code> <p>Set of tag names to preserve (e.g., {'p', 'li', 'table'})</p> <code>None</code> <code>to_markdown</code> <code>bool</code> <p>Convert common HTML tags to Markdown syntax</p> <code>False</code> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def __init__(\n    self,\n    preserve_tags: Optional[Set[str]] = None,\n    to_markdown: bool = False,\n):\n    \"\"\"\n    Initialize the HTML stripper.\n\n    Args:\n        preserve_tags: Set of tag names to preserve (e.g., {'p', 'li', 'table'})\n        to_markdown: Convert common HTML tags to Markdown syntax\n    \"\"\"\n    self.preserve_tags = preserve_tags or set()\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove HTML tags from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing HTML</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with HTML tags removed or converted to Markdown</p> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove HTML tags from the input text.\n\n    Args:\n        text: The input text containing HTML\n\n    Returns:\n        Text with HTML tags removed or converted to Markdown\n    \"\"\"\n    result = text\n\n    if self.to_markdown:\n        # Convert common HTML tags to Markdown\n        # Bold\n        result = re.sub(r\"&lt;strong&gt;(.*?)&lt;/strong&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;b&gt;(.*?)&lt;/b&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        # Italic\n        result = re.sub(r\"&lt;em&gt;(.*?)&lt;/em&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;i&gt;(.*?)&lt;/i&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        # Links\n        result = re.sub(\n            r'&lt;a[^&gt;]*href=[\"\\']([^\"\\']*)[\"\\'][^&gt;]*&gt;(.*?)&lt;/a&gt;',\n            r\"[\\2](\\1)\",\n            result,\n            flags=re.DOTALL,\n        )\n        # Headers\n        for i in range(1, 7):\n            result = re.sub(\n                f\"&lt;h{i}[^&gt;]*&gt;(.*?)&lt;/h{i}&gt;\",\n                f\"{'#' * i} \\\\1\\n\",\n                result,\n                flags=re.DOTALL,\n            )\n        # Code\n        result = re.sub(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", r\"`\\1`\", result, flags=re.DOTALL)\n        # Lists - simple conversion\n        result = re.sub(r\"&lt;li[^&gt;]*&gt;(.*?)&lt;/li&gt;\", r\"- \\1\\n\", result, flags=re.DOTALL)\n        # Paragraphs\n        result = re.sub(r\"&lt;p[^&gt;]*&gt;(.*?)&lt;/p&gt;\", r\"\\1\\n\\n\", result, flags=re.DOTALL)\n        # Line breaks\n        result = re.sub(r\"&lt;br\\s*/?&gt;\", \"\\n\", result)\n\n    if self.preserve_tags:\n        # Remove all tags except preserved ones\n        # This is a simplified implementation\n        tags_pattern = r\"&lt;/?(?!\" + \"|\".join(self.preserve_tags) + r\"\\b)[^&gt;]+&gt;\"\n        result = re.sub(tags_pattern, \"\", result)\n    else:\n        # Remove all HTML tags\n        result = re.sub(r\"&lt;[^&gt;]+&gt;\", \"\", result)\n\n    # Clean up excessive newlines\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n\n    return result.strip()\n</code></pre>"},{"location":"api-reference/cleaner/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import StripHTML\n\n# Basic HTML stripping\nstripper = StripHTML()\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\nstripper = StripHTML(to_markdown=True)\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n\n# Preserve specific tags\nstripper = StripHTML(preserve_tags={\"p\", \"div\"})\nresult = stripper.process(\"&lt;div&gt;Keep &lt;b&gt;Remove&lt;/b&gt;&lt;/div&gt;\")\n# Output: \"&lt;div&gt;Keep Remove&lt;/div&gt;\"\n</code></pre>"},{"location":"api-reference/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines into single spaces.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace","title":"prompt_refiner.cleaner.NormalizeWhitespace","text":"<p>               Bases: <code>Operation</code></p> <p>Normalize whitespace in text.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Normalize whitespace by collapsing multiple spaces into one.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with normalized whitespace</p> Source code in <code>src/prompt_refiner/cleaner/whitespace.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Normalize whitespace by collapsing multiple spaces into one.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with normalized whitespace\n    \"\"\"\n    # Replace multiple whitespace with single space and strip edges\n    return \" \".join(text.split())\n</code></pre>"},{"location":"api-reference/cleaner/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import NormalizeWhitespace\n\nnormalizer = NormalizeWhitespace()\nresult = normalizer.process(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre>"},{"location":"api-reference/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters including zero-width spaces and control characters.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode","title":"prompt_refiner.cleaner.FixUnicode","text":"<pre><code>FixUnicode(\n    remove_zero_width=True, remove_control_chars=True\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove or fix problematic Unicode characters.</p> <p>Initialize the Unicode fixer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_zero_width</code> <code>bool</code> <p>Remove zero-width spaces and similar characters</p> <code>True</code> <code>remove_control_chars</code> <code>bool</code> <p>Remove control characters (except newlines and tabs)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def __init__(self, remove_zero_width: bool = True, remove_control_chars: bool = True):\n    \"\"\"\n    Initialize the Unicode fixer.\n\n    Args:\n        remove_zero_width: Remove zero-width spaces and similar characters\n        remove_control_chars: Remove control characters (except newlines and tabs)\n    \"\"\"\n    self.remove_zero_width = remove_zero_width\n    self.remove_control_chars = remove_control_chars\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Clean problematic Unicode characters from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with problematic Unicode characters removed</p> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Clean problematic Unicode characters from text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with problematic Unicode characters removed\n    \"\"\"\n    result = text\n\n    if self.remove_zero_width:\n        # Remove zero-width characters\n        zero_width_chars = [\n            \"\\u200b\",  # Zero-width space\n            \"\\u200c\",  # Zero-width non-joiner\n            \"\\u200d\",  # Zero-width joiner\n            \"\\ufeff\",  # Zero-width no-break space (BOM)\n            \"\\u2060\",  # Word joiner\n        ]\n        for char in zero_width_chars:\n            result = result.replace(char, \"\")\n\n    if self.remove_control_chars:\n        # Remove control characters except newlines, tabs, and carriage returns\n        # Keep: \\n (0x0A), \\t (0x09), \\r (0x0D)\n        result = \"\".join(\n            char\n            for char in result\n            if not unicodedata.category(char).startswith(\"C\") or char in (\"\\n\", \"\\t\", \"\\r\")\n        )\n\n    # Normalize Unicode to NFC form (canonical composition)\n    result = unicodedata.normalize(\"NFC\", result)\n\n    return result\n</code></pre>"},{"location":"api-reference/cleaner/#examples_2","title":"Examples","text":"<pre><code>from prompt_refiner import FixUnicode\n\n# Remove zero-width spaces and control chars\nfixer = FixUnicode()\nresult = fixer.process(\"Hello\\u200bWorld\\u0000\")\n# Output: \"HelloWorld\"\n\n# Only remove zero-width spaces\nfixer = FixUnicode(remove_control_chars=False)\nresult = fixer.process(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre>"},{"location":"api-reference/cleaner/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/cleaner/#web-scraping","title":"Web Scraping","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    Refiner()\n    .pipe(StripHTML(to_markdown=True))\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import Refiner, NormalizeWhitespace, FixUnicode\n\ntext_normalizer = (\n    Refiner()\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/compressor/","title":"Compressor Module","text":"<p>The Compressor module provides operations for reducing text size through smart truncation and deduplication.</p>"},{"location":"api-reference/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens","title":"prompt_refiner.compressor.TruncateTokens","text":"<pre><code>TruncateTokens(\n    max_tokens,\n    strategy=\"head\",\n    respect_sentence_boundary=True,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p> <p>Initialize the truncation operation.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to keep</p> required <code>strategy</code> <code>Literal['head', 'tail', 'middle_out']</code> <p>Truncation strategy: - \"head\": Keep the beginning of the text - \"tail\": Keep the end of the text (useful for conversation history) - \"middle_out\": Keep beginning and end, remove middle</p> <code>'head'</code> <code>respect_sentence_boundary</code> <code>bool</code> <p>If True, truncate at sentence boundaries</p> <code>True</code> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int,\n    strategy: Literal[\"head\", \"tail\", \"middle_out\"] = \"head\",\n    respect_sentence_boundary: bool = True,\n):\n    \"\"\"\n    Initialize the truncation operation.\n\n    Args:\n        max_tokens: Maximum number of tokens to keep\n        strategy: Truncation strategy:\n            - \"head\": Keep the beginning of the text\n            - \"tail\": Keep the end of the text (useful for conversation history)\n            - \"middle_out\": Keep beginning and end, remove middle\n        respect_sentence_boundary: If True, truncate at sentence boundaries\n    \"\"\"\n    self.max_tokens = max_tokens\n    self.strategy = strategy\n    self.respect_sentence_boundary = respect_sentence_boundary\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Truncate text to max_tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text respecting sentence boundaries if configured</p> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate text to max_tokens.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Truncated text respecting sentence boundaries if configured\n    \"\"\"\n    estimated_tokens = self._estimate_tokens(text)\n\n    if estimated_tokens &lt;= self.max_tokens:\n        return text\n\n    if self.respect_sentence_boundary:\n        sentences = self._split_sentences(text)\n\n        if self.strategy == \"head\":\n            return self._truncate_head_sentences(sentences)\n        elif self.strategy == \"tail\":\n            return self._truncate_tail_sentences(sentences)\n        elif self.strategy == \"middle_out\":\n            return self._truncate_middle_out_sentences(sentences)\n    else:\n        # Fallback to word-based truncation\n        words = text.split()\n\n        if self.strategy == \"head\":\n            return \" \".join(words[: self.max_tokens])\n        elif self.strategy == \"tail\":\n            return \" \".join(words[-self.max_tokens :])\n        elif self.strategy == \"middle_out\":\n            half = self.max_tokens // 2\n            start_words = words[:half]\n            end_words = words[-(self.max_tokens - half) :]\n            return \" \".join(start_words) + \" ... \" + \" \".join(end_words)\n\n    return text\n</code></pre>"},{"location":"api-reference/compressor/#truncation-strategies","title":"Truncation Strategies","text":"<ul> <li><code>head</code>: Keep the beginning of the text (default)</li> <li><code>tail</code>: Keep the end of the text (useful for conversation history)</li> <li><code>middle_out</code>: Keep beginning and end, remove middle</li> </ul>"},{"location":"api-reference/compressor/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\nresult = truncator.process(long_text)\n\n# Keep last 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\nresult = truncator.process(long_text)\n\n# Keep first and last 50 tokens, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\nresult = truncator.process(long_text)\n\n# Truncate at word boundaries (faster, less precise)\ntruncator = TruncateTokens(\n    max_tokens=100,\n    strategy=\"head\",\n    respect_sentence_boundary=False\n)\nresult = truncator.process(long_text)\n</code></pre>"},{"location":"api-reference/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or highly similar text chunks, useful for RAG contexts.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate","title":"prompt_refiner.compressor.Deduplicate","text":"<pre><code>Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\",\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Remove duplicate or highly similar text chunks (useful for RAG contexts).</p> Performance Characteristics <p>This operation uses an O(n\u00b2) comparison algorithm, where each chunk is compared against all previously seen chunks. The total complexity is O(n\u00b2 \u00d7 comparison_cost), where comparison_cost depends on the selected similarity method: - Jaccard: O(m) where m is the chunk length (word-based) - Levenshtein: O(m\u2081 \u00d7 m\u2082) where m\u2081, m\u2082 are the chunk lengths (character-based)</p> <p>For typical RAG contexts (10-50 chunks), performance is acceptable with either method. For larger inputs (200+ chunks), consider using paragraph granularity to reduce the number of comparisons, or use Jaccard method for better performance.</p> <p>Initialize the deduplication operation.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for considering text similar (0.0-1.0)</p> <code>0.85</code> <code>method</code> <code>Literal['levenshtein', 'jaccard']</code> <p>Similarity calculation method - \"jaccard\": Jaccard similarity (word-based, faster)     * Complexity: O(m) per comparison where m is chunk length     * Recommended for most use cases (10-200 chunks)     * Fast even with long chunks - \"levenshtein\": Levenshtein distance (character-based)     * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison     * More precise but computationally expensive     * Can be slow with long chunks (1000+ characters)</p> <code>'jaccard'</code> <code>granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Text granularity to deduplicate at - \"sentence\": Deduplicate at sentence level     * More comparisons (more chunks) but smaller chunk sizes     * Better for fine-grained deduplication - \"paragraph\": Deduplicate at paragraph level     * Fewer comparisons but larger chunk sizes     * Recommended for large documents to reduce n\u00b2 scaling</p> <code>'paragraph'</code> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    similarity_threshold: float = 0.85,\n    method: Literal[\"levenshtein\", \"jaccard\"] = \"jaccard\",\n    granularity: Literal[\"sentence\", \"paragraph\"] = \"paragraph\",\n):\n    \"\"\"\n    Initialize the deduplication operation.\n\n    Args:\n        similarity_threshold: Threshold for considering text similar (0.0-1.0)\n        method: Similarity calculation method\n            - \"jaccard\": Jaccard similarity (word-based, faster)\n                * Complexity: O(m) per comparison where m is chunk length\n                * Recommended for most use cases (10-200 chunks)\n                * Fast even with long chunks\n            - \"levenshtein\": Levenshtein distance (character-based)\n                * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison\n                * More precise but computationally expensive\n                * Can be slow with long chunks (1000+ characters)\n        granularity: Text granularity to deduplicate at\n            - \"sentence\": Deduplicate at sentence level\n                * More comparisons (more chunks) but smaller chunk sizes\n                * Better for fine-grained deduplication\n            - \"paragraph\": Deduplicate at paragraph level\n                * Fewer comparisons but larger chunk sizes\n                * Recommended for large documents to reduce n\u00b2 scaling\n    \"\"\"\n    self.similarity_threshold = similarity_threshold\n    self.method = method\n    self.granularity = granularity\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove duplicate or similar text chunks.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with duplicates removed</p> Performance Note <p>This method uses O(n\u00b2) comparisons where n is the number of chunks. For large inputs (200+ chunks), consider using paragraph granularity to reduce the number of chunks, or ensure you're using the jaccard method for better performance.</p> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove duplicate or similar text chunks.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with duplicates removed\n\n    Performance Note:\n        This method uses O(n\u00b2) comparisons where n is the number of chunks.\n        For large inputs (200+ chunks), consider using paragraph granularity\n        to reduce the number of chunks, or ensure you're using the jaccard\n        method for better performance.\n    \"\"\"\n    chunks = self._split_text(text)\n\n    if not chunks:\n        return text\n\n    # Keep track of unique chunks\n    unique_chunks = []\n    seen_chunks = []\n\n    for chunk in chunks:\n        is_duplicate = False\n\n        # Check similarity with all previously seen chunks\n        for seen_chunk in seen_chunks:\n            similarity = self._calculate_similarity(chunk, seen_chunk)\n            if similarity &gt;= self.similarity_threshold:\n                is_duplicate = True\n                break\n\n        if not is_duplicate:\n            unique_chunks.append(chunk)\n            seen_chunks.append(chunk)\n\n    # Reconstruct text\n    if self.granularity == \"paragraph\":\n        return \"\\n\\n\".join(unique_chunks)\n    else:  # sentence\n        return \" \".join(unique_chunks)\n</code></pre>"},{"location":"api-reference/compressor/#similarity-methods","title":"Similarity Methods","text":"<ul> <li><code>jaccard</code>: Jaccard similarity (word-based, faster) - default</li> <li><code>levenshtein</code>: Levenshtein distance (character-based, more accurate)</li> </ul>"},{"location":"api-reference/compressor/#granularity-levels","title":"Granularity Levels","text":"<ul> <li><code>paragraph</code>: Deduplicate at paragraph level (split by <code>\\n\\n</code>) - default</li> <li><code>sentence</code>: Deduplicate at sentence level (split by <code>.</code>, <code>!</code>, <code>?</code>)</li> </ul>"},{"location":"api-reference/compressor/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# Basic deduplication (85% similarity threshold)\ndeduper = Deduplicate(similarity_threshold=0.85)\nresult = deduper.process(text_with_duplicates)\n\n# More aggressive (70% similarity)\ndeduper = Deduplicate(similarity_threshold=0.70)\nresult = deduper.process(text_with_duplicates)\n\n# Character-level similarity\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"levenshtein\"\n)\nresult = deduper.process(text_with_duplicates)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    granularity=\"sentence\"\n)\nresult = deduper.process(text_with_duplicates)\n</code></pre>"},{"location":"api-reference/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.85))  # Remove duplicates first\n    .pipe(TruncateTokens(max_tokens=2000))        # Then fit in context window\n)\n</code></pre>"},{"location":"api-reference/compressor/#conversation-history-compression","title":"Conversation History Compression","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nconversation_compressor = (\n    Refiner()\n    .pipe(Deduplicate(granularity=\"sentence\"))\n    .pipe(TruncateTokens(max_tokens=1000, strategy=\"tail\"))  # Keep recent messages\n)\n</code></pre>"},{"location":"api-reference/compressor/#document-summarization-prep","title":"Document Summarization Prep","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nsummarization_prep = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.90))  # Remove near-duplicates\n    .pipe(TruncateTokens(max_tokens=4000, strategy=\"middle_out\"))  # Keep intro + conclusion\n)\n</code></pre>"},{"location":"api-reference/refiner/","title":"Refiner Class","text":"<p>The <code>Refiner</code> class is the core pipeline builder that allows you to chain multiple operations together.</p>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner","title":"prompt_refiner.refiner.Refiner","text":"<pre><code>Refiner()\n</code></pre> <p>A pipeline builder for prompt refining operations.</p> <p>Initialize an empty refiner pipeline.</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize an empty refiner pipeline.\"\"\"\n    self._operations: List[Operation] = []\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner-functions","title":"Functions","text":""},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.pipe","title":"pipe","text":"<pre><code>pipe(operation)\n</code></pre> <p>Add an operation to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>Operation</code> <p>The operation to add</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def pipe(self, operation: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Add an operation to the pipeline.\n\n    Args:\n        operation: The operation to add\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._operations.append(operation)\n    return self\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.run","title":"run","text":"<pre><code>run(text)\n</code></pre> <p>Execute the pipeline on the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>The processed text after all operations</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def run(self, text: str) -&gt; str:\n    \"\"\"\n    Execute the pipeline on the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text after all operations\n    \"\"\"\n    result = text\n    for operation in self._operations:\n        result = operation.process(result)\n    return result\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre> <p>Support pipe operator syntax for adding operations to the pipeline.</p> <p>Enables continued chaining: (op1 | op2) | op3</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Operation</code> <p>The operation to add to the pipeline</p> required <p>Returns:</p> Type Description <code>Refiner</code> <p>Self for method chaining</p> Example <p>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100) result = pipeline.run(text)</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __or__(self, other: Operation) -&gt; \"Refiner\":\n    \"\"\"\n    Support pipe operator syntax for adding operations to the pipeline.\n\n    Enables continued chaining: (op1 | op2) | op3\n\n    Args:\n        other: The operation to add to the pipeline\n\n    Returns:\n        Self for method chaining\n\n    Example:\n        &gt;&gt;&gt; from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n        &gt;&gt;&gt; pipeline = StripHTML() | NormalizeWhitespace() | TruncateTokens(max_tokens=100)\n        &gt;&gt;&gt; result = pipeline.run(text)\n    \"\"\"\n    return self.pipe(other)\n</code></pre>"},{"location":"api-reference/refiner/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/refiner/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre>"},{"location":"api-reference/refiner/#fluent-api-with-pipe","title":"Fluent API with .pipe()","text":"<p>The <code>Refiner</code> class supports method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the fluent API\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre> <p>Both approaches work identically - choose the one that fits your style.</p>"},{"location":"api-reference/refiner/#pipeline-execution","title":"Pipeline Execution","text":"<p>When you call <code>run(text)</code>, the Refiner:</p> <ol> <li>Takes the input text</li> <li>Passes it through each operation in sequence</li> <li>Each operation's output becomes the next operation's input</li> <li>Returns the final processed text</li> </ol> <pre><code># Pipeline: text \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 result\nresult = refiner.run(text)\n</code></pre>"},{"location":"api-reference/scrubber/","title":"Scrubber Module","text":"<p>The Scrubber module provides operations for security and privacy, including automatic PII redaction.</p>"},{"location":"api-reference/scrubber/#redactpii","title":"RedactPII","text":"<p>Redact sensitive personally identifiable information (PII) from text using regex patterns.</p>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII","title":"prompt_refiner.scrubber.RedactPII","text":"<pre><code>RedactPII(\n    redact_types=None,\n    custom_patterns=None,\n    custom_keywords=None,\n)\n</code></pre> <p>               Bases: <code>Operation</code></p> <p>Redact sensitive information from text using regex patterns.</p> <p>Initialize the PII redaction operation.</p> <p>Parameters:</p> Name Type Description Default <code>redact_types</code> <code>Optional[Set[str]]</code> <p>Set of PII types to redact (default: all) Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"</p> <code>None</code> <code>custom_patterns</code> <code>Optional[dict[str, str]]</code> <p>Dictionary of custom regex patterns to apply Format: {\"name\": \"regex_pattern\"}</p> <code>None</code> <code>custom_keywords</code> <code>Optional[Set[str]]</code> <p>Set of custom keywords to redact (case-insensitive)</p> <code>None</code> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def __init__(\n    self,\n    redact_types: Optional[Set[str]] = None,\n    custom_patterns: Optional[dict[str, str]] = None,\n    custom_keywords: Optional[Set[str]] = None,\n):\n    \"\"\"\n    Initialize the PII redaction operation.\n\n    Args:\n        redact_types: Set of PII types to redact (default: all)\n            Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"\n        custom_patterns: Dictionary of custom regex patterns to apply\n            Format: {\"name\": \"regex_pattern\"}\n        custom_keywords: Set of custom keywords to redact (case-insensitive)\n    \"\"\"\n    self.redact_types = redact_types or set(self.PATTERNS.keys())\n    self.custom_patterns = custom_patterns or {}\n    self.custom_keywords = custom_keywords or set()\n</code></pre>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII-functions","title":"Functions","text":""},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Redact PII from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with PII redacted</p> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Redact PII from the input text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with PII redacted\n    \"\"\"\n    result = text\n\n    # Apply standard PII patterns\n    for pii_type in self.redact_types:\n        if pii_type in self.PATTERNS:\n            pattern = self.PATTERNS[pii_type]\n            replacement = self.REPLACEMENTS.get(pii_type, \"[REDACTED]\")\n            result = re.sub(pattern, replacement, result)\n\n    # Apply custom patterns\n    for name, pattern in self.custom_patterns.items():\n        replacement = f\"[{name.upper()}]\"\n        result = re.sub(pattern, replacement, result)\n\n    # Apply custom keywords (case-insensitive)\n    for keyword in self.custom_keywords:\n        # Use word boundaries to avoid partial matches\n        pattern = rf\"\\b{re.escape(keyword)}\\b\"\n        result = re.sub(pattern, \"[REDACTED]\", result, flags=re.IGNORECASE)\n\n    return result\n</code></pre>"},{"location":"api-reference/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code>: Email addresses \u2192 <code>[EMAIL]</code></li> <li><code>phone</code>: Phone numbers (US format) \u2192 <code>[PHONE]</code></li> <li><code>ip</code>: IP addresses \u2192 <code>[IP]</code></li> <li><code>credit_card</code>: Credit card numbers \u2192 <code>[CARD]</code></li> <li><code>ssn</code>: Social Security Numbers \u2192 <code>[SSN]</code></li> <li><code>url</code>: URLs \u2192 <code>[URL]</code></li> </ul>"},{"location":"api-reference/scrubber/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact me at john@example.com or 555-123-4567\")\n# Output: \"Contact me at [EMAIL] or [PHONE]\"\n\n# Redact specific types only\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\nresult = redactor.process(\"Email: john@example.com, IP: 192.168.1.1\")\n# Output: \"Email: [EMAIL], IP: 192.168.1.1\"\n\n# Custom patterns\nredactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\nresult = redactor.process(\"Employee EMP-12345 accessed the system\")\n# Output: \"Employee [EMPLOYEE_ID] accessed the system\"\n\n# Custom keywords (case-insensitive)\nredactor = RedactPII(\n    custom_keywords={\"confidential\", \"secret\"}\n)\nresult = redactor.process(\"This is Confidential information\")\n# Output: \"This is [REDACTED] information\"\n</code></pre>"},{"location":"api-reference/scrubber/#combining-options","title":"Combining Options","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact standard PII + custom patterns + keywords\nredactor = RedactPII(\n    redact_types={\"email\", \"phone\", \"ssn\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"},\n    custom_keywords={\"internal\", \"confidential\"}\n)\n\ntext = \"\"\"\nEmployee EMP-12345\nEmail: john@example.com\nPhone: 555-123-4567\nSSN: 123-45-6789\nThis is Confidential information for internal use only.\n\"\"\"\n\nresult = redactor.process(text)\n</code></pre>"},{"location":"api-reference/scrubber/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/scrubber/#before-sending-to-llm-apis","title":"Before Sending to LLM APIs","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nsecure_pipeline = (\n    Refiner()\n    .pipe(RedactPII(redact_types={\"email\", \"phone\", \"ssn\", \"credit_card\"}))\n)\n\n# Safe to send to external APIs\nsecure_text = secure_pipeline.run(user_input)\n</code></pre>"},{"location":"api-reference/scrubber/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nlog_redactor = (\n    Refiner()\n    .pipe(RedactPII())  # Redact all PII types\n)\n\n# Safe to log\nsafe_log = log_redactor.run(sensitive_data)\nlogger.info(safe_log)\n</code></pre>"},{"location":"api-reference/scrubber/#data-export-compliance","title":"Data Export Compliance","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\n# Custom redaction for specific compliance needs\ngdpr_redactor = (\n    Refiner()\n    .pipe(RedactPII(\n        redact_types={\"email\", \"phone\", \"ip\"},\n        custom_keywords={\"customer_name\", \"address\", \"dob\"}\n    ))\n)\n\nexport_data = gdpr_redactor.run(user_data)\n</code></pre>"},{"location":"api-reference/scrubber/#security-best-practices","title":"Security Best Practices","text":"<p>Regex Limitations</p> <p>PII redaction uses regex patterns which may not catch all variations. For production use:</p> <ul> <li>Test thoroughly with your specific data</li> <li>Consider using specialized PII detection services for critical applications</li> <li>Add custom patterns for domain-specific PII</li> <li>Review redacted output before sending to external services</li> </ul> <p>Defense in Depth</p> <p>PII redaction is one layer of security. Always:</p> <ul> <li>Validate and sanitize user input</li> <li>Use proper authentication and authorization</li> <li>Encrypt data in transit and at rest</li> <li>Follow your organization's security policies</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for each module in Prompt Refiner.</p>"},{"location":"examples/#by-module","title":"By Module","text":""},{"location":"examples/#cleaner-examples","title":"Cleaner Examples","text":"<ul> <li>HTML Cleaning - Strip HTML tags and convert to Markdown</li> <li>See more in: Cleaner Module</li> </ul>"},{"location":"examples/#compressor-examples","title":"Compressor Examples","text":"<ul> <li>Deduplication - Remove duplicate content from RAG results</li> <li>See more in: Compressor Module</li> </ul>"},{"location":"examples/#scrubber-examples","title":"Scrubber Examples","text":"<ul> <li>PII Redaction - Redact sensitive information</li> <li>See more in: Scrubber Module</li> </ul>"},{"location":"examples/#analyzer-examples","title":"Analyzer Examples","text":"<ul> <li>Token Analysis - Calculate token savings and ROI</li> <li>See more in: Analyzer Module</li> </ul>"},{"location":"examples/#complete-examples","title":"Complete Examples","text":"<ul> <li>Complete Pipeline - Full optimization with all modules</li> </ul>"},{"location":"examples/#running-examples-locally","title":"Running Examples Locally","text":"<p>All examples are available in the <code>examples/</code> directory:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run an example\npython examples/all_modules_demo.py\n</code></pre>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Getting Started Guide</li> <li>API Reference</li> <li>Report Issues</li> </ul>"},{"location":"examples/complete-pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive example using all 4 modules together.</p>"},{"location":"examples/complete-pipeline/#full-optimization-pipeline","title":"Full Optimization Pipeline","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\n# Messy input with HTML, PII, duplicates\nmessy_input = \"\"\"\n&lt;div&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;We provide excellent service   with   lots   of   spaces.&lt;/p&gt;\n    &lt;p&gt;Our IP address is 192.168.1.1 for reference.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Initialize counter\ncounter = CountTokens(original_text=messy_input)\n\n# Build complete pipeline using pipe operator (recommended)\npipeline = (\n    # Clean dirty data\n    StripHTML()\n    | FixUnicode()\n    | NormalizeWhitespace()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=50, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\", \"ip\"})\n    # Analyze\n    | counter\n)\n\n# Run pipeline\nresult = pipeline.run(messy_input)\n\nprint(\"Optimized result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(counter.format_stats())\n</code></pre> <p>Alternative: Fluent API</p> <p>You can also use <code>.pipe()</code> method chaining: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n    # ... continue with other operations\n)\n</code></pre></p>"},{"location":"examples/complete-pipeline/#full-example","title":"Full Example","text":"<p>See: <code>examples/all_modules_demo.py</code></p> <pre><code>python examples/all_modules_demo.py\n</code></pre>"},{"location":"examples/complete-pipeline/#related","title":"Related","text":"<ul> <li>Pipeline Basics</li> <li>All Modules Overview</li> </ul>"},{"location":"examples/deduplication/","title":"Deduplication Example","text":"<p>Remove duplicate content from RAG retrieval results.</p>"},{"location":"examples/deduplication/#scenario","title":"Scenario","text":"<p>Your RAG system retrieved multiple similar chunks that contain overlapping information.</p>"},{"location":"examples/deduplication/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# RAG results with duplicates\nrag_results = \"\"\"\nPython is a high-level programming language.\n\nPython is a high level programming language.\n\nPython supports multiple programming paradigms.\n\"\"\"\n\npipeline = Deduplicate(similarity_threshold=0.85)\ndeduplicated = pipeline.run(rag_results)\n\nprint(deduplicated)\n# Output: Only unique paragraphs remain\n</code></pre>"},{"location":"examples/deduplication/#adjusting-sensitivity","title":"Adjusting Sensitivity","text":"<pre><code># More aggressive (70% similarity)\npipeline = Deduplicate(similarity_threshold=0.70)\n\n# Sentence-level deduplication\npipeline = Deduplicate(granularity=\"sentence\")\n</code></pre>"},{"location":"examples/deduplication/#performance-considerations","title":"Performance Considerations","text":"<p>When working with large RAG contexts, keep these performance tips in mind:</p>"},{"location":"examples/deduplication/#choosing-a-similarity-method","title":"Choosing a Similarity Method","text":"<pre><code># Fast: Jaccard (word-based) - recommended for most use cases\npipeline = Deduplicate(method=\"jaccard\")\n\n# Precise but slower: Levenshtein (character-based)\n# Only use when you need character-level accuracy\npipeline = Deduplicate(method=\"levenshtein\")\n</code></pre>"},{"location":"examples/deduplication/#scaling-with-input-size","title":"Scaling with Input Size","text":"<p>The deduplication algorithm compares each chunk against all previous chunks (O(n\u00b2)):</p> <ul> <li>10-50 chunks: Fast with either method (typical RAG use case)</li> <li>50-200 chunks: Use Jaccard for better performance</li> <li>200+ chunks: Use <code>granularity=\"paragraph\"</code> to reduce chunk count</li> </ul> <pre><code># For large documents: use paragraph granularity\npipeline = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\"  # Fewer chunks = faster\n)\n</code></pre>"},{"location":"examples/deduplication/#full-example","title":"Full Example","text":"<p>See: <code>examples/compressor/deduplication.py</code></p>"},{"location":"examples/deduplication/#related","title":"Related","text":"<ul> <li>Deduplicate API Reference</li> <li>Compressor Module Guide</li> </ul>"},{"location":"examples/html-cleaning/","title":"HTML Cleaning Example","text":"<p>Clean HTML content from web scraping or user input.</p>"},{"location":"examples/html-cleaning/#scenario","title":"Scenario","text":"<p>You've scraped content from a website and need to clean it before sending to an LLM API.</p>"},{"location":"examples/html-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\nhtml_content = \"\"\"\n&lt;div class=\"article\"&gt;\n    &lt;h1&gt;Understanding &lt;strong&gt;LLMs&lt;/strong&gt;&lt;/h1&gt;\n    &lt;p&gt;Large Language Models are powerful &lt;em&gt;AI systems&lt;/em&gt;.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Remove all HTML and normalize whitespace\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\ncleaned = pipeline.run(html_content)\nprint(cleaned)\n# Output: \"Understanding LLMs Large Language Models are powerful AI systems.\"\n</code></pre>"},{"location":"examples/html-cleaning/#converting-to-markdown","title":"Converting to Markdown","text":"<pre><code># Convert HTML to Markdown instead of removing\npipeline = (\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n)\n\nmarkdown = pipeline.run(html_content)\nprint(markdown)\n# Output:\n# # Understanding **LLMs**\n#\n# Large Language Models are powerful *AI systems*.\n</code></pre>"},{"location":"examples/html-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/html_cleaning.py</code></p> <pre><code>python examples/cleaner/html_cleaning.py\n</code></pre>"},{"location":"examples/html-cleaning/#related","title":"Related","text":"<ul> <li>StripHTML API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/pii-redaction/","title":"PII Redaction Example","text":"<p>Automatically redact sensitive information before sending to APIs.</p>"},{"location":"examples/pii-redaction/#scenario","title":"Scenario","text":"<p>User input contains personal information that should not be sent to external LLM APIs.</p>"},{"location":"examples/pii-redaction/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import RedactPII\n\nuser_input = \"\"\"\nPlease contact me at john.doe@example.com or call 555-123-4567.\nMy account number is EMP-12345.\n\"\"\"\n\npipeline = RedactPII()\nsecure = pipeline.run(user_input)\n\nprint(secure)\n# Output:\n# Please contact me at [EMAIL] or call [PHONE].\n# My account number is EMP-12345.\n</code></pre>"},{"location":"examples/pii-redaction/#custom-patterns","title":"Custom Patterns","text":"<pre><code>pipeline = RedactPII(\n    redact_types={\"email\", \"phone\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n\nsecure = pipeline.run(user_input)\n# Now EMP-12345 is also redacted as [EMPLOYEE_ID]\n</code></pre>"},{"location":"examples/pii-redaction/#full-example","title":"Full Example","text":"<p>See: <code>examples/scrubber/pii_redaction.py</code></p>"},{"location":"examples/pii-redaction/#related","title":"Related","text":"<ul> <li>RedactPII API Reference</li> <li>Scrubber Module Guide</li> </ul>"},{"location":"examples/token-analysis/","title":"Token Analysis Example","text":"<p>Measure optimization impact and calculate cost savings.</p>"},{"location":"examples/token-analysis/#scenario","title":"Scenario","text":"<p>You want to demonstrate the value of prompt optimization.</p>"},{"location":"examples/token-analysis/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   from   HTML  &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | counter\n)\n\nresult = pipeline.run(original_text)\n\n# Show statistics\nprint(counter.format_stats())\n# Output:\n# Original: 10 tokens\n# Cleaned: 4 tokens\n# Saved: 6 tokens (60.0%)\n</code></pre>"},{"location":"examples/token-analysis/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\n\n# GPT-4 pricing: $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\n\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings_per_request = original_cost - cleaned_cost\n\nprint(f\"Savings: ${savings_per_request:.4f} per request\")\n\n# Project annual savings\nrequests_per_day = 10000\nannual_savings = savings_per_request * requests_per_day * 365\nprint(f\"Annual savings: ${annual_savings:.2f}\")\n</code></pre>"},{"location":"examples/token-analysis/#full-example","title":"Full Example","text":"<p>See: <code>examples/analyzer/token_counting.py</code></p>"},{"location":"examples/token-analysis/#related","title":"Related","text":"<ul> <li>CountTokens API Reference</li> <li>Analyzer Module Guide</li> </ul>"},{"location":"modules/analyzer/","title":"Analyzer Module","text":"<p>Track optimization impact and demonstrate value with token counting and statistics.</p>"},{"location":"modules/analyzer/#counttokens-operation","title":"CountTokens Operation","text":"<p>Measure token usage before and after optimization.</p>"},{"location":"modules/analyzer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World&lt;/p&gt;\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"modules/analyzer/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\ncost_per_token = 0.03 / 1000  # GPT-4 pricing\nsavings = stats['saved'] * cost_per_token\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data from various sources.</p>"},{"location":"modules/cleaner/#overview","title":"Overview","text":"<p>When working with real-world text data, you often encounter:</p> <ul> <li>HTML tags from web scraping</li> <li>Excessive whitespace and formatting issues</li> <li>Problematic Unicode characters</li> </ul> <p>The Cleaner module addresses these issues efficiently.</p>"},{"location":"modules/cleaner/#operations","title":"Operations","text":""},{"location":"modules/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags or convert them to Markdown.</p> <p>Use cases:</p> <ul> <li>Web scraping</li> <li>Email content processing</li> <li>User-generated HTML content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import StripHTML\n\n# Remove all HTML\ncleaner = StripHTML()\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\ncleaner = StripHTML(to_markdown=True)\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines.</p> <p>Use cases:</p> <ul> <li>Text from PDFs</li> <li>User input normalization</li> <li>Copy-pasted content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import NormalizeWhitespace\n\ncleaner = NormalizeWhitespace()\nresult = cleaner.run(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters.</p> <p>Use cases:</p> <ul> <li>Zero-width spaces from copy-paste</li> <li>Control characters</li> <li>Invisible characters causing issues</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import FixUnicode\n\ncleaner = FixUnicode()\nresult = cleaner.run(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/cleaner/#web-content-pipeline","title":"Web Content Pipeline","text":"<pre><code>from prompt_refiner import StripHTML, FixUnicode, NormalizeWhitespace\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import FixUnicode, NormalizeWhitespace\n\nnormalizer = (\n    FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples</li> <li>Full API Reference</li> <li>Explore Other Modules</li> </ul>"},{"location":"modules/compressor/","title":"Compressor Module","text":"<p>Reduce text size while preserving meaning through smart truncation and deduplication.</p>"},{"location":"modules/compressor/#operations","title":"Operations","text":""},{"location":"modules/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Smart text truncation respecting sentence boundaries.</p> <pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\n\n# Keep last 100 tokens (for conversation history)\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\n\n# Keep beginning and end, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or similar content chunks.</p> <pre><code>from prompt_refiner import Deduplicate\n\n# Remove paragraphs with 85% similarity\ndeduper = Deduplicate(similarity_threshold=0.85)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(granularity=\"sentence\")\n</code></pre> <p>Performance Considerations:</p> <ul> <li>Method Choice: Use <code>jaccard</code> (default) for most cases - it's fast and works well with typical prompts. Only use <code>levenshtein</code> when you need character-level precision.</li> <li>Complexity: Deduplication uses O(n\u00b2) comparisons where n is the number of chunks. For 50 chunks, this is ~1,225 comparisons.</li> <li>Large Inputs: For 200+ chunks, use <code>granularity=\"paragraph\"</code> to reduce chunk count and speed up processing.</li> <li>Jaccard: O(m) per comparison - fast even with long chunks</li> <li>Levenshtein: O(m\u2081 \u00d7 m\u2082) per comparison - can be slow with chunks over 1000 characters</li> </ul> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"modules/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n</code></pre> <p>View Examples</p>"},{"location":"modules/overview/","title":"Modules Overview","text":"<p>Prompt Refiner is organized into 4 specialized modules, each designed to handle a specific aspect of prompt optimization.</p>"},{"location":"modules/overview/#the-4-core-modules","title":"The 4 Core Modules","text":""},{"location":"modules/overview/#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<p>The Cleaner module removes unwanted artifacts from your text.</p> <p>Operations:</p> <ul> <li>StripHTML - Remove or convert HTML tags</li> <li>NormalizeWhitespace - Collapse excessive whitespace</li> <li>FixUnicode - Remove problematic Unicode characters</li> </ul> <p>When to use:</p> <ul> <li>Processing web-scraped content</li> <li>Cleaning user-generated text</li> <li>Normalizing text from various sources</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<p>The Compressor module reduces token count while preserving meaning.</p> <p>Operations:</p> <ul> <li>TruncateTokens - Smart text truncation with sentence boundaries</li> <li>Deduplicate - Remove similar or duplicate content</li> </ul> <p>When to use:</p> <ul> <li>Fitting content within context windows</li> <li>Optimizing RAG retrieval results</li> <li>Reducing API costs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<p>The Scrubber module protects sensitive information.</p> <p>Operations:</p> <ul> <li>RedactPII - Automatically redact personally identifiable information</li> </ul> <p>When to use:</p> <ul> <li>Before sending data to external APIs</li> <li>Compliance with privacy regulations</li> <li>Protecting user data in logs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#4-analyzer-show-value","title":"4. Analyzer - Show Value","text":"<p>The Analyzer module tracks optimization impact.</p> <p>Operations:</p> <ul> <li>CountTokens - Measure token savings and calculate ROI</li> </ul> <p>When to use:</p> <ul> <li>Demonstrating cost savings</li> <li>A/B testing optimization strategies</li> <li>Monitoring optimization impact</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#combining-modules","title":"Combining Modules","text":"<p>The real power comes from combining modules in a pipeline:</p> <pre><code>from prompt_refiner import (\n    Refiner,\n    StripHTML, NormalizeWhitespace,  # Cleaner\n    TruncateTokens,                  # Compressor\n    RedactPII,                       # Scrubber\n    CountTokens                      # Analyzer\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    Refiner()\n    # Clean first\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    # Then compress\n    .pipe(TruncateTokens(max_tokens=1000))\n    # Secure\n    .pipe(RedactPII())\n    # Analyze\n    .pipe(counter)\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"modules/overview/#module-relationships","title":"Module Relationships","text":"<pre><code>graph LR\n    A[Raw Input] --&gt; B[Cleaner]\n    B --&gt; C[Compressor]\n    C --&gt; D[Scrubber]\n    D --&gt; E[Analyzer]\n    E --&gt; F[Optimized Output]\n</code></pre>"},{"location":"modules/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Clean before compressing, compress before redacting</li> <li>Test your pipeline: Different inputs may need different operations</li> <li>Measure impact: Use CountTokens to track savings</li> <li>Start simple: Begin with one module and add more as needed</li> </ol>"},{"location":"modules/scrubber/","title":"Scrubber Module","text":"<p>Protect sensitive information with automatic PII redaction.</p>"},{"location":"modules/scrubber/#redactpii-operation","title":"RedactPII Operation","text":"<p>Automatically redact personally identifiable information using regex patterns.</p>"},{"location":"modules/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code> - Email addresses</li> <li><code>phone</code> - Phone numbers</li> <li><code>ip</code> - IP addresses</li> <li><code>credit_card</code> - Credit card numbers</li> <li><code>ssn</code> - Social Security Numbers</li> <li><code>url</code> - URLs</li> </ul>"},{"location":"modules/scrubber/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact john@example.com or 555-123-4567\")\n# Output: \"Contact [EMAIL] or [PHONE]\"\n\n# Redact specific types\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\n</code></pre>"},{"location":"modules/scrubber/#custom-patterns","title":"Custom Patterns","text":"<pre><code>redactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"user-guide/custom-operations/","title":"Custom Operations","text":"<p>Create your own operations to extend Prompt Refiner.</p>"},{"location":"user-guide/custom-operations/#creating-a-custom-operation","title":"Creating a Custom Operation","text":"<p>All operations inherit from the <code>Operation</code> base class and implement the <code>process</code> method:</p> <pre><code>from prompt_refiner import Operation\n\nclass RemoveEmojis(Operation):\n    \"\"\"Remove emoji characters from text.\"\"\"\n\n    def process(self, text: str) -&gt; str:\n        import re\n        # Simple emoji removal pattern\n        emoji_pattern = re.compile(\n            \"[\"\n            \"\\U0001F600-\\U0001F64F\"  # emoticons\n            \"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n            \"]+\", flags=re.UNICODE\n        )\n        return emoji_pattern.sub(\"\", text)\n</code></pre>"},{"location":"user-guide/custom-operations/#using-your-custom-operation","title":"Using Your Custom Operation","text":"<p>Use it like any built-in operation:</p> <pre><code>from prompt_refiner import Refiner, NormalizeWhitespace\n\npipeline = (\n    Refiner()\n    .pipe(RemoveEmojis())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = pipeline.run(\"Hello \ud83d\ude00 World \ud83c\udf0d!\")\n# Output: \"Hello World !\"\n</code></pre>"},{"location":"user-guide/custom-operations/#more-examples","title":"More Examples","text":""},{"location":"user-guide/custom-operations/#remove-urls","title":"Remove URLs","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveURLs(Operation):\n    def process(self, text: str) -&gt; str:\n        url_pattern = r'https?://\\S+|www\\.\\S+'\n        return re.sub(url_pattern, '[URL]', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#lowercase-text","title":"Lowercase Text","text":"<pre><code>from prompt_refiner import Operation\n\nclass Lowercase(Operation):\n    def process(self, text: str) -&gt; str:\n        return text.lower()\n</code></pre>"},{"location":"user-guide/custom-operations/#remove-numbers","title":"Remove Numbers","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveNumbers(Operation):\n    def process(self, text: str) -&gt; str:\n        return re.sub(r'\\d+', '', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#guidelines","title":"Guidelines","text":"<ol> <li>Single responsibility - Each operation should do one thing well</li> <li>Immutable - Don't modify the input, return a new string</li> <li>Deterministic - Same input should always produce same output</li> <li>Document - Add docstrings explaining what it does</li> </ol>"},{"location":"user-guide/custom-operations/#contributing","title":"Contributing","text":"<p>Have a useful operation? Consider contributing it to Prompt Refiner!</p> <p>See contributing guide \u2192</p>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Learn how to use Prompt Refiner effectively to optimize your LLM inputs.</p>"},{"location":"user-guide/overview/#what-is-prompt-refiner","title":"What is Prompt Refiner?","text":"<p>Prompt Refiner is a library for cleaning and optimizing text before sending it to LLM APIs. It helps you:</p> <ul> <li>Save money by reducing token usage</li> <li>Improve quality by cleaning and normalizing text</li> <li>Enhance security by redacting PII</li> <li>Track value by measuring optimization impact</li> </ul>"},{"location":"user-guide/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/overview/#operations","title":"Operations","text":"<p>An Operation is a single transformation that processes text:</p> <pre><code>from prompt_refiner import StripHTML\n\noperation = StripHTML()\nresult = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n# Output: \"Hello\"\n</code></pre> <p>All operations implement the same interface: <code>process(text: str) -&gt; str</code></p>"},{"location":"user-guide/overview/#pipelines","title":"Pipelines","text":"<p>A Pipeline chains multiple operations together:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Using the pipe operator (recommended)\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\nresult = pipeline.run(\"&lt;p&gt;Hello    World&lt;/p&gt;\")\n# Output: \"Hello World\"\n</code></pre> <p>Alternatively, use the fluent API: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"user-guide/overview/#the-4-modules","title":"The 4 Modules","text":"<ul> <li>Cleaner - Clean dirty data</li> <li>Compressor - Reduce size</li> <li>Scrubber - Security &amp; privacy</li> <li>Analyzer - Track metrics</li> </ul>"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about pipelines</li> <li>Create custom operations</li> <li>Browse examples</li> </ul>"},{"location":"user-guide/pipelines/","title":"Pipeline Basics","text":"<p>Learn how to build effective pipelines with Prompt Refiner.</p>"},{"location":"user-guide/pipelines/#two-ways-to-build-pipelines","title":"Two Ways to Build Pipelines","text":"<p>Prompt Refiner supports two syntax options for building pipelines:</p>"},{"location":"user-guide/pipelines/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<p>The pipe operator (<code>|</code>) provides a clean, Pythonic syntax similar to LangChain:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More concise - no need to import or instantiate <code>Refiner()</code> - Familiar to LangChain, LangGraph, and modern Python framework users - Cleaner visual appearance</p>"},{"location":"user-guide/pipelines/#fluent-api","title":"Fluent API","text":"<p>The fluent API uses method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(TruncateTokens(max_tokens=1000))\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More explicit - clear that you're creating a Refiner pipeline - Traditional method chaining pattern</p> <p>Choose One Style</p> <p>Pick one syntax style per project and use it consistently. Both work identically under the hood. Don't mix styles in the same pipeline.</p>"},{"location":"user-guide/pipelines/#the-pipeline-pattern","title":"The Pipeline Pattern","text":"<p>A pipeline chains operations in sequence:</p> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre> <p>All operations process the text in order, with each operation's output becoming the next operation's input.</p>"},{"location":"user-guide/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<ol> <li>Text enters the pipeline</li> <li>Each operation processes it in order</li> <li>Output of one operation becomes input of the next</li> <li>Final result is returned</li> </ol> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre>"},{"location":"user-guide/pipelines/#order-matters","title":"Order Matters","text":"<p>Operations run in the order you add them:</p> <pre><code># \u2705 Correct: Clean HTML first, then normalize\npipeline = StripHTML() | NormalizeWhitespace()\n\n# \u274c Wrong order - normalizes first, HTML remains\npipeline = NormalizeWhitespace() | StripHTML()\n</code></pre>"},{"location":"user-guide/pipelines/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/pipelines/#1-clean-before-compressing","title":"1. Clean Before Compressing","text":"<pre><code>pipeline = (\n    StripHTML()                  # Clean first\n    | NormalizeWhitespace()\n    | TruncateTokens()           # Then compress\n)\n</code></pre>"},{"location":"user-guide/pipelines/#2-compress-before-redacting","title":"2. Compress Before Redacting","text":"<pre><code>pipeline = (\n    TruncateTokens()  # Compress first\n    | RedactPII()     # Then redact\n)\n</code></pre>"},{"location":"user-guide/pipelines/#3-analyze-last","title":"3. Analyze Last","text":"<pre><code>counter = CountTokens(original_text=text)\npipeline = (\n    StripHTML()\n    | TruncateTokens()\n    | counter  # Analyze at the end\n)\n</code></pre>"},{"location":"user-guide/pipelines/#multiple-pipelines","title":"Multiple Pipelines","text":"<p>Create different pipelines for different use cases:</p> <pre><code># Pipeline for web content\nweb_pipeline = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n\n# Pipeline for RAG\nrag_pipeline = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n\n# Pipeline for secure processing\nsecure_pipeline = RedactPII()\n\n# Use them\ncleaned_web = web_pipeline.run(html_content)\noptimized_rag = rag_pipeline.run(rag_context)\nsafe_text = secure_pipeline.run(user_input)\n</code></pre> <p>Learn about custom operations \u2192</p>"}]}