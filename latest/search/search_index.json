{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prompt Refiner","text":"<p>A lightweight Python library for building production LLM applications. Save 5-70% on API costs - from function calling optimization to RAG context management.</p>"},{"location":"#overview","title":"Overview","text":"<p>Prompt Refiner solves three core problems for production LLM applications:</p> <ol> <li>Function Calling Optimization - Compress tool schemas by 57% on average with 100% lossless compression</li> <li>Token Optimization - Clean dirty inputs (HTML, whitespace, PII) to reduce API costs by 5-15%</li> <li>Context Management - Pack system prompts, RAG docs, and chat history with smart priority-based selection</li> </ol> <p>Perfect for AI agents, RAG applications, chatbots, and any production system that uses function calling or needs to manage LLM context windows efficiently.</p> <p>Proven Effectiveness</p> <p>Function Calling: Tested on 20 real-world API schemas (Stripe, Salesforce, HubSpot, Slack), achieving 56.9% average token reduction with 100% protocol field preservation and 100% callable (20/20 validated) with OpenAI function calling. Enterprise APIs see 70%+ reduction. A medium agent (10 tools, 500 calls/day) saves $541/month on GPT-4.</p> <p>RAG &amp; Text: Benchmarked on 30 real-world test cases, achieving 5-15% token reduction while maintaining 96-99% quality.</p> <p>Performance: Processing overhead is &lt; 0.5ms per 1k tokens - negligible compared to network and LLM latency.</p> <p>See comprehensive benchmark results \u2192</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#option-1-preset-strategies-easiest","title":"Option 1: Preset Strategies (Easiest)","text":"<p>New in v0.1.5: Use benchmark-tested preset strategies for instant token optimization:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy, AggressiveStrategy\n\n# Minimal: 4.3% reduction, 98.7% quality\nrefiner = MinimalStrategy().create_refiner()\ncleaned = refiner.run(\"&lt;div&gt;Your HTML content&lt;/div&gt;\")\n\n# Aggressive: 15% reduction, 96.4% quality\nrefiner = AggressiveStrategy(max_tokens=150).create_refiner()\ncleaned = refiner.run(long_context)\n</code></pre> <p>Learn more about strategies \u2192</p>"},{"location":"#option-2-custom-pipelines-flexible","title":"Option 2: Custom Pipelines (Flexible)","text":"<p>Build custom cleaning pipelines with the pipe operator:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\n# Define a cleaning pipeline\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000, strategy=\"middle_out\")\n)\n\nraw_input = \"&lt;div&gt;  User input with &lt;b&gt;lots&lt;/b&gt; of   spaces... &lt;/div&gt;\"\nclean_prompt = pipeline.run(raw_input)\n# Output: \"User input with lots of spaces...\"\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"#6-core-modules","title":"6 Core Modules","text":"<p>Prompt Refiner is organized into 6 specialized modules:</p>"},{"location":"#text-processing-operations","title":"Text Processing Operations","text":""},{"location":"#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<ul> <li>StripHTML() - Remove HTML tags, convert to Markdown</li> <li>NormalizeWhitespace() - Collapse excessive whitespace</li> <li>FixUnicode() - Remove zero-width spaces and problematic Unicode</li> <li>JsonCleaner() - Strip nulls/empties from JSON, minify</li> </ul> <p>Learn more about Cleaner \u2192</p>"},{"location":"#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<ul> <li>TruncateTokens() - Smart truncation with sentence boundaries<ul> <li>Strategies: <code>\"head\"</code>, <code>\"tail\"</code>, <code>\"middle_out\"</code></li> </ul> </li> <li>Deduplicate() - Remove similar content (great for RAG)</li> </ul> <p>Learn more about Compressor \u2192</p>"},{"location":"#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<ul> <li>RedactPII() - Automatically redact emails, phones, IPs, credit cards, URLs, SSNs</li> </ul> <p>Learn more about Scrubber \u2192</p>"},{"location":"#ai-agent-function-calling","title":"AI Agent &amp; Function Calling","text":""},{"location":"#4-tools-function-calling-optimization-v016","title":"4. Tools - Function Calling Optimization (v0.1.6+)","text":"<p>Dramatically reduce token costs for AI agents by compressing tool schemas and responses:</p> <ul> <li>SchemaCompressor() - Compress tool/function schemas by 57% on average<ul> <li>100% lossless - all protocol fields preserved</li> <li>Works with OpenAI and Anthropic function calling</li> <li>Enterprise APIs: 70%+ reduction</li> </ul> </li> <li>ResponseCompressor() - Compress verbose API responses by 30-70%<ul> <li>Removes debug/trace/logs fields</li> <li>Truncates long strings and lists</li> <li>Preserves essential data structure</li> </ul> </li> </ul> <pre><code>from prompt_refiner import SchemaCompressor, ResponseCompressor\nfrom pydantic import BaseModel\n\n# Compress tool schema (saves tokens on every request)\nclass SearchInput(BaseModel):\n    query: str\n    max_results: int = 10\n\ntool_schema = pydantic_function_tool(SearchInput, name=\"search\")\ncompressed = SchemaCompressor().process(tool_schema)\n# Use compressed schema in OpenAI/Anthropic function calling\n\n# Compress tool responses (saves tokens on responses)\nverbose_response = {\"results\": [...], \"debug_info\": {...}}\ncompact = ResponseCompressor().process(verbose_response)\n</code></pre> <p>Learn more about Tools \u2192</p>"},{"location":"#context-budget-management","title":"Context Budget Management","text":""},{"location":"#5-packer-intelligent-context-packing-v013","title":"5. Packer - Intelligent Context Packing (v0.1.3+)","text":"<p>For RAG applications and chatbots, the Packer module manages context budgets with priority-based selection:</p> <ul> <li>MessagesPacker() - For chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict]</code></li> <li>TextPacker() - For text completion APIs (Llama Base, GPT-3). Returns <code>str</code></li> </ul> <p>Key Features: - Smart priority-based selection (auto-prioritizes: system &gt; query &gt; context &gt; history) - JIT refinement with <code>refine_with</code> parameter - Automatic format overhead calculation - Semantic roles for clear intent</p> <pre><code>from prompt_refiner import MessagesPacker, StripHTML\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"You are helpful.\", role=\"system\")\n\n# Clean RAG documents on-the-fly\npacker.add(\n    \"&lt;div&gt;RAG doc...&lt;/div&gt;\",\n    role=\"context\",\n    refine_with=StripHTML()\n)\n\npacker.add(\"User question?\", role=\"query\")\n\nmessages = packer.pack()  # Returns List[Dict] ready for chat APIs\n</code></pre> <p>Learn more about Packer \u2192</p>"},{"location":"#5-strategy-preset-strategies-v015","title":"5. Strategy - Preset Strategies (v0.1.5+)","text":"<p>For quick setup, use benchmark-tested preset strategies:</p> <ul> <li>MinimalStrategy - 4.3% reduction, 98.7% quality (HTML + Whitespace)</li> <li>StandardStrategy - 4.8% reduction, 98.4% quality (+ Deduplication)</li> <li>AggressiveStrategy - 15% reduction, 96.4% quality (+ Truncation)</li> </ul> <pre><code>from prompt_refiner.strategy import StandardStrategy\n\n# Quick setup with preset\nrefiner = StandardStrategy().create_refiner()\ncleaned = refiner.run(\"&lt;div&gt;Your HTML content&lt;/div&gt;\")\n\n# Extend with additional operations\nrefiner.pipe(RedactPII(redact_types={\"email\"}))\n</code></pre> <p>Learn more about Strategy \u2192</p>"},{"location":"#measurement-analysis","title":"Measurement &amp; Analysis","text":"<p>Track optimization impact without transforming prompts:</p> <ul> <li>CountTokens() - Calculate token savings and ROI</li> <li>Estimation mode (default): Character-based approximation</li> <li>Precise mode (with tiktoken): Exact token counts</li> </ul> <p>Learn more about Analyzer \u2192</p>"},{"location":"#complete-example","title":"Complete Example","text":"<pre><code>from prompt_refiner import (\n    # Core Modules\n    StripHTML, NormalizeWhitespace, FixUnicode, JsonCleaner,  # Cleaner\n    Deduplicate, TruncateTokens,  # Compressor\n    RedactPII,  # Scrubber\n    # Measurement\n    CountTokens\n)\n\noriginal_text = \"\"\"Your messy input here...\"\"\"\n\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    # Clean\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n    | FixUnicode()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\"})\n    # Analyze\n    | counter\n)\n\nresult = pipeline.run(original_text)\nprint(counter.format_stats())  # Shows token savings\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Get Started</p> <p>Install Prompt Refiner and build your first pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting Started</p> </li> <li> <p>API Reference</p> <p>Complete API documentation for all operations and modules</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>Examples</p> <p>Browse practical examples for each module</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>Contributing</p> <p>Learn how to contribute to the project</p> <p>:octicons-arrow-right-24: Contributing Guide</p> </li> </ul>"},{"location":"benchmark/","title":"Benchmark Results","text":"<p>Prompt Refiner's effectiveness has been validated through 3 comprehensive benchmark suites covering function calling optimization, RAG applications, and performance.</p>"},{"location":"benchmark/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"benchmark/#function-calling-benchmark","title":"\u2b50 Function Calling Benchmark","text":"<p>SchemaCompressor tested on 20 real-world API schemas achieving 56.9% average token reduction with 100% lossless compression.</p> <p>Jump to Function Calling Benchmark \u2192</p>"},{"location":"benchmark/#rag-text-optimization","title":"\ud83d\udcda RAG &amp; Text Optimization","text":"<p>Comprehensive A/B testing on 30 real-world test cases measuring 5-15% token reduction and response quality preservation.</p> <p>Jump to RAG Benchmark \u2192</p>"},{"location":"benchmark/#latency-performance","title":"\u26a1 Latency &amp; Performance","text":"<p>Performance testing measuring processing overhead - &lt; 0.5ms per 1k tokens.</p> <p>Jump to Latency Benchmark \u2192</p>"},{"location":"benchmark/#function-calling-benchmark_1","title":"Function Calling Benchmark","text":"<p>SchemaCompressor was rigorously tested on 20 production API schemas from industry-leading platforms including Stripe, Salesforce, HubSpot, Slack, OpenAI, Anthropic, Google Calendar, Notion, and more.</p>"},{"location":"benchmark/#results-summary","title":"Results Summary","text":"Category Schemas Avg Reduction Top Performer Very Verbose (Enterprise APIs) 11 67.4% HubSpot Contact: 73.2% Complex (Rich APIs) 6 61.7% Slack Messaging: 70.8% Medium (Standard APIs) 2 13.1% Weather API: 20.1% Simple (Minimal APIs) 1 0.0% Calculator (already minimal) Overall Average 20 56.9% \u2014"},{"location":"benchmark/#key-highlights","title":"Key Highlights","text":"<ul> <li>\u2728 56.9% average reduction - 15,342 tokens saved across all schemas</li> <li>\ud83d\udd12 100% lossless compression - All protocol fields preserved (name, type, required, enum)</li> <li>\u2705 100% callable (20/20 validated) - All compressed schemas work correctly with OpenAI function calling</li> <li>\ud83c\udfe2 Enterprise APIs see 70%+ reduction - HubSpot (73.2%), OpenAI File Search (72.9%), Salesforce (72.1%)</li> <li>\ud83d\udcca Real-world schemas - Production APIs from Stripe, Slack, Twilio, SendGrid, etc.</li> <li>\u26a1 Zero API cost - Local processing with tiktoken</li> </ul>"},{"location":"benchmark/#functional-validation","title":"Functional Validation","text":"<p>We tested all 20 compressed schemas with real OpenAI function calling to prove they work correctly:</p> Category Schemas Identical Calls Different Args (Valid) Callable Rate Simple 1 1 (100%) 0 100% Medium 4 4 (100%) 0 100% Complex 6 4 (67%) 2 (33%) 100% Very Verbose 9 3 (33%) 6 (67%) 100% Overall 20 12 (60%) 8 (40%) 100% <p>Key Findings:</p> <ul> <li>\u2705 100% callable (20/20): Every compressed schema successfully triggers function calls</li> <li>\u2705 100% structurally valid: All function names, types, required fields preserved</li> <li>\u2705 60% identical (12/20): Majority produce exactly the same function call</li> <li>\u26a0\ufe0f 40% different but valid (8/20): Compressed descriptions influence LLM choices</li> <li>Different default values chosen (num_results: 10 \u2192 5, time_range: past_month \u2192 any)</li> <li>Different placeholder values (database: 'production' \u2192 'your_database_name')</li> <li>Different optional fields populated (location: 'Zoom' \u2192 'Conference Room A')</li> <li>All differences use valid enum/type values - schemas remain functionally correct</li> </ul> <p>Bottom Line: Compression doesn't break schemas - it's 100% safe for production use.</p>"},{"location":"benchmark/#top-performing-schemas","title":"Top Performing Schemas","text":"<ol> <li>HubSpot Contact Creation: 2,157 \u2192 578 tokens (73.2% reduction)</li> <li>OpenAI File Search: 2,019 \u2192 548 tokens (72.9% reduction)</li> <li>Salesforce Account Creation: 2,157 \u2192 602 tokens (72.1% reduction)</li> <li>Slack Send Message: 979 \u2192 286 tokens (70.8% reduction)</li> <li>Anthropic Computer Use: 1,598 \u2192 471 tokens (70.5% reduction)</li> </ol>"},{"location":"benchmark/#visualizations","title":"Visualizations","text":""},{"location":"benchmark/#token-reduction-by-category","title":"Token Reduction by Category","text":"<p>Complex and enterprise APIs achieve 60-70%+ token reduction</p>"},{"location":"benchmark/#cost-savings-projection","title":"Cost Savings Projection","text":"<p>Estimated monthly savings for different agent sizes (GPT-4 pricing)</p>"},{"location":"benchmark/#cost-savings-examples","title":"Cost Savings Examples","text":"<p>Real-world cost savings for AI agents with function calling:</p> Agent Size Tools Calls/Day Monthly Savings Annual Savings Small 5 100 $44 $528 Medium 10 500 $541 $6,492 Large 20 1,000 $3,249 $38,988 Enterprise 50 5,000 $40,664 $487,968 <p>Based on GPT-4 pricing ($0.03/1k input tokens) and 56.9% average reduction</p> <p>Why This Matters</p> <p>Function calling is one of the biggest sources of token consumption in AI agent systems. Verbose tool descriptions can consume thousands of tokens per request. SchemaCompressor optimizes documentation while preserving 100% of the protocol specification, making it completely safe to use in production.</p>"},{"location":"benchmark/#what-gets-compressed","title":"What Gets Compressed","text":"<p>SchemaCompressor optimizes:</p> <ul> <li>\u2705 Description fields (main source of verbosity)</li> <li>\u2705 Redundant explanations and examples</li> <li>\u2705 Marketing language and filler words</li> <li>\u2705 Overly detailed parameter descriptions</li> </ul> <p>SchemaCompressor NEVER modifies:</p> <ul> <li>\u274c Function name</li> <li>\u274c Parameter names</li> <li>\u274c Parameter types (string, number, boolean, etc.)</li> <li>\u274c Required fields</li> <li>\u274c Enum values</li> <li>\u274c Default values</li> <li>\u274c JSON structure</li> </ul>"},{"location":"benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>Want to validate these results yourself?</p> <pre><code># Install dependencies\nuv sync --group dev\n\n# Run benchmark (no API key needed!)\ncd benchmark/function_calling\npython benchmark_schemas.py\n\n# Generate visualizations\npython visualize_results.py\n</code></pre> <p>Cost: $0 (local token counting with tiktoken) Duration: ~1 minute</p> <p>Results are saved to <code>benchmark/function_calling/results/</code>: - <code>schema_compression_results.csv</code> - Full results table - <code>before_after_examples.md</code> - Top 3 examples with comparisons - <code>plots/</code> - Visualization charts</p> <p>View Full Function Calling Benchmark Documentation \u2192</p>"},{"location":"benchmark/#rag-text-optimization_1","title":"RAG &amp; Text Optimization","text":"<p>This benchmark validates token reduction and quality preservation for RAG applications and text optimization use cases.</p>"},{"location":"benchmark/#overview","title":"Overview","text":"<p>The benchmark measures two critical factors:</p> <ul> <li>Token Reduction - How much we can reduce prompt size (cost savings)</li> <li>Response Quality - Whether responses remain semantically equivalent</li> </ul> <p>Quality is evaluated using two methods: 1. Cosine Similarity - Semantic similarity of response embeddings (0-1 scale) 2. LLM Judge - GPT-4 evaluation of response equivalence</p>"},{"location":"benchmark/#results-summary_1","title":"Results Summary","text":"<p>We tested 3 optimization strategies on 30 test cases (15 SQuAD Q&amp;A pairs + 15 RAG scenarios):</p> Strategy Token Reduction Quality (Cosine) Judge Approval Overall Equivalent Minimal 4.3% 0.987 86.7% 86.7% Standard 4.8% 0.984 90.0% 86.7% Aggressive 15.0% 0.964 80.0% 66.7%"},{"location":"benchmark/#strategy-definitions","title":"Strategy Definitions","text":"<p>Minimal (Conservative cleaning): <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p> <p>Standard (Recommended for most use cases): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p> <p>Aggressive (Maximum savings): <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150, strategy=\"head\")\n)\n</code></pre></p>"},{"location":"benchmark/#key-findings","title":"Key Findings","text":""},{"location":"benchmark/#standard-strategy-best-balance","title":"\ud83c\udfaf Standard Strategy: Best Balance","text":"<p>The Standard strategy offers the best balance: - 4.8% token reduction with minimal quality impact - 90% judge approval - highest among all strategies - 0.984 cosine similarity - nearly perfect semantic preservation</p>"},{"location":"benchmark/#cost-savings","title":"\ud83d\udcb0 Cost Savings","text":"<p>Real-world cost savings for production applications:</p> GPT-4 TurboGPT-4 <p>Input cost: $0.01 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $4.30 $4.80 $15.00 1M tokens/month $43 $48 $150 10M tokens/month $430 $480 $1,500 <p>Input cost: $0.03 per 1K tokens</p> Volume Minimal (4.3%) Standard (4.8%) Aggressive (15%) 100K tokens/month $13 $14 $45 1M tokens/month $129 $144 $450 10M tokens/month $1,290 $1,440 $4,500"},{"location":"benchmark/#performance-by-scenario","title":"\ud83d\udcca Performance by Scenario","text":"<p>RAG Scenarios (with duplicates and HTML): - Minimal: 17% reduction on average - Standard: 31% reduction on average - Aggressive: 49% reduction on complex documents</p> <p>SQuAD Q&amp;A (clean academic text): - All strategies: 2-5% reduction (less messy data = less to clean)</p> <p>Key Insight</p> <p>Token savings scale with input messiness. RAG contexts with HTML, duplicates, and whitespace see 3-10x more reduction than clean text.</p>"},{"location":"benchmark/#visualizations_1","title":"Visualizations","text":""},{"location":"benchmark/#token-reduction-vs-quality","title":"Token Reduction vs Quality","text":"<p>The scatter plot shows each strategy's position in the cost-quality tradeoff space. Standard strategy achieves near-optimal quality while maintaining solid savings.</p>"},{"location":"benchmark/#test-dataset","title":"Test Dataset","text":"<p>The benchmark uses 30 carefully curated test cases:</p>"},{"location":"benchmark/#squad-samples-15-cases","title":"SQuAD Samples (15 cases)","text":"<p>Question-answer pairs with context covering: - History (\"When did Beyonce start becoming popular?\") - Science (\"What is DNA?\") - Geography, literature, technology</p>"},{"location":"benchmark/#rag-scenarios-15-cases","title":"RAG Scenarios (15 cases)","text":"<p>Realistic retrieval-augmented generation use cases: - E-commerce product catalogs with HTML - Documentation with excessive whitespace - Customer support tickets with duplicates - Code search results - Recipe collections</p>"},{"location":"benchmark/#running-the-benchmark_1","title":"Running the Benchmark","text":"<p>Want to validate these results yourself?</p>"},{"location":"benchmark/#prerequisites","title":"Prerequisites","text":"<pre><code># Install dependencies\nuv sync --group dev\n\n# Set up OpenAI API key\ncd benchmark/rag_quality\ncp .env.example .env\n# Edit .env and add your OPENAI_API_KEY\n</code></pre>"},{"location":"benchmark/#run-the-benchmark","title":"Run the Benchmark","text":"<pre><code>cd benchmark/rag_quality\npython benchmark.py\n</code></pre> <p>This will: 1. Test 30 cases with 3 strategies (90 total comparisons) 2. Generate detailed report with visualizations 3. Save results to <code>./results/</code> directory</p> <p>Estimated cost: ~$2-5 per full run (using gpt-4o-mini)</p>"},{"location":"benchmark/#advanced-options","title":"Advanced Options","text":"<pre><code># Use a different model\npython benchmark.py --model gpt-4o\n\n# Test specific strategies only\npython benchmark.py --strategies minimal standard\n\n# Use fewer test cases (faster, cheaper)\npython benchmark.py --limit 10\n</code></pre>"},{"location":"benchmark/#recommendations","title":"Recommendations","text":"<p>Based on benchmark results:</p>"},{"location":"benchmark/#for-production-rag-applications","title":"For Production RAG Applications","text":"<p>Use Standard strategy - Best balance of savings and quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n)\n</code></pre></p>"},{"location":"benchmark/#for-high-volume-cost-sensitive-applications","title":"For High-Volume, Cost-Sensitive Applications","text":"<p>Consider Aggressive strategy if 15% cost reduction outweighs slightly lower quality <pre><code>pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=150)\n)\n</code></pre></p>"},{"location":"benchmark/#for-quality-critical-applications","title":"For Quality-Critical Applications","text":"<p>Use Minimal strategy for maximum quality preservation <pre><code>pipeline = StripHTML() | NormalizeWhitespace()\n</code></pre></p>"},{"location":"benchmark/#latency-performance_1","title":"Latency &amp; Performance","text":"<p>\"What's the latency overhead?\" - Negligible. Prompt Refiner adds &lt; 0.5ms per 1k tokens of overhead.</p>"},{"location":"benchmark/#performance-results","title":"Performance Results","text":"Strategy @ 1k tokens @ 10k tokens @ 50k tokens Overhead per 1k tokens Minimal (HTML + Whitespace) 0.05ms 0.48ms 2.39ms 0.05ms Standard (+ Deduplicate) 0.26ms 2.47ms 12.27ms 0.25ms Aggressive (+ Truncate) 0.26ms 2.46ms 12.38ms 0.25ms"},{"location":"benchmark/#key-performance-insights","title":"Key Performance Insights","text":"<ul> <li>\u26a1 Minimal strategy: Only 0.05ms per 1k tokens (faster than a network packet)</li> <li>\ud83c\udfaf Standard strategy: 0.25ms per 1k tokens - adds ~2.5ms to a 10k token prompt</li> <li>\ud83d\udcca Context: Network + LLM TTFT is typically 600ms+, refining adds &lt; 0.5% overhead</li> <li>\ud83d\ude80 Individual operations (HTML, whitespace) are &lt; 0.5ms per 1k tokens</li> </ul>"},{"location":"benchmark/#real-world-impact","title":"Real-World Impact","text":"<pre><code>10k token RAG context refining: ~2.5ms overhead\nNetwork latency: ~100ms\nLLM Processing (TTFT): ~500ms+\nTotal overhead: &lt; 0.5% of request time\n</code></pre> <p>Performance Takeaway</p> <p>Refining overhead is negligible compared to network + LLM latency (600ms+). Standard refining adds ~2.5ms overhead - less than 0.5% of total request time.</p>"},{"location":"benchmark/#running-the-latency-benchmark","title":"Running the Latency Benchmark","text":"<p>The latency benchmark requires no API keys and runs completely offline:</p> <pre><code>cd benchmark/latency\npython benchmark.py\n</code></pre> <p>This will: 1. Test individual operations at multiple scales (1k, 10k, 50k tokens) 2. Test complete strategies (Minimal, Standard, Aggressive) 3. Report average, median, and P95 latency metrics 4. Show per-1k-token normalized overhead</p> <p>Cost: $0 (runs locally, no API calls)</p> <p>Duration: ~30-60 seconds</p>"},{"location":"benchmark/#learn-more","title":"Learn More","text":"<ul> <li>View Quality Benchmark Documentation</li> <li>View Latency Benchmark Documentation</li> <li>Browse Test Cases</li> <li>Examine Raw Results</li> </ul>"},{"location":"benchmark/#contributing","title":"Contributing","text":"<p>Have ideas to improve the benchmark? We welcome: - New test cases (especially domain-specific scenarios) - Additional evaluation metrics - Alternative refining strategies - Multi-model comparisons</p> <p>Open an issue or submit a PR!</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Prompt Refiner!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>uv package manager</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run tests\nmake test\n\n# Format code\nmake format\n\n# Run linter\nmake lint\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>prompt-refiner/\n\u251c\u2500\u2500 src/prompt_refiner/     # Source code\n\u2502   \u251c\u2500\u2500 cleaner/           # Cleaner module\n\u2502   \u251c\u2500\u2500 compressor/        # Compressor module\n\u2502   \u251c\u2500\u2500 scrubber/          # Scrubber module\n\u2502   \u2514\u2500\u2500 analyzer/          # Analyzer module\n\u251c\u2500\u2500 tests/                 # Test files\n\u251c\u2500\u2500 examples/              # Example scripts\n\u2514\u2500\u2500 docs/                  # Documentation\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following existing patterns</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_cleaner.py -v\n</code></pre>"},{"location":"contributing/#4-format-code","title":"4. Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"contributing/#5-commit-changes","title":"5. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre>"},{"location":"contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Write clear docstrings (Google style)</li> <li>Keep functions small and focused</li> </ul>"},{"location":"contributing/#example","title":"Example","text":"<pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text\n    \"\"\"\n    return text.strip()\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Aim for high test coverage</li> <li>Test edge cases</li> </ul> <pre><code>def test_strip_html():\n    operation = StripHTML()\n    result = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n    assert result == \"Hello\"\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update relevant documentation files</li> <li>Add examples for new features</li> <li>Keep API reference up to date (auto-generated from docstrings)</li> </ul>"},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code>make docs-serve\n</code></pre> <p>Then visit http://127.0.0.1:8000</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get up and running with Prompt Refiner in minutes.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"Default (Lightweight)With Precise Token Counting <p>Zero dependencies - perfect for most use cases:</p> <pre><code>pip install llm-prompt-refiner\n</code></pre> <p>Install with optional <code>tiktoken</code> for precise token counting:</p> <pre><code>pip install llm-prompt-refiner[token]\n</code></pre> <p>Then opt-in by passing a <code>model</code> parameter:</p> <pre><code>from prompt_refiner import CountTokens, MessagesPacker\n\ncounter = CountTokens(model=\"gpt-4\")  # Precise token counting\npacker = MessagesPacker()  # Context composition\n</code></pre>"},{"location":"getting-started/#your-first-pipeline","title":"Your First Pipeline","text":"<p>Let's create a simple pipeline to clean HTML and normalize whitespace:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Process some text\nraw_input = \"\"\"\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Welcome&lt;/h1&gt;\n        &lt;p&gt;This  has    excessive   spaces.&lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nclean_output = pipeline.run(raw_input)\nprint(clean_output)\n# Output: \"Welcome This has excessive spaces.\"\n</code></pre>"},{"location":"getting-started/#understanding-the-pipeline-pattern","title":"Understanding the Pipeline Pattern","text":"<p>Prompt Refiner uses a pipeline pattern where you chain operations together:</p> <ol> <li>Create operations - Initialize the operations you need</li> <li>Chain with <code>|</code> operator - Combine operations in order</li> <li>Run with <code>.run()</code> - Execute the pipeline on your text</li> </ol> <pre><code>pipeline = (\n    Operation1()            # 1. Create operations\n    | Operation2()          # 2. Chain with | operator\n    | Operation3()\n)\n\nresult = pipeline.run(text)  # 3. Run\n</code></pre> <p>Alternative: Fluent API</p> <p>Prefer method chaining? Use the traditional fluent API with <code>Refiner().pipe()</code>: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(Operation1())\n    .pipe(Operation2())\n    .pipe(Operation3())\n)\n</code></pre></p> <p>Order Matters</p> <p>Operations run in the order you add them. For example, you should typically clean HTML before normalizing whitespace.</p>"},{"location":"getting-started/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/#pattern-1-web-content-cleaning","title":"Pattern 1: Web Content Cleaning","text":"<p>Clean content scraped from the web:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)  # Convert to Markdown\n    | FixUnicode()               # Fix Unicode issues\n    | NormalizeWhitespace()      # Normalize spaces\n)\n</code></pre>"},{"location":"getting-started/#pattern-2-rag-context-optimization","title":"Pattern 2: RAG Context Optimization","text":"<p>Optimize retrieved context for RAG applications:</p> <pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate(similarity_threshold=0.85)  # Remove duplicates\n    | TruncateTokens(max_tokens=2000)       # Fit in context window\n)\n</code></pre>"},{"location":"getting-started/#pattern-3-secure-pii-handling","title":"Pattern 3: Secure PII Handling","text":"<p>Redact sensitive information before sending to APIs:</p> <pre><code>from prompt_refiner import RedactPII\n\nsecure_pipeline = RedactPII(redact_types={\"email\", \"phone\", \"ssn\"})\n</code></pre>"},{"location":"getting-started/#pattern-4-full-optimization-with-tracking","title":"Pattern 4: Full Optimization with Tracking","text":"<p>Complete optimization with metrics:</p> <pre><code>from prompt_refiner import (\n    StripHTML, NormalizeWhitespace,\n    TruncateTokens, RedactPII, CountTokens\n)\n\noriginal_text = \"Your text here...\"\ncounter = CountTokens(original_text=original_text)\n\nfull_pipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n    | RedactPII()\n    | counter\n)\n\nresult = full_pipeline.run(original_text)\nprint(counter.format_stats())\n</code></pre>"},{"location":"getting-started/#pattern-5-advanced-rag-with-context-budget-v013","title":"Pattern 5: Advanced - RAG with Context Budget (v0.1.3+)","text":"<p>For RAG applications, manage context budgets with smart priority-based packing:</p> <pre><code>from prompt_refiner import MessagesPacker, StripHTML, NormalizeWhitespace\n\npacker = MessagesPacker()\n\n# System prompt (auto-prioritized: highest)\npacker.add(\n    \"Answer based on provided context.\",\n    role=\"system\"\n)\n\n# RAG documents with JIT cleaning (auto-prioritized: high)\npacker.add(\n    \"&lt;div&gt;Document 1...&lt;/div&gt;\",\n    role=\"context\",\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n\n# Current user query (auto-prioritized: critical)\npacker.add(\n    \"What is the answer?\",\n    role=\"query\"\n)\n\nmessages = packer.pack()  # Ready for chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"getting-started/#proven-results","title":"Proven Results","text":"<p>Curious about the real-world effectiveness? Check out our comprehensive benchmark results:</p> <p>Benchmark Highlights</p> <ul> <li>4-15% token reduction across 30 test cases</li> <li>96-99% quality preservation (cosine similarity + LLM judge)</li> <li>Real cost savings: $48-$150/month per 1M tokens</li> </ul> <p>View Full Benchmark \u2192</p>"},{"location":"getting-started/#exploring-modules","title":"Exploring Modules","text":"<p>Prompt Refiner has 5 specialized modules:</p> <ul> <li>Cleaner - Clean dirty data (HTML, whitespace, Unicode, JSON)</li> <li>Compressor - Reduce size (truncation, deduplication)</li> <li>Scrubber - Security and privacy (PII redaction)</li> <li>Packer - Context budget management for RAG and chatbots (v0.1.3+)</li> <li>Strategy - Preset strategies for quick setup (v0.1.5+)</li> <li>Analyzer - Metrics and analysis (token counting)</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn the Modules</p> <p>Deep dive into each of the 5 core modules</p> <p>:octicons-arrow-right-24: Modules Overview</p> </li> <li> <p>Browse Examples</p> <p>See practical examples for each operation</p> <p>:octicons-arrow-right-24: Examples</p> </li> <li> <p>API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for all Prompt Refiner classes and operations.</p> <p>This section contains auto-generated documentation from the codebase docstrings. All operations inherit from the base <code>Operation</code> class and implement a <code>process(text: str) -&gt; str</code> method.</p>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":"<ul> <li> <p>:material-pipe:{ .lg .middle } Refiner</p> <p>Pipeline builder for chaining operations</p> <p>:octicons-arrow-right-24: Refiner API</p> </li> <li> <p>:material-broom:{ .lg .middle } Cleaner</p> <p>Operations for cleaning dirty data</p> <p>:octicons-arrow-right-24: Cleaner API</p> </li> <li> <p>:material-compress:{ .lg .middle } Compressor</p> <p>Operations for reducing size</p> <p>:octicons-arrow-right-24: Compressor API</p> </li> <li> <p>:material-shield-lock:{ .lg .middle } Scrubber</p> <p>Operations for security and privacy</p> <p>:octicons-arrow-right-24: Scrubber API</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Analyzer</p> <p>Operations for metrics and analysis</p> <p>:octicons-arrow-right-24: Analyzer API</p> </li> <li> <p>:material-package-variant:{ .lg .middle } Packer</p> <p>Context budget management with priorities</p> <p>:octicons-arrow-right-24: Packer API</p> </li> </ul>"},{"location":"api-reference/#operation-base-class","title":"Operation Base Class","text":"<p>All operations in Prompt Refiner inherit from the <code>Operation</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Operation(ABC):\n    @abstractmethod\n    def process(self, text: str) -&gt; str:\n        \"\"\"Process the input text and return the result.\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#usage-pattern","title":"Usage Pattern","text":"<p>Operations are used within a <code>Refiner</code> pipeline:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = refiner.run(\"Your text here...\")\n</code></pre>"},{"location":"api-reference/#module-organization","title":"Module Organization","text":"<ul> <li>Refiner - Core pipeline builder class</li> <li>Cleaner - <code>StripHTML</code>, <code>NormalizeWhitespace</code>, <code>FixUnicode</code></li> <li>Compressor - <code>TruncateTokens</code>, <code>Deduplicate</code></li> <li>Scrubber - <code>RedactPII</code></li> <li>Analyzer - <code>CountTokens</code></li> <li>Packer - <code>ContextPacker</code></li> </ul>"},{"location":"api-reference/analyzer/","title":"Analyzer Module","text":"<p>The Analyzer module provides utilities for measuring optimization impact and tracking token savings.</p>"},{"location":"api-reference/analyzer/#tokentracker","title":"TokenTracker","text":"<p>Context manager for tracking token usage before and after refinement operations.</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker","title":"prompt_refiner.analyzer.TokenTracker","text":"<pre><code>TokenTracker(refiner, token_counter)\n</code></pre> <p>Context manager for tracking token usage in refiners/pipelines.</p> <p>Wraps any Refiner (operation, pipeline, or strategy) and tracks token counts before and after processing. Users provide their own token counting function for maximum flexibility.</p> Example <p>def count_tokens(text: str) -&gt; int: ...     # User's custom counter (tiktoken, character-based, etc.) ...     return len(text) // 4</p> <p>refiner = StripHTML() | NormalizeWhitespace() with TokenTracker(refiner, count_tokens) as tracker: ...     result = tracker.process(\"Hello   World\")</p> <p>print(tracker.stats)</p> <p>Initialize token tracker.</p> <p>Parameters:</p> Name Type Description Default <code>refiner</code> <code>Refiner</code> <p>Any Refiner (operation or pipeline) to track</p> required <code>token_counter</code> <code>Callable[[str], int]</code> <p>Function that counts tokens in text. Should accept a string and return an integer token count.</p> required Source code in <code>src/prompt_refiner/analyzer/token_tracker.py</code> <pre><code>def __init__(\n    self,\n    refiner: Refiner,\n    token_counter: Callable[[str], int],\n):\n    \"\"\"\n    Initialize token tracker.\n\n    Args:\n        refiner: Any Refiner (operation or pipeline) to track\n        token_counter: Function that counts tokens in text.\n            Should accept a string and return an integer token count.\n    \"\"\"\n    self._refiner = refiner\n    self._counter = token_counter\n    self._original_tokens: Optional[int] = None\n    self._refined_tokens: Optional[int] = None\n    self._original_text: Optional[str] = None\n    self._result: Optional[str] = None\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker-attributes","title":"Attributes","text":""},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.stats","title":"stats  <code>property</code>","text":"<pre><code>stats\n</code></pre> <p>Get token statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with:</p> <code>dict</code> <ul> <li>original_tokens: Tokens before processing</li> </ul> <code>dict</code> <ul> <li>refined_tokens: Tokens after processing</li> </ul> <code>dict</code> <ul> <li>saved_tokens: Tokens saved (original - refined)</li> </ul> <code>dict</code> <ul> <li>saving_percent: Percentage saved as formatted string (e.g., \"12.5%\")</li> </ul> <code>dict</code> <p>Returns empty dict if process() hasn't been called yet.</p> Example <p>with TokenTracker(StripHTML(), lambda t: len(t)//4) as tracker: ...     tracker.process(\"Test\") ...     stats = tracker.stats ...     print(f\"Saved {stats['saved_tokens']} tokens\") Saved 3 tokens</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.original_text","title":"original_text  <code>property</code>","text":"<pre><code>original_text\n</code></pre> <p>Get the original input text.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The text passed to process(), or None if process() hasn't been called</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.result","title":"result  <code>property</code>","text":"<pre><code>result\n</code></pre> <p>Get the processed result text.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The refined text returned by process(), or None if process() hasn't been called</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker-functions","title":"Functions","text":""},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Enter context - returns self for method access.</p> Source code in <code>src/prompt_refiner/analyzer/token_tracker.py</code> <pre><code>def __enter__(self) -&gt; \"TokenTracker\":\n    \"\"\"Enter context - returns self for method access.\"\"\"\n    return self\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit context - cleanup if needed.</p> Source code in <code>src/prompt_refiner/analyzer/token_tracker.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit context - cleanup if needed.\"\"\"\n    # No cleanup needed, but required for context manager protocol\n    pass\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.TokenTracker.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Process text through refiner and track tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>Processed text from the refiner</p> Example <p>with TokenTracker(StripHTML(), lambda t: len(t)//4) as tracker: ...     result = tracker.process(\"<p>Hello</p>\") ...     print(tracker.stats[\"saved_tokens\"]) 3</p> Source code in <code>src/prompt_refiner/analyzer/token_tracker.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Process text through refiner and track tokens.\n\n    Args:\n        text: Input text to process\n\n    Returns:\n        Processed text from the refiner\n\n    Example:\n        &gt;&gt;&gt; with TokenTracker(StripHTML(), lambda t: len(t)//4) as tracker:\n        ...     result = tracker.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n        ...     print(tracker.stats[\"saved_tokens\"])\n        3\n    \"\"\"\n    # Track original\n    self._original_text = text\n    self._original_tokens = self._counter(text)\n\n    # Process through refiner\n    self._result = self._refiner.process(text)\n\n    # Track refined\n    self._refined_tokens = self._counter(self._result)\n\n    return self._result\n</code></pre>"},{"location":"api-reference/analyzer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TokenTracker, StripHTML, character_based_counter\n\nrefiner = StripHTML()\n\nwith TokenTracker(refiner, character_based_counter) as tracker:\n    result = tracker.process(\"&lt;div&gt;Hello World&lt;/div&gt;\")\n\nprint(tracker.stats)\n# {'original_tokens': 6, 'refined_tokens': 3, 'saved_tokens': 3, 'saving_percent': '50.0%'}\n</code></pre>"},{"location":"api-reference/analyzer/#pipeline-tracking","title":"Pipeline Tracking","text":"<pre><code>from prompt_refiner import (\n    TokenTracker,\n    StripHTML,\n    NormalizeWhitespace,\n    character_based_counter,\n)\n\n# Track entire pipeline\npipeline = StripHTML() | NormalizeWhitespace()\n\nwith TokenTracker(pipeline, character_based_counter) as tracker:\n    result = tracker.process(\"&lt;p&gt;Hello    World   &lt;/p&gt;\")\n\nstats = tracker.stats\nprint(f\"Saved {stats['saved_tokens']} tokens ({stats['saving_percent']})\")\n# Saved 4 tokens (66.7%)\n</code></pre>"},{"location":"api-reference/analyzer/#strategy-tracking","title":"Strategy Tracking","text":"<pre><code>from prompt_refiner import (\n    TokenTracker,\n    StandardStrategy,\n    character_based_counter,\n)\n\n# Track preset strategies\nstrategy = StandardStrategy()\n\nwith TokenTracker(strategy, character_based_counter) as tracker:\n    result = tracker.process(\"&lt;div&gt;Messy    input   text&lt;/div&gt;\")\n\nprint(tracker.stats)\n</code></pre>"},{"location":"api-reference/analyzer/#token-counter-functions","title":"Token Counter Functions","text":"<p>Built-in token counting functions for different use cases.</p>"},{"location":"api-reference/analyzer/#character_based_counter","title":"character_based_counter","text":"<p>Fast approximation using ~1 token \u2248 4 characters. Good for general use.</p> <pre><code>from prompt_refiner import character_based_counter\n\ntokens = character_based_counter(\"Hello World\")\nprint(tokens)  # 3\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.character_based_counter","title":"prompt_refiner.analyzer.character_based_counter","text":"<pre><code>character_based_counter(text)\n</code></pre> <p>Estimate tokens using character-based approximation.</p> <p>Uses conservative estimate: 1 token \u2248 4 characters. Fast but less accurate than model-specific counters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Example <p>character_based_counter(\"Hello World\") 3 character_based_counter(\"A longer sentence with many words\") 9</p> Source code in <code>src/prompt_refiner/analyzer/token_counters.py</code> <pre><code>def character_based_counter(text: str) -&gt; int:\n    \"\"\"\n    Estimate tokens using character-based approximation.\n\n    Uses conservative estimate: 1 token \u2248 4 characters.\n    Fast but less accurate than model-specific counters.\n\n    Args:\n        text: Input text to count tokens for\n\n    Returns:\n        Estimated token count\n\n    Example:\n        &gt;&gt;&gt; character_based_counter(\"Hello World\")\n        3\n        &gt;&gt;&gt; character_based_counter(\"A longer sentence with many words\")\n        9\n    \"\"\"\n    if not text:\n        return 0\n    return math.ceil(len(text) / 4)\n</code></pre>"},{"location":"api-reference/analyzer/#word_based_counter","title":"word_based_counter","text":"<p>Simple approximation using ~1 token \u2248 1 word. Reasonable for English text.</p> <pre><code>from prompt_refiner import word_based_counter\n\ntokens = word_based_counter(\"Hello World\")\nprint(tokens)  # 2\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.word_based_counter","title":"prompt_refiner.analyzer.word_based_counter","text":"<pre><code>word_based_counter(text)\n</code></pre> <p>Estimate tokens using word count approximation.</p> <p>Uses estimate: 1 token \u2248 1 word. Reasonable for English text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count based on word splits</p> Example <p>word_based_counter(\"Hello World\") 2 word_based_counter(\"A longer sentence with many words\") 6</p> Source code in <code>src/prompt_refiner/analyzer/token_counters.py</code> <pre><code>def word_based_counter(text: str) -&gt; int:\n    \"\"\"\n    Estimate tokens using word count approximation.\n\n    Uses estimate: 1 token \u2248 1 word.\n    Reasonable for English text.\n\n    Args:\n        text: Input text to count tokens for\n\n    Returns:\n        Estimated token count based on word splits\n\n    Example:\n        &gt;&gt;&gt; word_based_counter(\"Hello World\")\n        2\n        &gt;&gt;&gt; word_based_counter(\"A longer sentence with many words\")\n        6\n    \"\"\"\n    if not text:\n        return 0\n    return len(text.split())\n</code></pre>"},{"location":"api-reference/analyzer/#create_tiktoken_counter","title":"create_tiktoken_counter","text":"<p>Precise token counting using OpenAI's tiktoken. Requires optional dependency.</p> <pre><code>from prompt_refiner import create_tiktoken_counter\n\n# Requires: pip install llm-prompt-refiner[token]\ncounter = create_tiktoken_counter(model=\"gpt-4\")\n\ntokens = counter(\"Hello World\")\nprint(tokens)  # Exact token count for GPT-4\n</code></pre> <p>Optional Dependency</p> <p><code>create_tiktoken_counter</code> requires tiktoken to be installed:</p> <pre><code>pip install llm-prompt-refiner[token]\n</code></pre> <p>If tiktoken is not available, use <code>character_based_counter</code> or <code>word_based_counter</code> instead.</p>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.create_tiktoken_counter","title":"prompt_refiner.analyzer.create_tiktoken_counter","text":"<pre><code>create_tiktoken_counter(model='gpt-4')\n</code></pre> <p>Create a tiktoken-based counter for precise token counting.</p> <p>Requires tiktoken to be installed. Use this for accurate token counts when working with specific models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name for tokenizer selection (e.g., \"gpt-4\", \"gpt-3.5-turbo\")</p> <code>'gpt-4'</code> <p>Returns:</p> Type Description <code>Callable[[str], int]</code> <p>Token counting function that uses tiktoken</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tiktoken is not installed</p> Example <p>counter = create_tiktoken_counter(model=\"gpt-4\") counter(\"Hello World\") 2</p> Source code in <code>src/prompt_refiner/analyzer/token_counters.py</code> <pre><code>def create_tiktoken_counter(model: str = \"gpt-4\") -&gt; Callable[[str], int]:\n    \"\"\"\n    Create a tiktoken-based counter for precise token counting.\n\n    Requires tiktoken to be installed. Use this for accurate token counts\n    when working with specific models.\n\n    Args:\n        model: Model name for tokenizer selection (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n\n    Returns:\n        Token counting function that uses tiktoken\n\n    Raises:\n        ImportError: If tiktoken is not installed\n\n    Example:\n        &gt;&gt;&gt; counter = create_tiktoken_counter(model=\"gpt-4\")\n        &gt;&gt;&gt; counter(\"Hello World\")\n        2\n\n        &gt;&gt;&gt; # If tiktoken not installed:\n        &gt;&gt;&gt; try:\n        ...     counter = create_tiktoken_counter()\n        ... except ImportError as e:\n        ...     print(\"Install tiktoken: pip install llm-prompt-refiner[token]\")\n    \"\"\"\n    try:\n        import tiktoken\n    except ImportError:\n        raise ImportError(\n            \"tiktoken is required for precise token counting. \"\n            \"Install with: pip install llm-prompt-refiner[token]\"\n        )\n\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        # Fallback to cl100k_base (used by gpt-4, gpt-3.5-turbo)\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n    def counter(text: str) -&gt; int:\n        \"\"\"Count tokens using tiktoken encoding.\"\"\"\n        if not text:\n            return 0\n        return len(encoding.encode(text))\n\n    return counter\n</code></pre>"},{"location":"api-reference/analyzer/#prompt_refiner.analyzer.create_tiktoken_counter--if-tiktoken-not-installed","title":"If tiktoken not installed:","text":"<p>try: ...     counter = create_tiktoken_counter() ... except ImportError as e: ...     print(\"Install tiktoken: pip install llm-prompt-refiner[token]\")</p>"},{"location":"api-reference/analyzer/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/analyzer/#roi-demonstration","title":"ROI Demonstration","text":"<p>Track token savings to demonstrate optimization value:</p> <pre><code>from prompt_refiner import (\n    TokenTracker,\n    StandardStrategy,\n    character_based_counter,\n)\n\n# Your messy input\noriginal = \"&lt;div&gt;Lots of HTML and   extra   whitespace&lt;/div&gt;\"\n\n# Track optimization\nstrategy = StandardStrategy()\nwith TokenTracker(strategy, character_based_counter) as tracker:\n    result = tracker.process(original)\n\n# Show ROI\nstats = tracker.stats\nprint(f\"Original: {stats['original_tokens']} tokens\")\nprint(f\"Refined: {stats['refined_tokens']} tokens\")\nprint(f\"Saved: {stats['saved_tokens']} tokens ({stats['saving_percent']})\")\n\n# Calculate cost savings (example: $0.03 per 1K tokens)\ncost_per_token = 0.03 / 1000\nsavings = stats['saved_tokens'] * cost_per_token\nprint(f\"Cost savings: ${savings:.4f} per request\")\n</code></pre>"},{"location":"api-reference/analyzer/#ab-testing-different-strategies","title":"A/B Testing Different Strategies","text":"<p>Compare multiple optimization approaches:</p> <pre><code>from prompt_refiner import (\n    TokenTracker,\n    MinimalStrategy,\n    StandardStrategy,\n    AggressiveStrategy,\n    character_based_counter,\n)\n\noriginal = \"Your test text here...\"\n\n# Test different strategies\nstrategies = {\n    \"Minimal\": MinimalStrategy(),\n    \"Standard\": StandardStrategy(),\n    \"Aggressive\": AggressiveStrategy(),\n}\n\nfor name, strategy in strategies.items():\n    with TokenTracker(strategy, character_based_counter) as tracker:\n        result = tracker.process(original)\n\n    stats = tracker.stats\n    print(f\"{name}: {stats['saved_tokens']} tokens saved ({stats['saving_percent']})\")\n</code></pre>"},{"location":"api-reference/analyzer/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Track optimization in production:</p> <pre><code>import logging\nfrom prompt_refiner import (\n    TokenTracker,\n    StandardStrategy,\n    character_based_counter,\n)\n\nlogger = logging.getLogger(__name__)\n\ndef process_user_input(text: str) -&gt; str:\n    strategy = StandardStrategy()\n\n    with TokenTracker(strategy, character_based_counter) as tracker:\n        result = tracker.process(text)\n\n    stats = tracker.stats\n    logger.info(\n        f\"Processed input: \"\n        f\"original={stats['original_tokens']} tokens, \"\n        f\"refined={stats['refined_tokens']} tokens, \"\n        f\"saved={stats['saved_tokens']} tokens ({stats['saving_percent']})\"\n    )\n\n    return result\n</code></pre>"},{"location":"api-reference/analyzer/#packer-token-tracking","title":"Packer Token Tracking","text":"<p>Packers have built-in token tracking support:</p> <pre><code>from prompt_refiner import MessagesPacker, character_based_counter\n\npacker = MessagesPacker(\n    track_tokens=True,\n    token_counter=character_based_counter,\n    system=\"&lt;div&gt;You are helpful.&lt;/div&gt;\",\n    context=[\"&lt;p&gt;Doc 1&lt;/p&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"],\n    query=\"&lt;span&gt;What's the weather?&lt;/span&gt;\",\n)\n\nmessages = packer.pack()\n\n# Get token savings from automatic cleaning\nstats = packer.token_stats\nprint(f\"Saved {stats['saved_tokens']} tokens through automatic refinement\")\n</code></pre>"},{"location":"api-reference/analyzer/#choosing-a-token-counter","title":"Choosing a Token Counter","text":"<p>Which Counter Should I Use?</p> <p>For development and testing: - Use <code>character_based_counter</code> - fast and no dependencies</p> <p>For production cost estimation: - Use <code>create_tiktoken_counter(model=\"gpt-4\")</code> for precise costs - Requires: <code>pip install llm-prompt-refiner[token]</code></p> <p>For simple approximation: - Use <code>word_based_counter</code> for English text</p>"},{"location":"api-reference/analyzer/#tips","title":"Tips","text":"<p>Context Manager Best Practice</p> <p>Always use TokenTracker as a context manager with <code>with</code> statement:</p> <pre><code>with TokenTracker(refiner, counter) as tracker:\n    result = tracker.process(text)\n# Stats available after processing\nstats = tracker.stats\n</code></pre> <p>Custom Token Counters</p> <p>You can provide any callable that takes a string and returns an int:</p> <pre><code>def my_custom_counter(text: str) -&gt; int:\n    # Your custom logic here\n    return len(text) // 3  # Example: 1 token \u2248 3 chars\n\nwith TokenTracker(refiner, my_custom_counter) as tracker:\n    result = tracker.process(text)\n</code></pre> <p>Access to Original and Result</p> <p>TokenTracker provides properties to access the original and refined text:</p> <pre><code>with TokenTracker(refiner, counter) as tracker:\n    result = tracker.process(text)\n\nprint(tracker.original_text)  # Original input\nprint(tracker.result)         # Refined output\n</code></pre>"},{"location":"api-reference/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data, including HTML removal, whitespace normalization, Unicode fixing, and JSON compression.</p>"},{"location":"api-reference/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML","title":"prompt_refiner.cleaner.StripHTML","text":"<pre><code>StripHTML(preserve_tags=None, to_markdown=False)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Remove HTML tags from text, with options to preserve semantic tags or convert to Markdown.</p> <p>Initialize the HTML stripper.</p> <p>Parameters:</p> Name Type Description Default <code>preserve_tags</code> <code>Optional[Set[str]]</code> <p>Set of tag names to preserve (e.g., {'p', 'li', 'table'})</p> <code>None</code> <code>to_markdown</code> <code>bool</code> <p>Convert common HTML tags to Markdown syntax</p> <code>False</code> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def __init__(\n    self,\n    preserve_tags: Optional[Set[str]] = None,\n    to_markdown: bool = False,\n):\n    \"\"\"\n    Initialize the HTML stripper.\n\n    Args:\n        preserve_tags: Set of tag names to preserve (e.g., {'p', 'li', 'table'})\n        to_markdown: Convert common HTML tags to Markdown syntax\n    \"\"\"\n    self.preserve_tags = preserve_tags or set()\n    self.to_markdown = to_markdown\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.StripHTML.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove HTML tags from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing HTML</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with HTML tags removed or converted to Markdown</p> Source code in <code>src/prompt_refiner/cleaner/html.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove HTML tags from the input text.\n\n    Args:\n        text: The input text containing HTML\n\n    Returns:\n        Text with HTML tags removed or converted to Markdown\n    \"\"\"\n    result = text\n\n    if self.to_markdown:\n        # Convert common HTML tags to Markdown\n        # Bold\n        result = re.sub(r\"&lt;strong&gt;(.*?)&lt;/strong&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;b&gt;(.*?)&lt;/b&gt;\", r\"**\\1**\", result, flags=re.DOTALL)\n        # Italic\n        result = re.sub(r\"&lt;em&gt;(.*?)&lt;/em&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        result = re.sub(r\"&lt;i&gt;(.*?)&lt;/i&gt;\", r\"*\\1*\", result, flags=re.DOTALL)\n        # Links\n        result = re.sub(\n            r'&lt;a[^&gt;]*href=[\"\\']([^\"\\']*)[\"\\'][^&gt;]*&gt;(.*?)&lt;/a&gt;',\n            r\"[\\2](\\1)\",\n            result,\n            flags=re.DOTALL,\n        )\n        # Headers\n        for i in range(1, 7):\n            result = re.sub(\n                f\"&lt;h{i}[^&gt;]*&gt;(.*?)&lt;/h{i}&gt;\",\n                f\"{'#' * i} \\\\1\\n\",\n                result,\n                flags=re.DOTALL,\n            )\n        # Code\n        result = re.sub(r\"&lt;code&gt;(.*?)&lt;/code&gt;\", r\"`\\1`\", result, flags=re.DOTALL)\n        # Lists - simple conversion\n        result = re.sub(r\"&lt;li[^&gt;]*&gt;(.*?)&lt;/li&gt;\", r\"- \\1\\n\", result, flags=re.DOTALL)\n        # Paragraphs\n        result = re.sub(r\"&lt;p[^&gt;]*&gt;(.*?)&lt;/p&gt;\", r\"\\1\\n\\n\", result, flags=re.DOTALL)\n        # Line breaks\n        result = re.sub(r\"&lt;br\\s*/?&gt;\", \"\\n\", result)\n\n    if self.preserve_tags:\n        # Remove all tags except preserved ones\n        # This is a simplified implementation\n        tags_pattern = r\"&lt;/?(?!\" + \"|\".join(self.preserve_tags) + r\"\\b)[^&gt;]+&gt;\"\n        result = re.sub(tags_pattern, \"\", result)\n    else:\n        # Remove all HTML tags\n        result = re.sub(r\"&lt;[^&gt;]+&gt;\", \"\", result)\n\n    # Clean up excessive newlines\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n\n    return result.strip()\n</code></pre>"},{"location":"api-reference/cleaner/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import StripHTML\n\n# Basic HTML stripping\nstripper = StripHTML()\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\nstripper = StripHTML(to_markdown=True)\nresult = stripper.process(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n\n# Preserve specific tags\nstripper = StripHTML(preserve_tags={\"p\", \"div\"})\nresult = stripper.process(\"&lt;div&gt;Keep &lt;b&gt;Remove&lt;/b&gt;&lt;/div&gt;\")\n# Output: \"&lt;div&gt;Keep Remove&lt;/div&gt;\"\n</code></pre>"},{"location":"api-reference/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines into single spaces.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace","title":"prompt_refiner.cleaner.NormalizeWhitespace","text":"<p>               Bases: <code>Refiner</code></p> <p>Normalize whitespace in text.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.NormalizeWhitespace.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Normalize whitespace by collapsing multiple spaces into one.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with normalized whitespace</p> Source code in <code>src/prompt_refiner/cleaner/whitespace.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Normalize whitespace by collapsing multiple spaces into one.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with normalized whitespace\n    \"\"\"\n    # Replace multiple whitespace with single space and strip edges\n    return \" \".join(text.split())\n</code></pre>"},{"location":"api-reference/cleaner/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import NormalizeWhitespace\n\nnormalizer = NormalizeWhitespace()\nresult = normalizer.process(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre>"},{"location":"api-reference/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters including zero-width spaces and control characters.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode","title":"prompt_refiner.cleaner.FixUnicode","text":"<pre><code>FixUnicode(\n    remove_zero_width=True, remove_control_chars=True\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Remove or fix problematic Unicode characters.</p> <p>Initialize the Unicode fixer.</p> <p>Parameters:</p> Name Type Description Default <code>remove_zero_width</code> <code>bool</code> <p>Remove zero-width spaces and similar characters</p> <code>True</code> <code>remove_control_chars</code> <code>bool</code> <p>Remove control characters (except newlines and tabs)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def __init__(self, remove_zero_width: bool = True, remove_control_chars: bool = True):\n    \"\"\"\n    Initialize the Unicode fixer.\n\n    Args:\n        remove_zero_width: Remove zero-width spaces and similar characters\n        remove_control_chars: Remove control characters (except newlines and tabs)\n    \"\"\"\n    self.remove_zero_width = remove_zero_width\n    self.remove_control_chars = remove_control_chars\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.FixUnicode.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Clean problematic Unicode characters from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with problematic Unicode characters removed</p> Source code in <code>src/prompt_refiner/cleaner/unicode.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Clean problematic Unicode characters from text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with problematic Unicode characters removed\n    \"\"\"\n    result = text\n\n    if self.remove_zero_width:\n        # Remove zero-width characters\n        zero_width_chars = [\n            \"\\u200b\",  # Zero-width space\n            \"\\u200c\",  # Zero-width non-joiner\n            \"\\u200d\",  # Zero-width joiner\n            \"\\ufeff\",  # Zero-width no-break space (BOM)\n            \"\\u2060\",  # Word joiner\n        ]\n        for char in zero_width_chars:\n            result = result.replace(char, \"\")\n\n    if self.remove_control_chars:\n        # Remove control characters except newlines, tabs, and carriage returns\n        # Keep: \\n (0x0A), \\t (0x09), \\r (0x0D)\n        result = \"\".join(\n            char\n            for char in result\n            if not unicodedata.category(char).startswith(\"C\") or char in (\"\\n\", \"\\t\", \"\\r\")\n        )\n\n    # Normalize Unicode to NFC form (canonical composition)\n    result = unicodedata.normalize(\"NFC\", result)\n\n    return result\n</code></pre>"},{"location":"api-reference/cleaner/#examples_2","title":"Examples","text":"<pre><code>from prompt_refiner import FixUnicode\n\n# Remove zero-width spaces and control chars\nfixer = FixUnicode()\nresult = fixer.process(\"Hello\\u200bWorld\\u0000\")\n# Output: \"HelloWorld\"\n\n# Only remove zero-width spaces\nfixer = FixUnicode(remove_control_chars=False)\nresult = fixer.process(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre>"},{"location":"api-reference/cleaner/#jsoncleaner","title":"JsonCleaner","text":"<p>Clean and minify JSON by removing null values and empty containers.</p>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner","title":"prompt_refiner.cleaner.JsonCleaner","text":"<pre><code>JsonCleaner(strip_nulls=True, strip_empty=True)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Cleans and minifies JSON strings. Removes null values, empty containers, and extra whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>strip_nulls</code> <code>bool</code> <p>If True, remove null/None values from objects and arrays (default: True)</p> <code>True</code> <code>strip_empty</code> <code>bool</code> <p>If True, remove empty dicts, lists, and strings (default: True)</p> <code>True</code> Example <p>from prompt_refiner import JsonCleaner cleaner = JsonCleaner(strip_nulls=True, strip_empty=True)</p> <p>dirty_json = ''' ... { ...   \"name\": \"Alice\", ...   \"age\": null, ...   \"address\": {}, ...   \"tags\": [], ...   \"bio\": \"\" ... } ... ''' result = cleaner.run(dirty_json) print(result)</p> Use Cases <ul> <li>RAG Context Compression: Strip nulls/empties from API responses before feeding to LLM</li> <li>Cost Optimization: Reduce token count by removing unnecessary JSON structure</li> <li>Data Cleaning: Normalize JSON from multiple sources with inconsistent null handling</li> </ul> <p>Initialize JSON cleaner.</p> <p>Parameters:</p> Name Type Description Default <code>strip_nulls</code> <code>bool</code> <p>Remove null/None values</p> <code>True</code> <code>strip_empty</code> <code>bool</code> <p>Remove empty containers (dict, list, str)</p> <code>True</code> Source code in <code>src/prompt_refiner/cleaner/json.py</code> <pre><code>def __init__(self, strip_nulls: bool = True, strip_empty: bool = True):\n    \"\"\"\n    Initialize JSON cleaner.\n\n    Args:\n        strip_nulls: Remove null/None values\n        strip_empty: Remove empty containers (dict, list, str)\n    \"\"\"\n    self.strip_nulls = strip_nulls\n    self.strip_empty = strip_empty\n</code></pre>"},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner-functions","title":"Functions","text":""},{"location":"api-reference/cleaner/#prompt_refiner.cleaner.JsonCleaner.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Process the input JSON (string or object). Returns a minified JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, Dict, List]</code> <p>JSON string, dict, or list to clean</p> required <p>Returns:</p> Type Description <code>str</code> <p>Minified JSON string with nulls/empties removed</p> Note <p>If input is not valid JSON, returns input unchanged.</p> Source code in <code>src/prompt_refiner/cleaner/json.py</code> <pre><code>def process(self, text: Union[str, Dict, List]) -&gt; str:\n    \"\"\"\n    Process the input JSON (string or object).\n    Returns a minified JSON string.\n\n    Args:\n        text: JSON string, dict, or list to clean\n\n    Returns:\n        Minified JSON string with nulls/empties removed\n\n    Note:\n        If input is not valid JSON, returns input unchanged.\n    \"\"\"\n    # 1. Parse Input (Handle both string JSON and raw Dict/List)\n    data = text\n    if isinstance(text, str):\n        try:\n            data = json.loads(text)\n        except json.JSONDecodeError:\n            # If it's not valid JSON, return as-is to allow pipeline to continue safely\n            return text\n\n    # 2. Clean Structure\n    cleaned_data = self._clean_data(data)\n\n    # 3. Dump Minified String (No whitespace)\n    return json.dumps(cleaned_data, ensure_ascii=False, separators=(\",\", \":\"))\n</code></pre>"},{"location":"api-reference/cleaner/#examples_3","title":"Examples","text":"<pre><code>from prompt_refiner import JsonCleaner\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndirty_json = \"\"\"\n{\n    \"name\": \"Alice\",\n    \"age\": null,\n    \"address\": {},\n    \"tags\": [],\n    \"bio\": \"\"\n}\n\"\"\"\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\"}\n\n# Only strip nulls, keep empties\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=False)\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\",\"address\":{},\"tags\":[],\"bio\":\"\"}\n\n# Only minify (no cleaning)\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nresult = cleaner.process(dirty_json)\n# Output: {\"name\":\"Alice\",\"age\":null,\"address\":{},\"tags\":[],\"bio\":\"\"}\n\n# Works with dict/list inputs too\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndata = {\"name\": \"Bob\", \"tags\": [], \"age\": None}\nresult = cleaner.process(data)\n# Output: {\"name\":\"Bob\"}\n</code></pre>"},{"location":"api-reference/cleaner/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/cleaner/#web-scraping","title":"Web Scraping","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, FixUnicode\n\nweb_cleaner = (\n    Refiner()\n    .pipe(StripHTML(to_markdown=True))\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import Refiner, NormalizeWhitespace, FixUnicode\n\ntext_normalizer = (\n    Refiner()\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/cleaner/#rag-json-compression","title":"RAG JSON Compression","text":"<pre><code>from prompt_refiner import Refiner, JsonCleaner, TruncateTokens\n\nrag_compressor = (\n    Refiner()\n    .pipe(JsonCleaner(strip_nulls=True, strip_empty=True))\n    .pipe(TruncateTokens(max_tokens=500, strategy=\"head\"))\n)\n</code></pre>"},{"location":"api-reference/compressor/","title":"Compressor Module","text":"<p>The Compressor module provides operations for reducing text size through smart truncation and deduplication.</p>"},{"location":"api-reference/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens","title":"prompt_refiner.compressor.TruncateTokens","text":"<pre><code>TruncateTokens(\n    max_tokens,\n    strategy=\"head\",\n    respect_sentence_boundary=True,\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Truncate text to a maximum number of tokens with intelligent sentence boundary detection.</p> <p>Initialize the truncation operation.</p> <p>Parameters:</p> Name Type Description Default <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to keep</p> required <code>strategy</code> <code>Literal['head', 'tail', 'middle_out']</code> <p>Truncation strategy: - \"head\": Keep the beginning of the text - \"tail\": Keep the end of the text (useful for conversation history) - \"middle_out\": Keep beginning and end, remove middle</p> <code>'head'</code> <code>respect_sentence_boundary</code> <code>bool</code> <p>If True, truncate at sentence boundaries</p> <code>True</code> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def __init__(\n    self,\n    max_tokens: int,\n    strategy: Literal[\"head\", \"tail\", \"middle_out\"] = \"head\",\n    respect_sentence_boundary: bool = True,\n):\n    \"\"\"\n    Initialize the truncation operation.\n\n    Args:\n        max_tokens: Maximum number of tokens to keep\n        strategy: Truncation strategy:\n            - \"head\": Keep the beginning of the text\n            - \"tail\": Keep the end of the text (useful for conversation history)\n            - \"middle_out\": Keep beginning and end, remove middle\n        respect_sentence_boundary: If True, truncate at sentence boundaries\n    \"\"\"\n    self.max_tokens = max_tokens\n    self.strategy = strategy\n    self.respect_sentence_boundary = respect_sentence_boundary\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.TruncateTokens.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Truncate text to max_tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated text respecting sentence boundaries if configured</p> Source code in <code>src/prompt_refiner/compressor/truncate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Truncate text to max_tokens.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Truncated text respecting sentence boundaries if configured\n    \"\"\"\n    estimated_tokens = self._estimate_tokens(text)\n\n    if estimated_tokens &lt;= self.max_tokens:\n        return text\n\n    if self.respect_sentence_boundary:\n        sentences = self._split_sentences(text)\n\n        if self.strategy == \"head\":\n            return self._truncate_head_sentences(sentences)\n        elif self.strategy == \"tail\":\n            return self._truncate_tail_sentences(sentences)\n        elif self.strategy == \"middle_out\":\n            return self._truncate_middle_out_sentences(sentences)\n    else:\n        # Fallback to word-based truncation\n        words = text.split()\n\n        if self.strategy == \"head\":\n            return \" \".join(words[: self.max_tokens])\n        elif self.strategy == \"tail\":\n            return \" \".join(words[-self.max_tokens :])\n        elif self.strategy == \"middle_out\":\n            half = self.max_tokens // 2\n            start_words = words[:half]\n            end_words = words[-(self.max_tokens - half) :]\n            return \" \".join(start_words) + \" ... \" + \" \".join(end_words)\n\n    return text\n</code></pre>"},{"location":"api-reference/compressor/#truncation-strategies","title":"Truncation Strategies","text":"<ul> <li><code>head</code>: Keep the beginning of the text (default)</li> <li><code>tail</code>: Keep the end of the text (useful for conversation history)</li> <li><code>middle_out</code>: Keep beginning and end, remove middle</li> </ul>"},{"location":"api-reference/compressor/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\nresult = truncator.process(long_text)\n\n# Keep last 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\nresult = truncator.process(long_text)\n\n# Keep first and last 50 tokens, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\nresult = truncator.process(long_text)\n\n# Truncate at word boundaries (faster, less precise)\ntruncator = TruncateTokens(\n    max_tokens=100,\n    strategy=\"head\",\n    respect_sentence_boundary=False\n)\nresult = truncator.process(long_text)\n</code></pre>"},{"location":"api-reference/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or highly similar text chunks, useful for RAG contexts.</p>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate","title":"prompt_refiner.compressor.Deduplicate","text":"<pre><code>Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\",\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Remove duplicate or highly similar text chunks (useful for RAG contexts).</p> Performance Characteristics <p>This operation uses an O(n\u00b2) comparison algorithm, where each chunk is compared against all previously seen chunks. The total complexity is O(n\u00b2 \u00d7 comparison_cost), where comparison_cost depends on the selected similarity method: - Jaccard: O(m) where m is the chunk length (word-based) - Levenshtein: O(m\u2081 \u00d7 m\u2082) where m\u2081, m\u2082 are the chunk lengths (character-based)</p> <p>For typical RAG contexts (10-50 chunks), performance is acceptable with either method. For larger inputs (200+ chunks), consider using paragraph granularity to reduce the number of comparisons, or use Jaccard method for better performance.</p> <p>Initialize the deduplication operation.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>Threshold for considering text similar (0.0-1.0)</p> <code>0.85</code> <code>method</code> <code>Literal['levenshtein', 'jaccard']</code> <p>Similarity calculation method - \"jaccard\": Jaccard similarity (word-based, faster)     * Complexity: O(m) per comparison where m is chunk length     * Recommended for most use cases (10-200 chunks)     * Fast even with long chunks - \"levenshtein\": Levenshtein distance (character-based)     * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison     * More precise but computationally expensive     * Can be slow with long chunks (1000+ characters)</p> <code>'jaccard'</code> <code>granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Text granularity to deduplicate at - \"sentence\": Deduplicate at sentence level     * More comparisons (more chunks) but smaller chunk sizes     * Better for fine-grained deduplication - \"paragraph\": Deduplicate at paragraph level     * Fewer comparisons but larger chunk sizes     * Recommended for large documents to reduce n\u00b2 scaling</p> <code>'paragraph'</code> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def __init__(\n    self,\n    similarity_threshold: float = 0.85,\n    method: Literal[\"levenshtein\", \"jaccard\"] = \"jaccard\",\n    granularity: Literal[\"sentence\", \"paragraph\"] = \"paragraph\",\n):\n    \"\"\"\n    Initialize the deduplication operation.\n\n    Args:\n        similarity_threshold: Threshold for considering text similar (0.0-1.0)\n        method: Similarity calculation method\n            - \"jaccard\": Jaccard similarity (word-based, faster)\n                * Complexity: O(m) per comparison where m is chunk length\n                * Recommended for most use cases (10-200 chunks)\n                * Fast even with long chunks\n            - \"levenshtein\": Levenshtein distance (character-based)\n                * Complexity: O(m\u2081 \u00d7 m\u2082) per comparison\n                * More precise but computationally expensive\n                * Can be slow with long chunks (1000+ characters)\n        granularity: Text granularity to deduplicate at\n            - \"sentence\": Deduplicate at sentence level\n                * More comparisons (more chunks) but smaller chunk sizes\n                * Better for fine-grained deduplication\n            - \"paragraph\": Deduplicate at paragraph level\n                * Fewer comparisons but larger chunk sizes\n                * Recommended for large documents to reduce n\u00b2 scaling\n    \"\"\"\n    self.similarity_threshold = similarity_threshold\n    self.method = method\n    self.granularity = granularity\n</code></pre>"},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate-functions","title":"Functions","text":""},{"location":"api-reference/compressor/#prompt_refiner.compressor.Deduplicate.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Remove duplicate or similar text chunks.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with duplicates removed</p> Performance Note <p>This method uses O(n\u00b2) comparisons where n is the number of chunks. For large inputs (200+ chunks), consider using paragraph granularity to reduce the number of chunks, or ensure you're using the jaccard method for better performance.</p> Source code in <code>src/prompt_refiner/compressor/deduplicate.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Remove duplicate or similar text chunks.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with duplicates removed\n\n    Performance Note:\n        This method uses O(n\u00b2) comparisons where n is the number of chunks.\n        For large inputs (200+ chunks), consider using paragraph granularity\n        to reduce the number of chunks, or ensure you're using the jaccard\n        method for better performance.\n    \"\"\"\n    chunks = self._split_text(text)\n\n    if not chunks:\n        return text\n\n    # Keep track of unique chunks\n    unique_chunks = []\n    seen_chunks = []\n\n    for chunk in chunks:\n        is_duplicate = False\n\n        # Check similarity with all previously seen chunks\n        for seen_chunk in seen_chunks:\n            similarity = self._calculate_similarity(chunk, seen_chunk)\n            if similarity &gt;= self.similarity_threshold:\n                is_duplicate = True\n                break\n\n        if not is_duplicate:\n            unique_chunks.append(chunk)\n            seen_chunks.append(chunk)\n\n    # Reconstruct text\n    if self.granularity == \"paragraph\":\n        return \"\\n\\n\".join(unique_chunks)\n    else:  # sentence\n        return \" \".join(unique_chunks)\n</code></pre>"},{"location":"api-reference/compressor/#similarity-methods","title":"Similarity Methods","text":"<ul> <li><code>jaccard</code>: Jaccard similarity (word-based, faster) - default</li> <li><code>levenshtein</code>: Levenshtein distance (character-based, more accurate)</li> </ul>"},{"location":"api-reference/compressor/#granularity-levels","title":"Granularity Levels","text":"<ul> <li><code>paragraph</code>: Deduplicate at paragraph level (split by <code>\\n\\n</code>) - default</li> <li><code>sentence</code>: Deduplicate at sentence level (split by <code>.</code>, <code>!</code>, <code>?</code>)</li> </ul>"},{"location":"api-reference/compressor/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# Basic deduplication (85% similarity threshold)\ndeduper = Deduplicate(similarity_threshold=0.85)\nresult = deduper.process(text_with_duplicates)\n\n# More aggressive (70% similarity)\ndeduper = Deduplicate(similarity_threshold=0.70)\nresult = deduper.process(text_with_duplicates)\n\n# Character-level similarity\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"levenshtein\"\n)\nresult = deduper.process(text_with_duplicates)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(\n    similarity_threshold=0.85,\n    granularity=\"sentence\"\n)\nresult = deduper.process(text_with_duplicates)\n</code></pre>"},{"location":"api-reference/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.85))  # Remove duplicates first\n    .pipe(TruncateTokens(max_tokens=2000))        # Then fit in context window\n)\n</code></pre>"},{"location":"api-reference/compressor/#conversation-history-compression","title":"Conversation History Compression","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nconversation_compressor = (\n    Refiner()\n    .pipe(Deduplicate(granularity=\"sentence\"))\n    .pipe(TruncateTokens(max_tokens=1000, strategy=\"tail\"))  # Keep recent messages\n)\n</code></pre>"},{"location":"api-reference/compressor/#document-summarization-prep","title":"Document Summarization Prep","text":"<pre><code>from prompt_refiner import Refiner, Deduplicate, TruncateTokens\n\nsummarization_prep = (\n    Refiner()\n    .pipe(Deduplicate(similarity_threshold=0.90))  # Remove near-duplicates\n    .pipe(TruncateTokens(max_tokens=4000, strategy=\"middle_out\"))  # Keep intro + conclusion\n)\n</code></pre>"},{"location":"api-reference/packer/","title":"Packer Module API Reference","text":"<p>The Packer module provides specialized packers for composing prompts with automatic refinement and priority-based ordering. Version 0.1.3+ introduces two specialized packers following the Single Responsibility Principle. Version 0.2.1+ adds default refining strategies and removes token budget constraints. Version 0.2.2 removes unused <code>model</code> parameter for API simplification.</p>"},{"location":"api-reference/packer/#messagespacker","title":"MessagesPacker","text":"<p>Optimized for chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict[str, str]]</code> directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker","title":"prompt_refiner.packer.MessagesPacker","text":"<pre><code>MessagesPacker(\n    track_tokens=False,\n    token_counter=None,\n    system=None,\n    context=None,\n    history=None,\n    query=None,\n)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for chat completion APIs.</p> <p>Designed for: - OpenAI Chat Completions (gpt-4, gpt-3.5-turbo, etc.) - Anthropic Messages API (claude-3-opus, claude-3-sonnet, etc.) - Any API using ChatML-style message format</p> <p>Returns: List[Dict[str, str]] with 'role' and 'content' keys</p> Example <p>from prompt_refiner import MessagesPacker</p> <p>Initialize messages packer.</p> <p>Default Refining Strategies: When no explicit refiner is provided, automatic refining strategies are applied: - system/query: MinimalStrategy (StripHTML + NormalizeWhitespace) - context/history: StandardStrategy (StripHTML + NormalizeWhitespace + Deduplicate)</p> <p>To override defaults, provide explicit refiner tuple: (content, refiner). For raw content with no refinement, use .add() method with refine_with=None.</p> <p>Parameters:</p> Name Type Description Default <code>track_tokens</code> <code>bool</code> <p>Enable token tracking to measure refinement effectiveness</p> <code>False</code> <code>token_counter</code> <code>Optional[Callable[[str], int]]</code> <p>Function to count tokens (required if track_tokens=True)</p> <code>None</code> <code>system</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>System message. Can be: - str: \"You are helpful\"  (automatically refined with MinimalStrategy) - Tuple[str, Refiner]: (\"You are helpful\", StripHTML()) - Tuple[str, Pipeline]: (\"You are helpful\", StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>context</code> <code>Optional[Union[List[str], Tuple[List[str], Refiner]]]</code> <p>Context documents. Can be: - List[str]: [\"doc1\", \"doc2\"] - Tuple[List[str], Refiner]: ([\"doc1\", \"doc2\"], StripHTML()) - Tuple[List[str], Pipeline]: ([\"doc1\", \"doc2\"],     StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>history</code> <code>Optional[Union[List[Dict[str, str]], Tuple[List[Dict[str, str]], Refiner]]]</code> <p>Conversation history. Can be: - List[Dict]: [{\"role\": \"user\", \"content\": \"Hi\"}] - Tuple[List[Dict], Refiner]: ([{\"role\": \"user\", \"content\": \"Hi\"}], StripHTML()) - Tuple[List[Dict], Pipeline]: ([{\"role\": \"user\", \"content\": \"Hi\"}],     StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>query</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>Current query. Can be: - str: \"What's the weather?\" - Tuple[str, Refiner]: (\"What's the weather?\", StripHTML()) - Tuple[str, Pipeline]: (\"What's the weather?\", StripHTML() | NormalizeWhitespace())</p> <code>None</code> <p>Example (Simple - no refiners):     &gt;&gt;&gt; packer = MessagesPacker(     ...     system=\"You are helpful.\",     ...     context=[\"Doc 1\", \"<p>Doc 2</p>\"],     ...     history=[{\"role\": \"user\", \"content\": \"Hi\"}],     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; messages = packer.pack()</p> <p>Example (With single Refiner):     &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML     &gt;&gt;&gt; packer = MessagesPacker(     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\", \"<p>Doc 2</p>\"], StripHTML()),     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; messages = packer.pack()</p> <p>Example (With Pipeline - multiple refiners):     &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML, NormalizeWhitespace, Pipeline     &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()     &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])     &gt;&gt;&gt; packer = MessagesPacker(     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\", \"<p>Doc 2</p>\"], cleaner),     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; messages = packer.pack()</p> <p>Example (Traditional API - still supported):     &gt;&gt;&gt; packer = MessagesPacker()     &gt;&gt;&gt; packer.add(\"You are helpful.\", role=\"system\")     &gt;&gt;&gt; packer.add(\"Doc 1\", role=\"context\")     &gt;&gt;&gt; messages = packer.pack()</p> Source code in <code>src/prompt_refiner/packer/messages.py</code> <pre><code>def __init__(\n    self,\n    track_tokens: bool = False,\n    token_counter: Optional[Callable[[str], int]] = None,\n    system: Optional[Union[str, Tuple[str, Refiner]]] = None,\n    context: Optional[Union[List[str], Tuple[List[str], Refiner]]] = None,\n    history: Optional[\n        Union[\n            List[Dict[str, str]],\n            Tuple[List[Dict[str, str]], Refiner],\n        ]\n    ] = None,\n    query: Optional[Union[str, Tuple[str, Refiner]]] = None,\n):\n    \"\"\"\n    Initialize messages packer.\n\n    **Default Refining Strategies**:\n    When no explicit refiner is provided, automatic refining strategies are applied:\n    - system/query: MinimalStrategy (StripHTML + NormalizeWhitespace)\n    - context/history: StandardStrategy (StripHTML + NormalizeWhitespace + Deduplicate)\n\n    To override defaults, provide explicit refiner tuple: (content, refiner).\n    For raw content with no refinement, use .add() method with refine_with=None.\n\n    Args:\n        track_tokens: Enable token tracking to measure refinement effectiveness\n        token_counter: Function to count tokens (required if track_tokens=True)\n        system: System message. Can be:\n            - str: \"You are helpful\"  (automatically refined with MinimalStrategy)\n            - Tuple[str, Refiner]: (\"You are helpful\", StripHTML())\n            - Tuple[str, Pipeline]: (\"You are helpful\", StripHTML() | NormalizeWhitespace())\n        context: Context documents. Can be:\n            - List[str]: [\"doc1\", \"doc2\"]\n            - Tuple[List[str], Refiner]: ([\"doc1\", \"doc2\"], StripHTML())\n            - Tuple[List[str], Pipeline]: ([\"doc1\", \"doc2\"],\n                StripHTML() | NormalizeWhitespace())\n        history: Conversation history. Can be:\n            - List[Dict]: [{\"role\": \"user\", \"content\": \"Hi\"}]\n            - Tuple[List[Dict], Refiner]: ([{\"role\": \"user\", \"content\": \"Hi\"}], StripHTML())\n            - Tuple[List[Dict], Pipeline]: ([{\"role\": \"user\", \"content\": \"Hi\"}],\n                StripHTML() | NormalizeWhitespace())\n        query: Current query. Can be:\n            - str: \"What's the weather?\"\n            - Tuple[str, Refiner]: (\"What's the weather?\", StripHTML())\n            - Tuple[str, Pipeline]: (\"What's the weather?\", StripHTML() | NormalizeWhitespace())\n\n    Example (Simple - no refiners):\n        &gt;&gt;&gt; packer = MessagesPacker(\n        ...     system=\"You are helpful.\",\n        ...     context=[\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"],\n        ...     history=[{\"role\": \"user\", \"content\": \"Hi\"}],\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; messages = packer.pack()\n\n    Example (With single Refiner):\n        &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML\n        &gt;&gt;&gt; packer = MessagesPacker(\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"], StripHTML()),\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; messages = packer.pack()\n\n    Example (With Pipeline - multiple refiners):\n        &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML, NormalizeWhitespace, Pipeline\n        &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()\n        &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])\n        &gt;&gt;&gt; packer = MessagesPacker(\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"], cleaner),\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; messages = packer.pack()\n\n    Example (Traditional API - still supported):\n        &gt;&gt;&gt; packer = MessagesPacker()\n        &gt;&gt;&gt; packer.add(\"You are helpful.\", role=\"system\")\n        &gt;&gt;&gt; packer.add(\"Doc 1\", role=\"context\")\n        &gt;&gt;&gt; messages = packer.pack()\n    \"\"\"\n    super().__init__(track_tokens, token_counter)\n    logger.debug(\"MessagesPacker initialized\")\n\n    # Auto-add items if provided (convenient API)\n    # Extract content and refiner from tuple if provided\n    # Apply default strategies when no explicit refiner provided\n    if system is not None:\n        system_content, system_refiner = self._extract_field(system)\n        # Apply MinimalStrategy to system if no explicit refiner\n        if system_refiner is None:\n            system_refiner = MinimalStrategy()\n        self.add(system_content, role=\"system\", refine_with=system_refiner)\n\n    if context is not None:\n        context_docs, context_refiner = self._extract_field(context)\n        # Apply StandardStrategy to context if no explicit refiner\n        if context_refiner is None:\n            context_refiner = StandardStrategy()\n        for doc in context_docs:\n            self.add(doc, role=\"context\", refine_with=context_refiner)\n\n    if history is not None:\n        history_msgs, history_refiner = self._extract_field(history)\n        # Apply StandardStrategy to history if no explicit refiner\n        if history_refiner is None:\n            history_refiner = StandardStrategy()\n        for msg in history_msgs:\n            self.add(msg[\"content\"], role=msg[\"role\"], refine_with=history_refiner)\n\n    if query is not None:\n        query_content, query_refiner = self._extract_field(query)\n        # Apply MinimalStrategy to query if no explicit refiner\n        if query_refiner is None:\n            query_refiner = MinimalStrategy()\n        self.add(query_content, role=\"query\", refine_with=query_refiner)\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--basic-usage-with-automatic-refining","title":"Basic usage with automatic refining","text":"<p>packer = MessagesPacker( ...     system=\"You are helpful.\", ...     context=[\"Doc 1\", \"Doc 2\"], ...     query=\"What's the weather?\" ... ) messages = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--use-directly-openaichatcompletionscreatemessagesmessages","title":"Use directly: openai.chat.completions.create(messages=messages)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker--traditional-api-still-supported","title":"Traditional API still supported","text":"<p>packer = MessagesPacker() packer.add(\"System prompt\", role=\"system\") packer.add(\"User query\", role=\"user\") messages = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker.quick_pack","title":"quick_pack  <code>classmethod</code>","text":"<pre><code>quick_pack(\n    system=None, context=None, history=None, query=None\n)\n</code></pre> <p>One-liner to create packer and pack messages immediately.</p> <p>Default refining strategies are automatically applied (same as init): - system/query: MinimalStrategy - context/history: StandardStrategy</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>System message (str or (str, Refiner/Pipeline) tuple)</p> <code>None</code> <code>context</code> <code>Optional[Union[List[str], Tuple[List[str], Refiner]]]</code> <p>Context documents (list or (list, Refiner/Pipeline) tuple)</p> <code>None</code> <code>history</code> <code>Optional[Union[List[Dict[str, str]], Tuple[List[Dict[str, str]], Refiner]]]</code> <p>Conversation history (list or (list, Refiner/Pipeline) tuple)</p> <code>None</code> <code>query</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>Current query (str or (str, Refiner/Pipeline) tuple)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>Packed messages ready for LLM API</p> <p>Example (Simple):     &gt;&gt;&gt; messages = MessagesPacker.quick_pack(     ...     system=\"You are helpful.\",     ...     context=[\"Doc 1\", \"<p>Doc 2</p>\"],     ...     query=\"What's the weather?\"     ... )</p> <p>Example (With single Refiner):     &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML     &gt;&gt;&gt; messages = MessagesPacker.quick_pack(     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\", \"<p>Doc 2</p>\"], StripHTML()),     ...     query=\"What's the weather?\"     ... )</p> <p>Example (With Pipeline - multiple refiners):     &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML, NormalizeWhitespace, Pipeline     &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()     &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])     &gt;&gt;&gt; messages = MessagesPacker.quick_pack(     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\", \"<p>Doc 2</p>\"], cleaner),     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; # Ready to use: client.chat.completions.create(messages=messages)</p> Source code in <code>src/prompt_refiner/packer/messages.py</code> <pre><code>@classmethod\ndef quick_pack(\n    cls,\n    system: Optional[Union[str, Tuple[str, Refiner]]] = None,\n    context: Optional[Union[List[str], Tuple[List[str], Refiner]]] = None,\n    history: Optional[\n        Union[\n            List[Dict[str, str]],\n            Tuple[List[Dict[str, str]], Refiner],\n        ]\n    ] = None,\n    query: Optional[Union[str, Tuple[str, Refiner]]] = None,\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    One-liner to create packer and pack messages immediately.\n\n    Default refining strategies are automatically applied (same as __init__):\n    - system/query: MinimalStrategy\n    - context/history: StandardStrategy\n\n    Args:\n        system: System message (str or (str, Refiner/Pipeline) tuple)\n        context: Context documents (list or (list, Refiner/Pipeline) tuple)\n        history: Conversation history (list or (list, Refiner/Pipeline) tuple)\n        query: Current query (str or (str, Refiner/Pipeline) tuple)\n\n    Returns:\n        Packed messages ready for LLM API\n\n    Example (Simple):\n        &gt;&gt;&gt; messages = MessagesPacker.quick_pack(\n        ...     system=\"You are helpful.\",\n        ...     context=[\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"],\n        ...     query=\"What's the weather?\"\n        ... )\n\n    Example (With single Refiner):\n        &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML\n        &gt;&gt;&gt; messages = MessagesPacker.quick_pack(\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"], StripHTML()),\n        ...     query=\"What's the weather?\"\n        ... )\n\n    Example (With Pipeline - multiple refiners):\n        &gt;&gt;&gt; from prompt_refiner import MessagesPacker, StripHTML, NormalizeWhitespace, Pipeline\n        &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()\n        &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])\n        &gt;&gt;&gt; messages = MessagesPacker.quick_pack(\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\", \"&lt;p&gt;Doc 2&lt;/p&gt;\"], cleaner),\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; # Ready to use: client.chat.completions.create(messages=messages)\n    \"\"\"\n    packer = cls(\n        system=system,\n        context=context,\n        history=history,\n        query=query,\n    )\n    return packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.MessagesPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into message format for chat APIs.</p> <p>Automatically maps semantic roles to API-compatible roles: - ROLE_CONTEXT \u2192 \"user\" (RAG documents as user-provided context) - ROLE_QUERY \u2192 \"user\" (current user question) - Other roles (system, user, assistant) remain unchanged</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List of message dictionaries with 'role' and 'content' keys,</p> <code>List[Dict[str, str]]</code> <p>ready for OpenAI, Anthropic, and other chat completion APIs.</p> Example <p>messages = packer.pack() openai.chat.completions.create(model=\"gpt-4\", messages=messages)</p> Source code in <code>src/prompt_refiner/packer/messages.py</code> <pre><code>def pack(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Pack items into message format for chat APIs.\n\n    Automatically maps semantic roles to API-compatible roles:\n    - ROLE_CONTEXT \u2192 \"user\" (RAG documents as user-provided context)\n    - ROLE_QUERY \u2192 \"user\" (current user question)\n    - Other roles (system, user, assistant) remain unchanged\n\n    Returns:\n        List of message dictionaries with 'role' and 'content' keys,\n        ready for OpenAI, Anthropic, and other chat completion APIs.\n\n    Example:\n        &gt;&gt;&gt; messages = packer.pack()\n        &gt;&gt;&gt; openai.chat.completions.create(model=\"gpt-4\", messages=messages)\n    \"\"\"\n    selected_items = self._select_items()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty message list\")\n        return []\n\n    messages = []\n    for item in selected_items:\n        # Map semantic roles to API-compatible roles\n        api_role = item.role\n\n        if item.role == ROLE_CONTEXT:\n            # RAG documents become user messages (context provided by user)\n            api_role = \"user\"\n        elif item.role == ROLE_QUERY:\n            # Current query becomes user message\n            api_role = \"user\"\n        # Other roles (system, user, assistant) remain unchanged\n\n        messages.append({\"role\": api_role, \"content\": item.content})\n\n    logger.info(f\"Packed {len(messages)} messages for chat API\")\n    return messages\n</code></pre>"},{"location":"api-reference/packer/#textpacker","title":"TextPacker","text":"<p>Optimized for text completion APIs (Llama Base, GPT-3). Returns <code>str</code> directly with multiple text formats.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker","title":"prompt_refiner.packer.TextPacker","text":"<pre><code>TextPacker(\n    track_tokens=False,\n    token_counter=None,\n    text_format=TextFormat.RAW,\n    separator=None,\n    system=None,\n    context=None,\n    history=None,\n    query=None,\n)\n</code></pre> <p>               Bases: <code>BasePacker</code></p> <p>Packer for text completion APIs.</p> <p>Designed for: - Base models (Llama-2-base, GPT-3, etc.) - Completion endpoints (not chat) - Custom prompt templates</p> <p>Returns: str (formatted text ready for completion API)</p> <p>Supports multiple text formatting strategies to prevent instruction drifting: - RAW: Simple concatenation with separators - MARKDOWN: Grouped sections (INSTRUCTIONS, CONTEXT, CONVERSATION, INPUT) - XML: Semantic content tags</p> Example <p>from prompt_refiner import TextPacker, TextFormat</p> <p>Initialize text packer.</p> <p>Default Refining Strategies: When no explicit refiner is provided, automatic refining strategies are applied: - system/query: MinimalStrategy (StripHTML + NormalizeWhitespace) - context/history: StandardStrategy (StripHTML + NormalizeWhitespace + Deduplicate)</p> <p>To override defaults, provide explicit refiner tuple: (content, refiner). For raw content with no refinement, use .add() method with refine_with=None.</p> <p>Parameters:</p> Name Type Description Default <code>track_tokens</code> <code>bool</code> <p>Enable token tracking to measure refinement effectiveness</p> <code>False</code> <code>token_counter</code> <code>Optional[Callable[[str], int]]</code> <p>Function to count tokens (required if track_tokens=True)</p> <code>None</code> <code>text_format</code> <code>TextFormat</code> <p>Text formatting strategy (RAW, MARKDOWN, XML)</p> <code>RAW</code> <code>separator</code> <code>Optional[str]</code> <p>String to join items (default: \"\\n\\n\" for clarity)</p> <code>None</code> <code>system</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>System message. Can be: - str: \"You are helpful\" - Tuple[str, Refiner]: (\"You are helpful\", StripHTML()) - Tuple[str, Pipeline]: (\"You are helpful\", StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>context</code> <code>Optional[Union[List[str], Tuple[List[str], Refiner]]]</code> <p>Context documents. Can be: - List[str]: [\"doc1\", \"doc2\"] - Tuple[List[str], Refiner]: ([\"doc1\", \"doc2\"], StripHTML()) - Tuple[List[str], Pipeline]: ([\"doc1\", \"doc2\"],     StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>history</code> <code>Optional[Union[List[Dict[str, str]], Tuple[List[Dict[str, str]], Refiner]]]</code> <p>Conversation history. Can be: - List[Dict]: [{\"role\": \"user\", \"content\": \"Hi\"}] - Tuple[List[Dict], Refiner]: ([{\"role\": \"user\", \"content\": \"Hi\"}], StripHTML()) - Tuple[List[Dict], Pipeline]: ([{\"role\": \"user\", \"content\": \"Hi\"}],     StripHTML() | NormalizeWhitespace())</p> <code>None</code> <code>query</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>Current query. Can be: - str: \"What's the weather?\" - Tuple[str, Refiner]: (\"What's the weather?\", StripHTML()) - Tuple[str, Pipeline]: (\"What's the weather?\", StripHTML() | NormalizeWhitespace())</p> <code>None</code> <p>Example (Simple - no refiners):     &gt;&gt;&gt; packer = TextPacker(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=[\"Doc 1\", \"Doc 2\"],     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; prompt = packer.pack()</p> <p>Example (With single Refiner):     &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML     &gt;&gt;&gt; packer = TextPacker(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\"], StripHTML()),     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; prompt = packer.pack()</p> <p>Example (With Pipeline - multiple refiners):     &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML, NormalizeWhitespace, Pipeline     &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()     &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])     &gt;&gt;&gt; packer = TextPacker(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\"], cleaner),     ...     query=\"What's the weather?\"     ... )     &gt;&gt;&gt; prompt = packer.pack()</p> Source code in <code>src/prompt_refiner/packer/text.py</code> <pre><code>def __init__(\n    self,\n    track_tokens: bool = False,\n    token_counter: Optional[Callable[[str], int]] = None,\n    text_format: TextFormat = TextFormat.RAW,\n    separator: Optional[str] = None,\n    system: Optional[Union[str, Tuple[str, Refiner]]] = None,\n    context: Optional[Union[List[str], Tuple[List[str], Refiner]]] = None,\n    history: Optional[\n        Union[\n            List[Dict[str, str]],\n            Tuple[List[Dict[str, str]], Refiner],\n        ]\n    ] = None,\n    query: Optional[Union[str, Tuple[str, Refiner]]] = None,\n):\n    \"\"\"\n    Initialize text packer.\n\n    **Default Refining Strategies**:\n    When no explicit refiner is provided, automatic refining strategies are applied:\n    - system/query: MinimalStrategy (StripHTML + NormalizeWhitespace)\n    - context/history: StandardStrategy (StripHTML + NormalizeWhitespace + Deduplicate)\n\n    To override defaults, provide explicit refiner tuple: (content, refiner).\n    For raw content with no refinement, use .add() method with refine_with=None.\n\n    Args:\n        track_tokens: Enable token tracking to measure refinement effectiveness\n        token_counter: Function to count tokens (required if track_tokens=True)\n        text_format: Text formatting strategy (RAW, MARKDOWN, XML)\n        separator: String to join items (default: \"\\\\n\\\\n\" for clarity)\n        system: System message. Can be:\n            - str: \"You are helpful\"\n            - Tuple[str, Refiner]: (\"You are helpful\", StripHTML())\n            - Tuple[str, Pipeline]: (\"You are helpful\", StripHTML() | NormalizeWhitespace())\n        context: Context documents. Can be:\n            - List[str]: [\"doc1\", \"doc2\"]\n            - Tuple[List[str], Refiner]: ([\"doc1\", \"doc2\"], StripHTML())\n            - Tuple[List[str], Pipeline]: ([\"doc1\", \"doc2\"],\n                StripHTML() | NormalizeWhitespace())\n        history: Conversation history. Can be:\n            - List[Dict]: [{\"role\": \"user\", \"content\": \"Hi\"}]\n            - Tuple[List[Dict], Refiner]: ([{\"role\": \"user\", \"content\": \"Hi\"}], StripHTML())\n            - Tuple[List[Dict], Pipeline]: ([{\"role\": \"user\", \"content\": \"Hi\"}],\n                StripHTML() | NormalizeWhitespace())\n        query: Current query. Can be:\n            - str: \"What's the weather?\"\n            - Tuple[str, Refiner]: (\"What's the weather?\", StripHTML())\n            - Tuple[str, Pipeline]: (\"What's the weather?\", StripHTML() | NormalizeWhitespace())\n\n    Example (Simple - no refiners):\n        &gt;&gt;&gt; packer = TextPacker(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=[\"Doc 1\", \"Doc 2\"],\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; prompt = packer.pack()\n\n    Example (With single Refiner):\n        &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML\n        &gt;&gt;&gt; packer = TextPacker(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\"], StripHTML()),\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; prompt = packer.pack()\n\n    Example (With Pipeline - multiple refiners):\n        &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML, NormalizeWhitespace, Pipeline\n        &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()\n        &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])\n        &gt;&gt;&gt; packer = TextPacker(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\"], cleaner),\n        ...     query=\"What's the weather?\"\n        ... )\n        &gt;&gt;&gt; prompt = packer.pack()\n    \"\"\"\n    super().__init__(track_tokens, token_counter)\n    self.text_format = text_format\n    self.separator = separator if separator is not None else \"\\n\\n\"\n\n    logger.debug(\n        f\"TextPacker initialized with format={text_format.value}, \"\n        f\"separator={repr(self.separator)}\"\n    )\n\n    # Auto-add items if provided (convenient API)\n    # Extract content and refiner from tuple if provided\n    # Apply default strategies when no explicit refiner provided\n    if system is not None:\n        system_content, system_refiner = self._extract_field(system)\n        # Apply MinimalStrategy to system if no explicit refiner\n        if system_refiner is None:\n            system_refiner = MinimalStrategy()\n        self.add(system_content, role=\"system\", refine_with=system_refiner)\n\n    if context is not None:\n        context_docs, context_refiner = self._extract_field(context)\n        # Apply StandardStrategy to context if no explicit refiner\n        if context_refiner is None:\n            context_refiner = StandardStrategy()\n        for doc in context_docs:\n            self.add(doc, role=\"context\", refine_with=context_refiner)\n\n    if history is not None:\n        history_msgs, history_refiner = self._extract_field(history)\n        # Apply StandardStrategy to history if no explicit refiner\n        if history_refiner is None:\n            history_refiner = StandardStrategy()\n        for msg in history_msgs:\n            self.add(msg[\"content\"], role=msg[\"role\"], refine_with=history_refiner)\n\n    if query is not None:\n        query_content, query_refiner = self._extract_field(query)\n        # Apply MinimalStrategy to query if no explicit refiner\n        if query_refiner is None:\n            query_refiner = MinimalStrategy()\n        self.add(query_content, role=\"query\", refine_with=query_refiner)\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--basic-usage-with-automatic-refining","title":"Basic usage with automatic refining","text":"<p>packer = TextPacker( ...     text_format=TextFormat.MARKDOWN, ...     system=\"You are helpful.\", ...     context=[\"Doc 1\", \"Doc 2\"], ...     query=\"What's the weather?\" ... ) prompt = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--use-directly-completioncreatepromptprompt","title":"Use directly: completion.create(prompt=prompt)","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker--traditional-api-still-supported","title":"Traditional API still supported","text":"<p>packer = TextPacker() packer.add(\"System prompt\", role=\"system\") prompt = packer.pack()</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker.quick_pack","title":"quick_pack  <code>classmethod</code>","text":"<pre><code>quick_pack(\n    system=None,\n    context=None,\n    history=None,\n    query=None,\n    text_format=TextFormat.RAW,\n    separator=None,\n)\n</code></pre> <p>One-liner to create packer and pack text immediately.</p> <p>Default refining strategies are automatically applied (same as init): - system/query: MinimalStrategy - context/history: StandardStrategy</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>System message (str or (str, Refiner/Pipeline) tuple)</p> <code>None</code> <code>context</code> <code>Optional[Union[List[str], Tuple[List[str], Refiner]]]</code> <p>Context documents (list or (list, Refiner/Pipeline) tuple)</p> <code>None</code> <code>history</code> <code>Optional[Union[List[Dict[str, str]], Tuple[List[Dict[str, str]], Refiner]]]</code> <p>Conversation history (list or (list, Refiner/Pipeline) tuple)</p> <code>None</code> <code>query</code> <code>Optional[Union[str, Tuple[str, Refiner]]]</code> <p>Current query (str or (str, Refiner/Pipeline) tuple)</p> <code>None</code> <code>text_format</code> <code>TextFormat</code> <p>Text formatting strategy (RAW, MARKDOWN, XML)</p> <code>RAW</code> <code>separator</code> <code>Optional[str]</code> <p>String to join items</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Packed text ready for completion API</p> <p>Example (Simple):     &gt;&gt;&gt; prompt = TextPacker.quick_pack(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=[\"Doc 1\", \"Doc 2\"],     ...     query=\"What's the weather?\"     ... )</p> <p>Example (With single Refiner):     &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML, TextFormat     &gt;&gt;&gt; prompt = TextPacker.quick_pack(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\"], StripHTML()),     ...     query=\"What's the weather?\"     ... )</p> <p>Example (With Pipeline - multiple refiners):     &gt;&gt;&gt; from prompt_refiner import (     ...     TextPacker, StripHTML, NormalizeWhitespace, TextFormat, Pipeline     ... )     &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()     &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])     &gt;&gt;&gt; prompt = TextPacker.quick_pack(     ...     text_format=TextFormat.MARKDOWN,     ...     system=\"You are helpful.\",     ...     context=([\"Doc 1\"], cleaner),     ...     query=\"What's the weather?\"     ... )</p> Source code in <code>src/prompt_refiner/packer/text.py</code> <pre><code>@classmethod\ndef quick_pack(\n    cls,\n    system: Optional[Union[str, Tuple[str, Refiner]]] = None,\n    context: Optional[Union[List[str], Tuple[List[str], Refiner]]] = None,\n    history: Optional[\n        Union[\n            List[Dict[str, str]],\n            Tuple[List[Dict[str, str]], Refiner],\n        ]\n    ] = None,\n    query: Optional[Union[str, Tuple[str, Refiner]]] = None,\n    text_format: TextFormat = TextFormat.RAW,\n    separator: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    One-liner to create packer and pack text immediately.\n\n    Default refining strategies are automatically applied (same as __init__):\n    - system/query: MinimalStrategy\n    - context/history: StandardStrategy\n\n    Args:\n        system: System message (str or (str, Refiner/Pipeline) tuple)\n        context: Context documents (list or (list, Refiner/Pipeline) tuple)\n        history: Conversation history (list or (list, Refiner/Pipeline) tuple)\n        query: Current query (str or (str, Refiner/Pipeline) tuple)\n        text_format: Text formatting strategy (RAW, MARKDOWN, XML)\n        separator: String to join items\n\n    Returns:\n        Packed text ready for completion API\n\n    Example (Simple):\n        &gt;&gt;&gt; prompt = TextPacker.quick_pack(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=[\"Doc 1\", \"Doc 2\"],\n        ...     query=\"What's the weather?\"\n        ... )\n\n    Example (With single Refiner):\n        &gt;&gt;&gt; from prompt_refiner import TextPacker, StripHTML, TextFormat\n        &gt;&gt;&gt; prompt = TextPacker.quick_pack(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\"], StripHTML()),\n        ...     query=\"What's the weather?\"\n        ... )\n\n    Example (With Pipeline - multiple refiners):\n        &gt;&gt;&gt; from prompt_refiner import (\n        ...     TextPacker, StripHTML, NormalizeWhitespace, TextFormat, Pipeline\n        ... )\n        &gt;&gt;&gt; cleaner = StripHTML() | NormalizeWhitespace()\n        &gt;&gt;&gt; # Or: cleaner = Pipeline([StripHTML(), NormalizeWhitespace()])\n        &gt;&gt;&gt; prompt = TextPacker.quick_pack(\n        ...     text_format=TextFormat.MARKDOWN,\n        ...     system=\"You are helpful.\",\n        ...     context=([\"&lt;div&gt;Doc 1&lt;/div&gt;\"], cleaner),\n        ...     query=\"What's the weather?\"\n        ... )\n    \"\"\"\n    packer = cls(\n        text_format=text_format,\n        separator=separator,\n        system=system,\n        context=context,\n        history=history,\n        query=query,\n    )\n    return packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.TextPacker.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>Pack items into formatted text for completion APIs.</p> <p>MARKDOWN format uses grouped sections: - INSTRUCTIONS: System prompts (ROLE_SYSTEM) - CONTEXT: RAG documents (ROLE_CONTEXT) - CONVERSATION: User/assistant history (ROLE_USER, ROLE_ASSISTANT) - INPUT: Current user query (ROLE_QUERY)</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted text string ready for completion API</p> Example <p>prompt = packer.pack() response = completion.create(model=\"llama-2-70b\", prompt=prompt)</p> Source code in <code>src/prompt_refiner/packer/text.py</code> <pre><code>def pack(self) -&gt; str:\n    \"\"\"\n    Pack items into formatted text for completion APIs.\n\n    MARKDOWN format uses grouped sections:\n    - INSTRUCTIONS: System prompts (ROLE_SYSTEM)\n    - CONTEXT: RAG documents (ROLE_CONTEXT)\n    - CONVERSATION: User/assistant history (ROLE_USER, ROLE_ASSISTANT)\n    - INPUT: Current user query (ROLE_QUERY)\n\n    Returns:\n        Formatted text string ready for completion API\n\n    Example:\n        &gt;&gt;&gt; prompt = packer.pack()\n        &gt;&gt;&gt; response = completion.create(model=\"llama-2-70b\", prompt=prompt)\n    \"\"\"\n    selected_items = self._select_items()\n\n    if not selected_items:\n        logger.warning(\"No items selected, returning empty string\")\n        return \"\"\n\n    # MARKDOWN format: Use grouped sections (saves tokens)\n    if self.text_format == TextFormat.MARKDOWN:\n        result = self._pack_markdown_grouped(selected_items)\n    else:\n        # RAW and XML: Use item-by-item formatting\n        parts = []\n        for item in selected_items:\n            formatted = self._format_item(item)\n            parts.append(formatted)\n        result = self.separator.join(parts)\n\n    logger.info(f\"Packed {len(selected_items)} items (format={self.text_format.value})\")\n    return result\n</code></pre>"},{"location":"api-reference/packer/#basepacker","title":"BasePacker","text":"<p>Abstract base class providing common packer functionality. You typically won't use this directly.</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker","title":"prompt_refiner.packer.BasePacker","text":"<pre><code>BasePacker(track_tokens=False, token_counter=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for prompt packers.</p> <p>Provides common functionality: - Adding items with priorities - JIT refinement with strategies/operations - Priority-based sorting</p> <p>Subclasses must implement: - pack(): Format and return packed items</p> <p>Initialize packer.</p> <p>Parameters:</p> Name Type Description Default <code>track_tokens</code> <code>bool</code> <p>Enable token tracking to measure refinement effectiveness</p> <code>False</code> <code>token_counter</code> <code>Optional[Callable[[str], int]]</code> <p>Function to count tokens (required if track_tokens=True)</p> <code>None</code> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def __init__(\n    self,\n    track_tokens: bool = False,\n    token_counter: Optional[Callable[[str], int]] = None,\n):\n    \"\"\"\n    Initialize packer.\n\n    Args:\n        track_tokens: Enable token tracking to measure refinement effectiveness\n        token_counter: Function to count tokens (required if track_tokens=True)\n    \"\"\"\n    self._items: List[PackableItem] = []\n    self._insertion_counter = 0\n\n    # Token tracking\n    self._track_tokens = track_tokens\n    self._token_counter = token_counter\n    self._raw_tokens = 0\n    self._refined_tokens = 0\n\n    # Validate: if tracking enabled, counter is required\n    if self._track_tokens and self._token_counter is None:\n        raise ValueError(\"token_counter is required when track_tokens=True\")\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker-attributes","title":"Attributes","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.token_stats","title":"token_stats  <code>property</code>","text":"<pre><code>token_stats\n</code></pre> <p>Get token savings statistics (only available when track_tokens=True).</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with the following keys: - raw_tokens: int - Total tokens before refinement - refined_tokens: int - Total tokens after refinement - saved_tokens: int - Tokens saved by refinement - saving_percent: str - Percentage saved (e.g., \"25.5%\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If token tracking is not enabled</p> Example <p>packer = MessagesPacker(track_tokens=True, token_counter=character_based_counter) packer.add(\"Hello\", role=\"user\", refine_with=StripHTML()) stats = packer.token_stats print(stats)</p>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker-functions","title":"Functions","text":""},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add","title":"add","text":"<pre><code>add(content, role, priority=None, refine_with=None)\n</code></pre> <p>Add an item to the packer.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text content to add</p> required <code>role</code> <code>RoleType</code> <p>Semantic role (required). Use ROLE_* constants: - ROLE_SYSTEM: System instructions - ROLE_QUERY: Current user question - ROLE_CONTEXT: RAG retrieved documents - ROLE_USER: User messages in conversation history - ROLE_ASSISTANT: Assistant messages in history</p> required <code>priority</code> <code>Optional[int]</code> <p>Priority level (use PRIORITY_* constants). If None, infers from role: - ROLE_SYSTEM \u2192 PRIORITY_SYSTEM (0) - ROLE_QUERY \u2192 PRIORITY_QUERY (10) - ROLE_CONTEXT \u2192 PRIORITY_HIGH (20) - ROLE_USER/ROLE_ASSISTANT \u2192 PRIORITY_LOW (40) - Other roles \u2192 PRIORITY_MEDIUM (30)</p> <code>None</code> <code>refine_with</code> <code>Optional[Refiner]</code> <p>Optional refiner or pipeline to apply before adding. Can be: - Single refiner: StripHTML() - Pipeline: StripHTML() | NormalizeWhitespace() - Pipeline from list: Pipeline([StripHTML(), NormalizeWhitespace()])</p> <code>None</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add(\n    self,\n    content: str,\n    role: RoleType,\n    priority: Optional[int] = None,\n    refine_with: Optional[Refiner] = None,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Add an item to the packer.\n\n    Args:\n        content: Text content to add\n        role: Semantic role (required). Use ROLE_* constants:\n            - ROLE_SYSTEM: System instructions\n            - ROLE_QUERY: Current user question\n            - ROLE_CONTEXT: RAG retrieved documents\n            - ROLE_USER: User messages in conversation history\n            - ROLE_ASSISTANT: Assistant messages in history\n        priority: Priority level (use PRIORITY_* constants). If None, infers from role:\n            - ROLE_SYSTEM \u2192 PRIORITY_SYSTEM (0)\n            - ROLE_QUERY \u2192 PRIORITY_QUERY (10)\n            - ROLE_CONTEXT \u2192 PRIORITY_HIGH (20)\n            - ROLE_USER/ROLE_ASSISTANT \u2192 PRIORITY_LOW (40)\n            - Other roles \u2192 PRIORITY_MEDIUM (30)\n        refine_with: Optional refiner or pipeline to apply before adding.\n            Can be:\n            - Single refiner: StripHTML()\n            - Pipeline: StripHTML() | NormalizeWhitespace()\n            - Pipeline from list: Pipeline([StripHTML(), NormalizeWhitespace()])\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Token tracking: count raw tokens BEFORE refinement\n    if self._track_tokens:\n        self._raw_tokens += self._token_counter(content)\n\n    # Smart priority defaults based on semantic roles\n    if priority is None:\n        if role == ROLE_SYSTEM:\n            priority = PRIORITY_SYSTEM  # 0 - Highest priority\n        elif role == ROLE_QUERY:\n            priority = PRIORITY_QUERY  # 10 - Current query is critical\n        elif role == ROLE_CONTEXT:\n            priority = PRIORITY_HIGH  # 20 - RAG documents\n        elif role in (ROLE_USER, ROLE_ASSISTANT):\n            priority = PRIORITY_LOW  # 40 - Conversation history\n        else:\n            priority = PRIORITY_MEDIUM  # 30 - Unknown roles\n\n    # JIT refinement\n    refined_content = content\n    if refine_with:\n        # Apply refinement (Refiner or Pipeline both use process() method)\n        refined_content = refine_with.process(content)\n\n    # Token tracking: count refined tokens AFTER refinement\n    if self._track_tokens:\n        self._refined_tokens += self._token_counter(refined_content)\n\n    content = refined_content\n\n    item = PackableItem(\n        content=content,\n        priority=priority,\n        insertion_index=self._insertion_counter,\n        role=role,\n    )\n\n    self._items.append(item)\n    self._insertion_counter += 1\n\n    logger.debug(f\"Added item: priority={priority}, role={role}\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.add_messages","title":"add_messages","text":"<pre><code>add_messages(messages, priority=PRIORITY_LOW)\n</code></pre> <p>Batch add messages (convenience method).</p> <p>Defaults to PRIORITY_LOW because conversation history is usually the first to be dropped in favor of RAG context and current queries.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, str]]</code> <p>List of message dicts with 'role' and 'content' keys</p> required <code>priority</code> <code>int</code> <p>Priority level for all messages (default: PRIORITY_LOW for history)</p> <code>PRIORITY_LOW</code> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def add_messages(\n    self,\n    messages: List[Dict[str, str]],\n    priority: int = PRIORITY_LOW,\n) -&gt; \"BasePacker\":\n    \"\"\"\n    Batch add messages (convenience method).\n\n    Defaults to PRIORITY_LOW because conversation history is usually the first\n    to be dropped in favor of RAG context and current queries.\n\n    Args:\n        messages: List of message dicts with 'role' and 'content' keys\n        priority: Priority level for all messages (default: PRIORITY_LOW for history)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    for msg in messages:\n        self.add(content=msg[\"content\"], role=msg[\"role\"], priority=priority)\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset the packer, removing all items and token stats.</p> <p>Returns:</p> Type Description <code>BasePacker</code> <p>Self for method chaining</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def reset(self) -&gt; \"BasePacker\":\n    \"\"\"\n    Reset the packer, removing all items and token stats.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._items.clear()\n    self._insertion_counter = 0\n\n    # Reset token tracking\n    if self._track_tokens:\n        self._raw_tokens = 0\n        self._refined_tokens = 0\n\n    logger.debug(\"Packer reset\")\n    return self\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.get_items","title":"get_items","text":"<pre><code>get_items()\n</code></pre> <p>Get information about all added items.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries containing item metadata</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>def get_items(self) -&gt; List[dict]:\n    \"\"\"\n    Get information about all added items.\n\n    Returns:\n        List of dictionaries containing item metadata\n    \"\"\"\n    return [\n        {\n            \"priority\": item.priority,\n            \"insertion_index\": item.insertion_index,\n            \"role\": item.role,\n        }\n        for item in self._items\n    ]\n</code></pre>"},{"location":"api-reference/packer/#prompt_refiner.packer.BasePacker.pack","title":"pack  <code>abstractmethod</code>","text":"<pre><code>pack()\n</code></pre> <p>Pack items into final format.</p> <p>Subclasses must implement this to return format-specific output: - MessagesPacker: Returns List[Dict[str, str]] - TextPacker: Returns str</p> Source code in <code>src/prompt_refiner/packer/base.py</code> <pre><code>@abstractmethod\ndef pack(self):\n    \"\"\"\n    Pack items into final format.\n\n    Subclasses must implement this to return format-specific output:\n    - MessagesPacker: Returns List[Dict[str, str]]\n    - TextPacker: Returns str\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/packer/#constants","title":"Constants","text":""},{"location":"api-reference/packer/#semantic-role-constants-recommended","title":"Semantic Role Constants (Recommended)","text":"<pre><code>from prompt_refiner import (\n    ROLE_SYSTEM,      # \"system\" - System instructions (auto: PRIORITY_SYSTEM = 0)\n    ROLE_QUERY,       # \"query\" - Current user question (auto: PRIORITY_QUERY = 10)\n    ROLE_CONTEXT,     # \"context\" - RAG documents (auto: PRIORITY_HIGH = 20)\n    ROLE_USER,        # \"user\" - User messages in history (auto: PRIORITY_LOW = 40)\n    ROLE_ASSISTANT,   # \"assistant\" - Assistant messages in history (auto: PRIORITY_LOW = 40)\n)\n</code></pre>"},{"location":"api-reference/packer/#priority-constants-optional","title":"Priority Constants (Optional)","text":"<pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_QUERY,    # 10 - Current user query (critical for response)\n    PRIORITY_HIGH,     # 20 - Important context (core RAG documents)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG documents)\n    PRIORITY_LOW,      # 40 - Optional content (old conversation history)\n)\n</code></pre> <p>Smart Priority Defaults</p> <p>You usually don't need to specify priority! Just use semantic roles and priority is auto-inferred:</p> <pre><code># Recommended: Use semantic roles (priority auto-inferred)\npacker.add(\"System prompt\", role=ROLE_SYSTEM)  # Auto: PRIORITY_SYSTEM (0)\npacker.add(\"User query\", role=ROLE_QUERY)      # Auto: PRIORITY_QUERY (10)\npacker.add(\"RAG doc\", role=ROLE_CONTEXT)       # Auto: PRIORITY_HIGH (20)\n\n# Advanced: Override priority if needed\npacker.add(\"Urgent RAG doc\", role=ROLE_CONTEXT, priority=PRIORITY_QUERY)\n</code></pre>"},{"location":"api-reference/packer/#textformat-enum","title":"TextFormat Enum","text":"<pre><code>from prompt_refiner import TextFormat\n\nTextFormat.RAW       # No delimiters, simple concatenation\nTextFormat.MARKDOWN  # Use ### ROLE: headers (grouped sections in v0.1.3+)\nTextFormat.XML       # Use &lt;role&gt;content&lt;/role&gt; tags\n</code></pre>"},{"location":"api-reference/packer/#default-refining-strategies","title":"Default Refining Strategies","text":"<p>Version 0.2.1+ introduces automatic refining strategies. When no explicit refiner is provided, packers apply sensible defaults:</p> <ul> <li><code>system</code>/<code>query</code>: MinimalStrategy (StripHTML + NormalizeWhitespace)</li> <li><code>context</code>/<code>history</code>: StandardStrategy (StripHTML + NormalizeWhitespace + Deduplicate)</li> </ul> <pre><code>from prompt_refiner import MessagesPacker\n\n# Automatic refining with defaults\npacker = MessagesPacker(\n    system=\"&lt;p&gt;You are helpful.&lt;/p&gt;\",  # Auto: MinimalStrategy\n    context=[\"&lt;div&gt;Doc 1&lt;/div&gt;\"],      # Auto: StandardStrategy\n    query=\"&lt;span&gt;What's the weather?&lt;/span&gt;\"  # Auto: MinimalStrategy\n)\n\n# Override with custom pipeline\npacker = MessagesPacker(\n    context=([\"&lt;div&gt;Doc&lt;/div&gt;\"], StripHTML() | NormalizeWhitespace())\n)\n</code></pre>"},{"location":"api-reference/packer/#token-savings-tracking","title":"Token Savings Tracking","text":"<p>Version 0.1.5+ introduces automatic token savings tracking to measure the optimization impact of <code>refine_with</code> operations.</p>"},{"location":"api-reference/packer/#enable-tracking","title":"Enable Tracking","text":"<pre><code># Opt-in to tracking with track_savings parameter\npacker = MessagesPacker(track_savings=True)\n\n# Add items with refinement\npacker.add(\n    \"&lt;div&gt;  Messy   HTML  &lt;/div&gt;\",\n    role=ROLE_CONTEXT,\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n\n# Get savings statistics\nsavings = packer.get_token_savings()\n# Returns: {\n#   'original_tokens': 25,      # Tokens before refinement\n#   'refined_tokens': 12,       # Tokens after refinement\n#   'saved_tokens': 13,         # Tokens saved\n#   'saving_percent': 52.0,     # Percentage saved\n#   'items_refined': 1          # Count of refined items\n# }\n</code></pre>"},{"location":"api-reference/packer/#key-features","title":"Key Features","text":"<ul> <li>Opt-in: Disabled by default (no overhead when not needed)</li> <li>Automatic aggregation: Tracks all items that use <code>refine_with</code></li> <li>Per-item and total: Aggregates savings across all refined items</li> <li>Works with both packers: Available for <code>MessagesPacker</code> and <code>TextPacker</code></li> </ul>"},{"location":"api-reference/packer/#example-with-real-api","title":"Example with Real API","text":"<pre><code>from prompt_refiner import MessagesPacker, StripHTML\nfrom openai import OpenAI\n\nclient = OpenAI()\npacker = MessagesPacker(track_savings=True)\n\n# Add multiple RAG documents with automatic cleaning\nfor doc in scraped_html_docs:\n    packer.add(doc, role=\"context\", refine_with=StripHTML())\n\n# Pack messages and check savings\nmessages = packer.pack()\nsavings = packer.get_token_savings()\n\nprint(f\"Saved {savings['saved_tokens']} tokens ({savings['saving_percent']:.1f}%)\")\n# Example output: \"Saved 1,234 tokens (23.5%)\"\n\n# Use cleaned messages with API\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages\n)\n</code></pre>"},{"location":"api-reference/packer/#when-to-use","title":"When to Use","text":"<p>\u2705 Use token savings tracking when: - You want to measure ROI of optimization efforts - Demonstrating token savings to stakeholders - A/B testing different refinement strategies - Monitoring optimization impact in production</p> <p>\u274c Skip tracking when: - Not using <code>refine_with</code> parameter (returns empty dict) - Performance is absolutely critical (negligible overhead, but why enable?) - You don't need savings metrics</p> <p>Combine with CountTokens</p> <p>For pipeline optimization (not packer), use <code>CountTokens</code> instead: <pre><code>from prompt_refiner import CountTokens, StripHTML, NormalizeWhitespace\n\ncounter = CountTokens(original_text=dirty_html)\npipeline = StripHTML() | NormalizeWhitespace()\nclean = pipeline.run(dirty_html)\ncounter.process(clean)\nprint(counter.format_stats())\n</code></pre></p>"},{"location":"api-reference/packer/#messagespacker-examples","title":"MessagesPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker\n\npacker = MessagesPacker()\n\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\"  # Auto: PRIORITY_SYSTEM (0)\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"query\"  # Auto: PRIORITY_QUERY (10)\n)\n\nmessages = packer.pack()  # List[Dict[str, str]]\n# Use directly: openai.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"api-reference/packer/#rag-with-conversation-history","title":"RAG with Conversation History","text":"<pre><code>from prompt_refiner import MessagesPacker, StripHTML\n\npacker = MessagesPacker()\n\n# System prompt\npacker.add(\n    \"Answer based on provided context.\",\n    role=\"system\"  # Auto: PRIORITY_SYSTEM (0)\n)\n\n# RAG documents with JIT cleaning\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=\"context\",  # Auto: PRIORITY_HIGH (20)\n    refine_with=StripHTML()\n)\n\n# Old conversation history\nold_messages = [\n    {\"role\": \"user\", \"content\": \"What is this library?\"},\n    {\"role\": \"assistant\", \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages)  # Auto: PRIORITY_LOW (40) for history\n\n# Current query\npacker.add(\n    \"How does it reduce costs?\",\n    role=\"query\"  # Auto: PRIORITY_QUERY (10)\n)\n\nmessages = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#textpacker-examples","title":"TextPacker Examples","text":""},{"location":"api-reference/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat\n\npacker = TextPacker(text_format=TextFormat.MARKDOWN)\n\npacker.add(\n    \"You are a QA assistant.\",\n    role=\"system\"  # Auto: PRIORITY_SYSTEM (0)\n)\n\npacker.add(\n    \"Context: Prompt-refiner is a library...\",\n    role=\"context\"  # Auto: PRIORITY_HIGH (20)\n)\n\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"query\"  # Auto: PRIORITY_QUERY (10)\n)\n\nprompt = packer.pack()  # str\n# Use with: completion.create(prompt=prompt)\n</code></pre>"},{"location":"api-reference/packer/#text-format-comparison","title":"Text Format Comparison","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat\n\n# RAW format (simple concatenation)\npacker = TextPacker(text_format=TextFormat.RAW)\npacker.add(\"System prompt\", role=\"system\")\npacker.add(\"User query\", role=\"query\")\nprompt = packer.pack()\n# Output:\n# System prompt\n#\n# User query\n\n# MARKDOWN format (grouped sections in v0.1.3+)\npacker = TextPacker(text_format=TextFormat.MARKDOWN)\npacker.add(\"System prompt\", role=\"system\")\npacker.add(\"Doc 1\", role=\"context\")\npacker.add(\"Doc 2\", role=\"context\")\npacker.add(\"User query\", role=\"query\")\nprompt = packer.pack()\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Doc 1\n# - Doc 2\n#\n# ### INPUT:\n# User query\n\n# XML format\npacker = TextPacker(text_format=TextFormat.XML)\npacker.add(\"System prompt\", role=\"system\")\npacker.add(\"User query\", role=\"query\")\nprompt = packer.pack()\n# Output:\n# &lt;system&gt;\n# System prompt\n# &lt;/system&gt;\n#\n# &lt;query&gt;\n# User query\n# &lt;/query&gt;\n</code></pre>"},{"location":"api-reference/packer/#common-features","title":"Common Features","text":""},{"location":"api-reference/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Both packers support Just-In-Time refinement:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Single operation\npacker.add(\n    \"&lt;div&gt;HTML content&lt;/div&gt;\",\n    role=\"context\",\n    refine_with=StripHTML()\n)\n\n# Multiple operations\npacker.add(\n    \"&lt;p&gt;  Messy   HTML  &lt;/p&gt;\",\n    role=\"context\",\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"api-reference/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>from prompt_refiner import MessagesPacker\n\nmessages = (\n    MessagesPacker()\n    .add(\"System prompt\", role=\"system\")\n    .add(\"User query\", role=\"query\")\n    .pack()\n)\n</code></pre>"},{"location":"api-reference/packer/#inspection","title":"Inspection","text":"<pre><code>from prompt_refiner import MessagesPacker\n\npacker = MessagesPacker()\npacker.add(\"Item 1\", role=\"system\")\npacker.add(\"Item 2\", role=\"query\")\n\n# Inspect items before packing\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Role: {item['role']}\")\n</code></pre>"},{"location":"api-reference/packer/#reset","title":"Reset","text":"<pre><code>from prompt_refiner import MessagesPacker\n\npacker = MessagesPacker()\npacker.add(\"First batch\", role=\"context\")\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=\"context\")\nmessages2 = packer.pack()\n</code></pre>"},{"location":"api-reference/packer/#algorithm-details","title":"Algorithm Details","text":"<ol> <li>Add Phase: Items are added with priorities, optional roles, and automatic/explicit refinement</li> <li>Refinement (v0.2.1+):</li> <li>Default strategies applied automatically (MinimalStrategy for system/query, StandardStrategy for context/history)</li> <li>Override with explicit refiner: <code>context=(docs, StripHTML() | NormalizeWhitespace())</code></li> <li>Skip refinement: Use <code>.add()</code> method with <code>refine_with=None</code></li> <li>Token Counting: Content tokens counted for savings tracking (when enabled)</li> <li>Sort Phase: Items are sorted by priority (lower number = higher priority), stable sort preserves insertion order</li> <li>Order Restoration: All items restored to insertion order for natural reading flow</li> <li>Format Phase:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code> (semantic roles mapped to API roles)</li> <li>TextPacker: Returns formatted <code>str</code> based on <code>text_format</code> (RAW, MARKDOWN, or XML)</li> </ol>"},{"location":"api-reference/packer/#tips","title":"Tips","text":"<p>Choose the Right Packer</p> <ul> <li>Use MessagesPacker for chat APIs (OpenAI, Anthropic)</li> <li>Use TextPacker for completion APIs (Llama Base, GPT-3)</li> </ul> <p>Use Semantic Roles (Recommended)</p> <p>Semantic roles auto-infer priorities, making code clearer:</p> <ul> <li><code>ROLE_SYSTEM</code>: System instructions \u2192 PRIORITY_SYSTEM (0)</li> <li><code>ROLE_QUERY</code>: Current user question \u2192 PRIORITY_QUERY (10)</li> <li><code>ROLE_CONTEXT</code>: RAG documents \u2192 PRIORITY_HIGH (20)</li> <li><code>ROLE_USER</code> / <code>ROLE_ASSISTANT</code>: Conversation history \u2192 PRIORITY_LOW (40)</li> </ul> <pre><code># Recommended: Clear intent with semantic roles\npacker.add(\"System prompt\", role=ROLE_SYSTEM)\npacker.add(\"Current query\", role=ROLE_QUERY)\npacker.add(\"RAG doc\", role=ROLE_CONTEXT)\n</code></pre> <p>Override Priority When Needed</p> <p>Most of the time semantic roles are enough, but you can override:</p> <pre><code># Make a RAG document urgent (higher priority than normal)\npacker.add(\"Critical doc\", role=ROLE_CONTEXT, priority=PRIORITY_QUERY)\n</code></pre> <p>Clean Before Packing</p> <p>Use <code>refine_with</code> to clean items before token counting:</p> <pre><code>packer.add(\n    dirty_html,\n    role=ROLE_CONTEXT,\n    refine_with=StripHTML()\n)\n</code></pre> <p>Grouped MARKDOWN Saves Tokens</p> <p>TextPacker with MARKDOWN format groups items by section, saving tokens:</p> <pre><code># Old (per-item headers): ### CONTEXT:\\nDoc 1\\n\\n### CONTEXT:\\nDoc 2\n# New (grouped): ### CONTEXT:\\n- Doc 1\\n- Doc 2\n</code></pre>"},{"location":"api-reference/refiner/","title":"Refiner Class","text":"<p>The <code>Refiner</code> class is the core pipeline builder that allows you to chain multiple operations together.</p>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner","title":"prompt_refiner.refiner.Refiner","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all prompt refining operations.</p>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner-functions","title":"Functions","text":""},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(text)\n</code></pre> <p>Process the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>The processed text</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>@abstractmethod\ndef process(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The input text to process\n\n    Returns:\n        The processed text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre> <p>Support pipe operator syntax for composing refiners.</p> <p>Enables LangChain-style pipeline composition: refiner1 | refiner2 | refiner3</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Refiner</code> <p>The refiner to chain with this refiner</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>A Pipeline containing both refiners</p> Example <p>from prompt_refiner import StripHTML, NormalizeWhitespace pipeline = StripHTML() | NormalizeWhitespace() result = pipeline.run(\"  hello  \")</p> Source code in <code>src/prompt_refiner/refiner.py</code> <pre><code>def __or__(self, other: \"Refiner\") -&gt; \"Pipeline\":\n    \"\"\"\n    Support pipe operator syntax for composing refiners.\n\n    Enables LangChain-style pipeline composition: refiner1 | refiner2 | refiner3\n\n    Args:\n        other: The refiner to chain with this refiner\n\n    Returns:\n        A Pipeline containing both refiners\n\n    Example:\n        &gt;&gt;&gt; from prompt_refiner import StripHTML, NormalizeWhitespace\n        &gt;&gt;&gt; pipeline = StripHTML() | NormalizeWhitespace()\n        &gt;&gt;&gt; result = pipeline.run(\"&lt;div&gt;  hello  &lt;/div&gt;\")\n        &gt;&gt;&gt; # Returns: \"hello\"\n    \"\"\"\n    from .pipeline import Pipeline\n\n    return Pipeline().pipe(self).pipe(other)\n</code></pre>"},{"location":"api-reference/refiner/#prompt_refiner.refiner.Refiner.__or__--returns-hello","title":"Returns: \"hello\"","text":""},{"location":"api-reference/refiner/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/refiner/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the pipe operator\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre>"},{"location":"api-reference/refiner/#fluent-api-with-pipe","title":"Fluent API with .pipe()","text":"<p>The <code>Refiner</code> class supports method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace\n\n# Create a pipeline using the fluent API\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n)\n\n# Execute the pipeline\nresult = pipeline.run(\"&lt;p&gt;Hello   World!&lt;/p&gt;\")\nprint(result)  # \"Hello World!\"\n</code></pre> <p>Both approaches work identically - choose the one that fits your style.</p>"},{"location":"api-reference/refiner/#pipeline-execution","title":"Pipeline Execution","text":"<p>When you call <code>run(text)</code>, the Refiner:</p> <ol> <li>Takes the input text</li> <li>Passes it through each operation in sequence</li> <li>Each operation's output becomes the next operation's input</li> <li>Returns the final processed text</li> </ol> <pre><code># Pipeline: text \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 result\nresult = refiner.run(text)\n</code></pre>"},{"location":"api-reference/scrubber/","title":"Scrubber Module","text":"<p>The Scrubber module provides operations for security and privacy, including automatic PII redaction.</p>"},{"location":"api-reference/scrubber/#redactpii","title":"RedactPII","text":"<p>Redact sensitive personally identifiable information (PII) from text using regex patterns.</p>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII","title":"prompt_refiner.scrubber.RedactPII","text":"<pre><code>RedactPII(\n    redact_types=None,\n    custom_patterns=None,\n    custom_keywords=None,\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Redact sensitive information from text using regex patterns.</p> <p>Initialize the PII redaction operation.</p> <p>Parameters:</p> Name Type Description Default <code>redact_types</code> <code>Optional[Set[str]]</code> <p>Set of PII types to redact (default: all) Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"</p> <code>None</code> <code>custom_patterns</code> <code>Optional[dict[str, str]]</code> <p>Dictionary of custom regex patterns to apply Format: {\"name\": \"regex_pattern\"}</p> <code>None</code> <code>custom_keywords</code> <code>Optional[Set[str]]</code> <p>Set of custom keywords to redact (case-insensitive)</p> <code>None</code> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def __init__(\n    self,\n    redact_types: Optional[Set[str]] = None,\n    custom_patterns: Optional[dict[str, str]] = None,\n    custom_keywords: Optional[Set[str]] = None,\n):\n    \"\"\"\n    Initialize the PII redaction operation.\n\n    Args:\n        redact_types: Set of PII types to redact (default: all)\n            Options: \"email\", \"phone\", \"ip\", \"credit_card\", \"ssn\", \"url\"\n        custom_patterns: Dictionary of custom regex patterns to apply\n            Format: {\"name\": \"regex_pattern\"}\n        custom_keywords: Set of custom keywords to redact (case-insensitive)\n    \"\"\"\n    self.redact_types = redact_types or set(self.PATTERNS.keys())\n    self.custom_patterns = custom_patterns or {}\n    self.custom_keywords = custom_keywords or set()\n</code></pre>"},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII-functions","title":"Functions","text":""},{"location":"api-reference/scrubber/#prompt_refiner.scrubber.RedactPII.process","title":"process","text":"<pre><code>process(text)\n</code></pre> <p>Redact PII from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with PII redacted</p> Source code in <code>src/prompt_refiner/scrubber/pii.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"\n    Redact PII from the input text.\n\n    Args:\n        text: The input text\n\n    Returns:\n        Text with PII redacted\n    \"\"\"\n    result = text\n\n    # Apply standard PII patterns\n    for pii_type in self.redact_types:\n        if pii_type in self.PATTERNS:\n            pattern = self.PATTERNS[pii_type]\n            replacement = self.REPLACEMENTS.get(pii_type, \"[REDACTED]\")\n            result = re.sub(pattern, replacement, result)\n\n    # Apply custom patterns\n    for name, pattern in self.custom_patterns.items():\n        replacement = f\"[{name.upper()}]\"\n        result = re.sub(pattern, replacement, result)\n\n    # Apply custom keywords (case-insensitive)\n    for keyword in self.custom_keywords:\n        # Use word boundaries to avoid partial matches\n        pattern = rf\"\\b{re.escape(keyword)}\\b\"\n        result = re.sub(pattern, \"[REDACTED]\", result, flags=re.IGNORECASE)\n\n    return result\n</code></pre>"},{"location":"api-reference/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code>: Email addresses \u2192 <code>[EMAIL]</code></li> <li><code>phone</code>: Phone numbers (US format) \u2192 <code>[PHONE]</code></li> <li><code>ip</code>: IP addresses \u2192 <code>[IP]</code></li> <li><code>credit_card</code>: Credit card numbers \u2192 <code>[CARD]</code></li> <li><code>ssn</code>: Social Security Numbers \u2192 <code>[SSN]</code></li> <li><code>url</code>: URLs \u2192 <code>[URL]</code></li> </ul>"},{"location":"api-reference/scrubber/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact me at john@example.com or 555-123-4567\")\n# Output: \"Contact me at [EMAIL] or [PHONE]\"\n\n# Redact specific types only\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\nresult = redactor.process(\"Email: john@example.com, IP: 192.168.1.1\")\n# Output: \"Email: [EMAIL], IP: 192.168.1.1\"\n\n# Custom patterns\nredactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\nresult = redactor.process(\"Employee EMP-12345 accessed the system\")\n# Output: \"Employee [EMPLOYEE_ID] accessed the system\"\n\n# Custom keywords (case-insensitive)\nredactor = RedactPII(\n    custom_keywords={\"confidential\", \"secret\"}\n)\nresult = redactor.process(\"This is Confidential information\")\n# Output: \"This is [REDACTED] information\"\n</code></pre>"},{"location":"api-reference/scrubber/#combining-options","title":"Combining Options","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact standard PII + custom patterns + keywords\nredactor = RedactPII(\n    redact_types={\"email\", \"phone\", \"ssn\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"},\n    custom_keywords={\"internal\", \"confidential\"}\n)\n\ntext = \"\"\"\nEmployee EMP-12345\nEmail: john@example.com\nPhone: 555-123-4567\nSSN: 123-45-6789\nThis is Confidential information for internal use only.\n\"\"\"\n\nresult = redactor.process(text)\n</code></pre>"},{"location":"api-reference/scrubber/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-reference/scrubber/#before-sending-to-llm-apis","title":"Before Sending to LLM APIs","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nsecure_pipeline = (\n    Refiner()\n    .pipe(RedactPII(redact_types={\"email\", \"phone\", \"ssn\", \"credit_card\"}))\n)\n\n# Safe to send to external APIs\nsecure_text = secure_pipeline.run(user_input)\n</code></pre>"},{"location":"api-reference/scrubber/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\nlog_redactor = (\n    Refiner()\n    .pipe(RedactPII())  # Redact all PII types\n)\n\n# Safe to log\nsafe_log = log_redactor.run(sensitive_data)\nlogger.info(safe_log)\n</code></pre>"},{"location":"api-reference/scrubber/#data-export-compliance","title":"Data Export Compliance","text":"<pre><code>from prompt_refiner import Refiner, RedactPII\n\n# Custom redaction for specific compliance needs\ngdpr_redactor = (\n    Refiner()\n    .pipe(RedactPII(\n        redact_types={\"email\", \"phone\", \"ip\"},\n        custom_keywords={\"customer_name\", \"address\", \"dob\"}\n    ))\n)\n\nexport_data = gdpr_redactor.run(user_data)\n</code></pre>"},{"location":"api-reference/scrubber/#security-best-practices","title":"Security Best Practices","text":"<p>Regex Limitations</p> <p>PII redaction uses regex patterns which may not catch all variations. For production use:</p> <ul> <li>Test thoroughly with your specific data</li> <li>Consider using specialized PII detection services for critical applications</li> <li>Add custom patterns for domain-specific PII</li> <li>Review redacted output before sending to external services</li> </ul> <p>Defense in Depth</p> <p>PII redaction is one layer of security. Always:</p> <ul> <li>Validate and sanitize user input</li> <li>Use proper authentication and authorization</li> <li>Encrypt data in transit and at rest</li> <li>Follow your organization's security policies</li> </ul>"},{"location":"api-reference/strategy/","title":"Strategy Module API Reference","text":"<p>The Strategy module provides benchmark-tested preset strategies for token optimization. Use these when you want quick savings without manually configuring individual operations.</p>"},{"location":"api-reference/strategy/#overview","title":"Overview","text":"<p>Version 0.1.5+ introduces three preset strategies optimized for different use cases. Version 0.2.0 refactored strategies to inherit directly from Pipeline for a simpler API.</p> Strategy Token Reduction Quality Use Case Minimal 4.3% 98.7% Maximum quality, minimal risk Standard 4.8% 98.4% RAG contexts with duplicates Aggressive 15% 96.4% Cost optimization, long contexts <p>Strategies now inherit from <code>Pipeline</code>, so you can use them directly without calling <code>.create_refiner()</code>. They're fully extensible with <code>.pipe()</code>.</p>"},{"location":"api-reference/strategy/#minimalstrategy","title":"MinimalStrategy","text":"<p>Basic cleaning with minimal token reduction, prioritizing quality preservation.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy","title":"prompt_refiner.strategy.MinimalStrategy","text":"<pre><code>MinimalStrategy(\n    strip_html=True, strip_html_to_markdown=False\n)\n</code></pre> <p>               Bases: <code>Pipeline</code></p> <p>Minimal strategy: Basic cleaning with minimal token reduction.</p> <p>This strategy is itself a Pipeline, so you can use it directly or extend it.</p> <p>Refiners: - StripHTML: Remove HTML tags (optional) - NormalizeWhitespace: Collapse excessive whitespace</p> <p>Characteristics: - Token reduction: ~4.3% - Quality: 98.7% (cosine similarity) - Use case: When quality is paramount, minimal risk - Latency: 0.05ms per 1k tokens</p> Example <p>Initialize minimal strategy with configured operators.</p> <p>Parameters:</p> Name Type Description Default <code>strip_html</code> <code>bool</code> <p>Whether to include StripHTML operator (default: True)</p> <code>True</code> <code>strip_html_to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> Source code in <code>src/prompt_refiner/strategy/minimal.py</code> <pre><code>def __init__(\n    self,\n    strip_html: bool = True,\n    strip_html_to_markdown: bool = False,\n):\n    \"\"\"\n    Initialize minimal strategy with configured operators.\n\n    Args:\n        strip_html: Whether to include StripHTML operator (default: True)\n        strip_html_to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n    \"\"\"\n    operations = []\n\n    if strip_html:\n        operations.append(StripHTML(to_markdown=strip_html_to_markdown))\n\n    operations.append(NormalizeWhitespace())\n\n    # Initialize Pipeline with the configured operators\n    super().__init__(operations)\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy--use-with-defaults","title":"Use with defaults","text":"<p>strategy = MinimalStrategy() cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy--customize-operators","title":"Customize operators","text":"<p>strategy = MinimalStrategy( ...     strip_html_to_markdown=True ... ) cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy--extend-with-additional-operators","title":"Extend with additional operators","text":"<p>extended = MinimalStrategy().pipe(RedactPII()) cleaned = extended.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.MinimalStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#operations","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> </ul>"},{"location":"api-reference/strategy/#example","title":"Example","text":"<pre><code>from prompt_refiner.strategy import MinimalStrategy\n\n# Use strategy directly (v0.2.0+)\nstrategy = MinimalStrategy()\ncleaned = strategy.run(\"&lt;div&gt;  Your HTML content  &lt;/div&gt;\")\n# Output: \"Your HTML content\"\n\n# With Markdown conversion\nstrategy = MinimalStrategy(strip_html_to_markdown=True)\ncleaned = strategy.run(\"&lt;strong&gt;bold&lt;/strong&gt; text\")\n# Output: \"**bold** text\"\n\n# Extend with additional operations\nfrom prompt_refiner import RedactPII\nextended = MinimalStrategy().pipe(RedactPII(redact_types={\"email\"}))\ncleaned = extended.run(text)\n</code></pre>"},{"location":"api-reference/strategy/#standardstrategy","title":"StandardStrategy","text":"<p>Enhanced cleaning with deduplication for RAG contexts with potential duplicates.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy","title":"prompt_refiner.strategy.StandardStrategy","text":"<pre><code>StandardStrategy(\n    strip_html=True,\n    strip_html_to_markdown=False,\n    deduplicate_method=\"jaccard\",\n    deduplicate_similarity_threshold=0.8,\n    deduplicate_granularity=\"sentence\",\n)\n</code></pre> <p>               Bases: <code>Pipeline</code></p> <p>Standard strategy: Cleaning plus deduplication.</p> <p>This strategy is itself a Pipeline, so you can use it directly or extend it.</p> <p>Refiners: - StripHTML: Remove HTML tags (optional) - NormalizeWhitespace: Collapse excessive whitespace - Deduplicate: Remove similar content</p> <p>Characteristics: - Token reduction: ~4.8% - Quality: 98.4% (cosine similarity) - Use case: RAG contexts with potential duplicates - Latency: 0.25ms per 1k tokens</p> Example <p>Initialize standard strategy with configured operators.</p> <p>Parameters:</p> Name Type Description Default <code>strip_html</code> <code>bool</code> <p>Whether to include StripHTML operator (default: True)</p> <code>True</code> <code>strip_html_to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> <code>deduplicate_method</code> <code>Literal['jaccard', 'levenshtein']</code> <p>Deduplication method (default: \"jaccard\")</p> <code>'jaccard'</code> <code>deduplicate_similarity_threshold</code> <code>float</code> <p>Similarity threshold (default: 0.8)</p> <code>0.8</code> <code>deduplicate_granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Deduplication granularity (default: \"sentence\")</p> <code>'sentence'</code> Source code in <code>src/prompt_refiner/strategy/standard.py</code> <pre><code>def __init__(\n    self,\n    # Parameters to configure StripHTML operator\n    strip_html: bool = True,\n    strip_html_to_markdown: bool = False,\n    # Parameters to configure Deduplicate operator\n    deduplicate_method: Literal[\"jaccard\", \"levenshtein\"] = \"jaccard\",\n    deduplicate_similarity_threshold: float = 0.8,\n    deduplicate_granularity: Literal[\"sentence\", \"paragraph\"] = \"sentence\",\n):\n    \"\"\"\n    Initialize standard strategy with configured operators.\n\n    Args:\n        strip_html: Whether to include StripHTML operator (default: True)\n        strip_html_to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n        deduplicate_method: Deduplication method (default: \"jaccard\")\n        deduplicate_similarity_threshold: Similarity threshold (default: 0.8)\n        deduplicate_granularity: Deduplication granularity (default: \"sentence\")\n    \"\"\"\n    operations = []\n\n    if strip_html:\n        operations.append(StripHTML(to_markdown=strip_html_to_markdown))\n\n    operations.append(NormalizeWhitespace())\n\n    operations.append(\n        Deduplicate(\n            method=deduplicate_method,\n            similarity_threshold=deduplicate_similarity_threshold,\n            granularity=deduplicate_granularity,\n        )\n    )\n\n    # Initialize Pipeline with the configured operators\n    super().__init__(operations)\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy--use-with-defaults","title":"Use with defaults","text":"<p>strategy = StandardStrategy() cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy--customize-operator-parameters","title":"Customize operator parameters","text":"<p>strategy = StandardStrategy( ...     strip_html_to_markdown=True, ...     deduplicate_method=\"levenshtein\", ...     deduplicate_similarity_threshold=0.9, ...     deduplicate_granularity=\"paragraph\" ... ) cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy--extend-with-additional-operators","title":"Extend with additional operators","text":"<p>extended = StandardStrategy().pipe(TruncateTokens(max_tokens=500)) cleaned = extended.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.StandardStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#operations_1","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> <li><code>Deduplicate()</code> - Remove similar content (sentence-level, 0.8 threshold)</li> </ul>"},{"location":"api-reference/strategy/#example_1","title":"Example","text":"<pre><code>from prompt_refiner.strategy import StandardStrategy\n\n# Use strategy directly (v0.2.0+)\nstrategy = StandardStrategy()\ntext = \"&lt;div&gt;Hello world. Hello world. Goodbye world.&lt;/div&gt;\"\ncleaned = strategy.run(text)\n# Output: \"Hello world. Goodbye world.\"  (duplicate removed)\n\n# Custom similarity threshold\nstrategy = StandardStrategy(deduplicate_similarity_threshold=0.7)\n\n# Alternative deduplication method\nstrategy = StandardStrategy(deduplicate_method=\"levenshtein\")\n</code></pre>"},{"location":"api-reference/strategy/#aggressivestrategy","title":"AggressiveStrategy","text":"<p>Maximum token reduction with deduplication and truncation for cost optimization.</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy","title":"prompt_refiner.strategy.AggressiveStrategy","text":"<pre><code>AggressiveStrategy(\n    strip_html=True,\n    strip_html_to_markdown=False,\n    deduplicate_method=\"jaccard\",\n    deduplicate_similarity_threshold=0.7,\n    deduplicate_granularity=\"sentence\",\n)\n</code></pre> <p>               Bases: <code>Pipeline</code></p> <p>Aggressive strategy: Maximum token reduction through aggressive deduplication.</p> <p>This strategy is itself a Pipeline, so you can use it directly or extend it.</p> <p>Refiners: - StripHTML: Remove HTML tags (optional) - NormalizeWhitespace: Collapse excessive whitespace - Deduplicate: Aggressively remove similar content (threshold: 0.7)</p> <p>Characteristics: - Token reduction: ~5-10% (higher with duplicate content) - Quality: 96-98% (cosine similarity) - Use case: Cost optimization with duplicate/redundant content - Latency: 0.25ms per 1k tokens</p> <p>Note: For token budget control, use Packer's max_tokens parameter instead.</p> Example <p>Initialize aggressive strategy with configured operators.</p> <p>Parameters:</p> Name Type Description Default <code>strip_html</code> <code>bool</code> <p>Whether to include StripHTML operator (default: True)</p> <code>True</code> <code>strip_html_to_markdown</code> <code>bool</code> <p>Convert HTML to Markdown instead of stripping (default: False)</p> <code>False</code> <code>deduplicate_method</code> <code>Literal['jaccard', 'levenshtein']</code> <p>Deduplication method (default: \"jaccard\")</p> <code>'jaccard'</code> <code>deduplicate_similarity_threshold</code> <code>float</code> <p>Similarity threshold for aggressive deduplication (default: 0.7)</p> <code>0.7</code> <code>deduplicate_granularity</code> <code>Literal['sentence', 'paragraph']</code> <p>Deduplication granularity (default: \"sentence\")</p> <code>'sentence'</code> Source code in <code>src/prompt_refiner/strategy/aggressive.py</code> <pre><code>def __init__(\n    self,\n    # Parameters to configure StripHTML operator\n    strip_html: bool = True,\n    strip_html_to_markdown: bool = False,\n    # Parameters to configure Deduplicate operator\n    deduplicate_method: Literal[\"jaccard\", \"levenshtein\"] = \"jaccard\",\n    deduplicate_similarity_threshold: float = 0.7,\n    deduplicate_granularity: Literal[\"sentence\", \"paragraph\"] = \"sentence\",\n):\n    \"\"\"\n    Initialize aggressive strategy with configured operators.\n\n    Args:\n        strip_html: Whether to include StripHTML operator (default: True)\n        strip_html_to_markdown: Convert HTML to Markdown instead of stripping (default: False)\n        deduplicate_method: Deduplication method (default: \"jaccard\")\n        deduplicate_similarity_threshold: Similarity threshold for aggressive deduplication\n            (default: 0.7)\n        deduplicate_granularity: Deduplication granularity (default: \"sentence\")\n    \"\"\"\n    operations = []\n\n    if strip_html:\n        operations.append(StripHTML(to_markdown=strip_html_to_markdown))\n\n    operations.append(NormalizeWhitespace())\n\n    operations.append(\n        Deduplicate(\n            method=deduplicate_method,\n            similarity_threshold=deduplicate_similarity_threshold,\n            granularity=deduplicate_granularity,\n        )\n    )\n\n    # Initialize Pipeline with the configured operators\n    super().__init__(operations)\n</code></pre>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy--use-with-defaults","title":"Use with defaults","text":"<p>strategy = AggressiveStrategy() cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy--customize-operator-parameters","title":"Customize operator parameters","text":"<p>strategy = AggressiveStrategy( ...     strip_html_to_markdown=True, ...     deduplicate_method=\"levenshtein\", ...     deduplicate_similarity_threshold=0.6, ...     deduplicate_granularity=\"paragraph\" ... ) cleaned = strategy.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy--extend-with-additional-operators","title":"Extend with additional operators","text":"<p>extended = AggressiveStrategy().pipe(RedactPII()) cleaned = extended.run(text)</p>"},{"location":"api-reference/strategy/#prompt_refiner.strategy.AggressiveStrategy-functions","title":"Functions","text":""},{"location":"api-reference/strategy/#operations_2","title":"Operations","text":"<ul> <li><code>StripHTML()</code> - Remove HTML tags</li> <li><code>NormalizeWhitespace()</code> - Collapse excessive whitespace</li> <li><code>Deduplicate()</code> - Remove similar content (sentence-level, 0.7 threshold)</li> <li><code>TruncateTokens()</code> - Limit to max_tokens (default: 150)</li> </ul>"},{"location":"api-reference/strategy/#example_2","title":"Example","text":"<pre><code>from prompt_refiner.strategy import AggressiveStrategy\n\n# Use strategy directly (v0.2.0+) with default truncate_max_tokens=150\nstrategy = AggressiveStrategy()\nlong_text = \"word \" * 100  # 100 words\ncleaned = strategy.run(long_text)\n# Output: Truncated to ~150 tokens with duplicates removed\n\n# Custom max_tokens and truncation strategy\nstrategy = AggressiveStrategy(\n    truncate_max_tokens=200,\n    truncate_strategy=\"tail\"  # Keep last 200 tokens\n)\n\n# More aggressive deduplication\nstrategy = AggressiveStrategy(\n    truncate_max_tokens=100,\n    deduplicate_similarity_threshold=0.6  # More aggressive duplicate detection\n)\n</code></pre>"},{"location":"api-reference/strategy/#creating-custom-strategies","title":"Creating Custom Strategies","text":"<p>Custom strategies can be created by inheriting from <code>Pipeline</code>:</p> <pre><code>from prompt_refiner import Pipeline, StripHTML, NormalizeWhitespace, RedactPII\n\nclass CustomStrategy(Pipeline):\n    def __init__(self, redact_pii: bool = True):\n        operations = [StripHTML(), NormalizeWhitespace()]\n        if redact_pii:\n            operations.append(RedactPII(redact_types={\"email\", \"phone\"}))\n        super().__init__(operations)\n\n# Use custom strategy directly\nstrategy = CustomStrategy(redact_pii=True)\ncleaned = strategy.run(text)\n</code></pre>"},{"location":"api-reference/strategy/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api-reference/strategy/#basic-usage-v020","title":"Basic Usage (v0.2.0+)","text":"<pre><code>from prompt_refiner.strategy import MinimalStrategy, StandardStrategy, AggressiveStrategy\n\n# Quick start with minimal - use directly\nstrategy = MinimalStrategy()\ncleaned = strategy.run(text)\n\n# Standard for RAG with duplicates\nstrategy = StandardStrategy()\ncleaned = strategy.run(rag_context)\n\n# Aggressive for cost optimization\nstrategy = AggressiveStrategy(truncate_max_tokens=200)\ncleaned = strategy.run(long_context)\n</code></pre>"},{"location":"api-reference/strategy/#composition-with-additional-operations","title":"Composition with Additional Operations","text":"<p>Strategies inherit from <code>Pipeline</code>, so you can extend them with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy\nfrom prompt_refiner import RedactPII, Deduplicate\n\n# Start with minimal, add PII redaction\nextended = MinimalStrategy().pipe(RedactPII(redact_types={\"email\"}))\ncleaned = extended.run(text)\n\n# Start with standard, add more aggressive deduplication\nfrom prompt_refiner.strategy import StandardStrategy\nextended = StandardStrategy().pipe(Deduplicate(similarity_threshold=0.6))\ncleaned = extended.run(text)\n</code></pre>"},{"location":"api-reference/strategy/#using-process-method","title":"Using .process() Method","text":"<p>Strategies also support the <code>.process()</code> method from the Refiner interface:</p> <pre><code>from prompt_refiner.strategy import MinimalStrategy\n\nstrategy = MinimalStrategy()\ncleaned = strategy.process(text)  # Equivalent to strategy.run(text)\n</code></pre>"},{"location":"api-reference/strategy/#choosing-a-strategy","title":"Choosing a Strategy","text":""},{"location":"api-reference/strategy/#minimal-strategy","title":"Minimal Strategy","text":"<p>\u2705 Use when: - Quality is paramount - Minimal risk tolerance - Processing structured content - First time optimizing prompts</p> <p>\u274c Avoid when: - Budget constraints are tight - Dealing with very long contexts - Content has significant duplication</p>"},{"location":"api-reference/strategy/#standard-strategy","title":"Standard Strategy","text":"<p>\u2705 Use when: - RAG contexts with potential duplicates - Balanced quality and savings needed - Processing web-scraped content - General-purpose optimization</p> <p>\u274c Avoid when: - Context is already clean and unique - Maximum quality preservation required - Very tight token budgets</p>"},{"location":"api-reference/strategy/#aggressive-strategy","title":"Aggressive Strategy","text":"<p>\u2705 Use when: - Cost optimization is priority - Token budgets are tight - Processing very long contexts - Quality tolerance is lenient</p> <p>\u274c Avoid when: - Quality cannot be compromised - Context is already short - Truncation would remove critical info</p>"},{"location":"api-reference/strategy/#configuration-reference-v020","title":"Configuration Reference (v0.2.0+)","text":""},{"location":"api-reference/strategy/#minimalstrategy-parameters","title":"MinimalStrategy Parameters","text":"Parameter Type Default Description <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>strip_html_to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping"},{"location":"api-reference/strategy/#standardstrategy-parameters","title":"StandardStrategy Parameters","text":"Parameter Type Default Description <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>strip_html_to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping <code>deduplicate_method</code> <code>Literal[\"jaccard\", \"levenshtein\"]</code> <code>\"jaccard\"</code> Deduplication algorithm <code>deduplicate_similarity_threshold</code> <code>float</code> <code>0.8</code> Threshold for deduplication (0.0-1.0) <code>deduplicate_granularity</code> <code>Literal[\"sentence\", \"paragraph\"]</code> <code>\"sentence\"</code> Deduplication granularity"},{"location":"api-reference/strategy/#aggressivestrategy-parameters","title":"AggressiveStrategy Parameters","text":"Parameter Type Default Description <code>truncate_max_tokens</code> <code>int</code> <code>150</code> Maximum tokens to keep <code>truncate_strategy</code> <code>Literal[\"head\", \"tail\", \"middle_out\"]</code> <code>\"head\"</code> Which part of text to keep <code>strip_html</code> <code>bool</code> <code>True</code> Whether to strip HTML tags <code>strip_html_to_markdown</code> <code>bool</code> <code>False</code> Convert HTML to Markdown instead of stripping <code>deduplicate_method</code> <code>Literal[\"jaccard\", \"levenshtein\"]</code> <code>\"jaccard\"</code> Deduplication algorithm <code>deduplicate_similarity_threshold</code> <code>float</code> <code>0.7</code> Threshold for deduplication (0.0-1.0) <code>deduplicate_granularity</code> <code>Literal[\"sentence\", \"paragraph\"]</code> <code>\"sentence\"</code> Deduplication granularity"},{"location":"api-reference/strategy/#see-also","title":"See Also","text":"<ul> <li>Examples - Comprehensive examples</li> <li>Benchmark Results - Performance and quality metrics</li> <li>Refiner API - Pipeline composition</li> </ul>"},{"location":"api-reference/tools/","title":"Tools Module","text":"<p>The Tools module optimizes AI agent function calling by compressing tool schemas and responses. Achieves 57% average token reduction with 100% lossless compression.</p>"},{"location":"api-reference/tools/#schemacompressor","title":"SchemaCompressor","text":"<p>Compress tool/function schemas (OpenAI, Anthropic format) while preserving 100% of protocol fields.</p>"},{"location":"api-reference/tools/#prompt_refiner.tools.SchemaCompressor","title":"prompt_refiner.tools.SchemaCompressor","text":"<pre><code>SchemaCompressor(\n    drop_examples=True,\n    drop_titles=True,\n    drop_markdown_formatting=True,\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Compress tool schemas to save tokens while preserving functionality.</p> <p>This operation compresses tool schema definitions (e.g., OpenAI function calling schemas) by removing documentation overhead while keeping all protocol-level fields intact.</p> <p>What is modified: - description fields (truncated and cleaned) - title fields (removed if configured) - examples fields (removed if configured) - markdown formatting (removed if configured) - excessive whitespace</p> <p>What is never modified: - name - type - properties - required - enum - Any other protocol-level fields</p> <p>Parameters:</p> Name Type Description Default <code>drop_examples</code> <code>bool</code> <p>Remove examples fields (default: True)</p> <code>True</code> <code>drop_titles</code> <code>bool</code> <p>Remove title fields (default: True)</p> <code>True</code> <code>drop_markdown_formatting</code> <code>bool</code> <p>Remove markdown formatting (default: True)</p> <code>True</code> Example <p>from prompt_refiner import SchemaCompressor</p> <p>tools = [{ ...     \"type\": \"function\", ...     \"function\": { ...         \"name\": \"search_flights\", ...         \"description\": \"Search for available flights between two airports. \" ...                        \"This is a very long description with examples...\", ...         \"parameters\": { ...             \"type\": \"object\", ...             \"properties\": { ...                 \"origin\": { ...                     \"type\": \"string\", ...                     \"description\": \"Origin airport IATA code, like <code>LAX</code>\" ...                 } ...             } ...         } ...     } ... }]</p> <p>compressor = SchemaCompressor(drop_markdown_formatting=True) compressed = compressor.process(tools)</p> Use Cases <ul> <li>Function Calling: Reduce token usage in OpenAI/Anthropic function schemas</li> <li>Agent Systems: Optimize tool definitions in agent prompts</li> <li>Cost Reduction: Save 20-60% tokens on verbose tool schemas</li> <li>Context Management: Fit more tools within token budget</li> </ul> <p>Initialize schema compressor.</p> <p>Parameters:</p> Name Type Description Default <code>drop_examples</code> <code>bool</code> <p>Remove examples fields (default: True)</p> <code>True</code> <code>drop_titles</code> <code>bool</code> <p>Remove title fields (default: True)</p> <code>True</code> <code>drop_markdown_formatting</code> <code>bool</code> <p>Remove markdown formatting (default: True)</p> <code>True</code> Source code in <code>src/prompt_refiner/tools/schema_compressor.py</code> <pre><code>def __init__(\n    self,\n    drop_examples: bool = True,\n    drop_titles: bool = True,\n    drop_markdown_formatting: bool = True,\n):\n    \"\"\"\n    Initialize schema compressor.\n\n    Args:\n        drop_examples: Remove examples fields (default: True)\n        drop_titles: Remove title fields (default: True)\n        drop_markdown_formatting: Remove markdown formatting (default: True)\n    \"\"\"\n    self.drop_examples = drop_examples\n    self.drop_titles = drop_titles\n    self.drop_markdown_formatting = drop_markdown_formatting\n</code></pre>"},{"location":"api-reference/tools/#prompt_refiner.tools.SchemaCompressor--markdown-removed-tokens-saved","title":"Markdown removed, tokens saved!","text":""},{"location":"api-reference/tools/#prompt_refiner.tools.SchemaCompressor-functions","title":"Functions","text":""},{"location":"api-reference/tools/#prompt_refiner.tools.SchemaCompressor.process","title":"process","text":"<pre><code>process(tool)\n</code></pre> <p>Process a single tool schema and return compressed JSON.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>JSON</code> <p>Tool schema dictionary (e.g., OpenAI function calling schema)</p> required <p>Returns:</p> Type Description <code>JSON</code> <p>Compressed tool schema dictionary</p> Example <p>tool = { ...     \"type\": \"function\", ...     \"function\": { ...         \"name\": \"search\", ...         \"description\": \"Search for items...\", ...         \"parameters\": {...} ...     } ... } compressor = SchemaCompressor() compressed = compressor.process(tool)</p> Source code in <code>src/prompt_refiner/tools/schema_compressor.py</code> <pre><code>def process(self, tool: JSON) -&gt; JSON:\n    \"\"\"\n    Process a single tool schema and return compressed JSON.\n\n    Args:\n        tool: Tool schema dictionary (e.g., OpenAI function calling schema)\n\n    Returns:\n        Compressed tool schema dictionary\n\n    Example:\n        &gt;&gt;&gt; tool = {\n        ...     \"type\": \"function\",\n        ...     \"function\": {\n        ...         \"name\": \"search\",\n        ...         \"description\": \"Search for items...\",\n        ...         \"parameters\": {...}\n        ...     }\n        ... }\n        &gt;&gt;&gt; compressor = SchemaCompressor()\n        &gt;&gt;&gt; compressed = compressor.process(tool)\n    \"\"\"\n    # Compress the tool\n    return self._compress_single_tool(tool)\n</code></pre>"},{"location":"api-reference/tools/#key-features","title":"Key Features","text":"<ul> <li>57% average reduction across 20 real-world API schemas</li> <li>100% lossless - all protocol fields preserved (name, type, required, enum)</li> <li>100% callable (20/20 validated) - all compressed schemas work correctly with OpenAI function calling</li> <li>70%+ reduction on enterprise APIs (HubSpot, Salesforce, OpenAI)</li> <li>Works with OpenAI and Anthropic function calling format</li> </ul>"},{"location":"api-reference/tools/#examples","title":"Examples","text":"<pre><code>from prompt_refiner import SchemaCompressor\n\n# Basic usage\ntool_schema = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search_products\",\n        \"description\": \"Search for products in the e-commerce catalog...\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\"type\": \"string\", \"description\": \"Search query...\"}\n            },\n            \"required\": [\"query\"]\n        }\n    }\n}\n\ncompressor = SchemaCompressor()\ncompressed = compressor.process(tool_schema)\n# Result: 30-70% smaller, functionally identical\n</code></pre> <pre><code># With Pydantic\nfrom pydantic import BaseModel, Field\nfrom openai.pydantic_function_tool import pydantic_function_tool\nfrom prompt_refiner import SchemaCompressor\n\nclass SearchInput(BaseModel):\n    query: str = Field(description=\"The search query...\")\n    category: str | None = Field(default=None, description=\"Filter by category...\")\n\n# Generate and compress schema\ntool_schema = pydantic_function_tool(SearchInput, name=\"search\")\ncompressed = SchemaCompressor().process(tool_schema)\n\n# Use with OpenAI\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=[compressed]  # Compressed but functionally identical\n)\n</code></pre> <pre><code># Batch compression\ntools = [search_schema, create_schema, update_schema, delete_schema]\ncompressor = SchemaCompressor()\ncompressed_tools = [compressor.process(tool) for tool in tools]\n\n# Use all compressed tools\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=compressed_tools\n)\n</code></pre>"},{"location":"api-reference/tools/#what-gets-compressed","title":"What Gets Compressed","text":"<p>\u2705 Optimized (Documentation): - <code>description</code> fields (main source of verbosity) - Redundant explanations and examples - Marketing language and filler words - Overly detailed parameter descriptions</p> <p>\u274c Never Modified (Protocol): - Function <code>name</code> - Parameter <code>names</code> - Parameter <code>type</code> (string, number, boolean, etc.) - <code>required</code> fields list - <code>enum</code> values - <code>default</code> values - JSON structure</p> <p>100% Lossless</p> <p>SchemaCompressor never modifies protocol fields. The compressed schema is functionally identical to the original - LLMs will call the function with the same arguments.</p>"},{"location":"api-reference/tools/#responsecompressor","title":"ResponseCompressor","text":"<p>Compress verbose API/tool responses before sending back to the LLM.</p>"},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor","title":"prompt_refiner.tools.ResponseCompressor","text":"<pre><code>ResponseCompressor(\n    drop_keys=None,\n    drop_null_fields=True,\n    drop_empty_fields=True,\n    max_depth=8,\n    add_truncation_marker=True,\n    truncation_suffix=\"\u2026 (truncated)\",\n)\n</code></pre> <p>               Bases: <code>Refiner</code></p> <p>Compress tool responses to reduce token usage before sending to LLM.</p> <p>This operation compresses JSON-like tool responses by removing verbose content while preserving essential information. Perfect for agent systems that need to fit tool outputs within LLM context windows.</p> <p>What is modified: - Long strings (truncated to 512 chars) - Long lists (truncated to 16 items) - Debug/trace/log fields (removed if in drop_keys) - Null values (removed if drop_null_fields=True) - Empty containers (removed if drop_empty_fields=True) - Deep nesting (truncated beyond max_depth)</p> <p>What is preserved: - Overall structure (dict keys, list order) - Essential data fields - Numbers and booleans (never modified) - Type information</p> <p>IMPORTANT: Use this ONLY for LLM-facing payloads. Do NOT use compressed output for business logic or APIs that expect complete data.</p> <p>Parameters:</p> Name Type Description Default <code>drop_keys</code> <code>Set[str] | None</code> <p>Field names to remove (default: debug, trace, logs, etc.)</p> <code>None</code> <code>drop_null_fields</code> <code>bool</code> <p>Remove fields with None values (default: True)</p> <code>True</code> <code>drop_empty_fields</code> <code>bool</code> <p>Remove empty strings/lists/dicts (default: True)</p> <code>True</code> <code>max_depth</code> <code>int</code> <p>Maximum nesting depth before truncation (default: 8)</p> <code>8</code> <code>add_truncation_marker</code> <code>bool</code> <p>Add markers when truncating (default: True)</p> <code>True</code> <code>truncation_suffix</code> <code>str</code> <p>Suffix for truncated content (default: \"\u2026 (truncated)\")</p> <code>'\u2026 (truncated)'</code> Example <p>from prompt_refiner import ResponseCompressor</p> Use Cases <ul> <li>Agent Systems: Compress verbose tool outputs before sending to LLM</li> <li>API Integration: Reduce token usage from third-party API responses</li> <li>Cost Optimization: Save 30-70% tokens on verbose tool responses</li> <li>Context Management: Fit more tool results within token budget</li> </ul> <p>Initialize ResponseCompressor with compression settings.</p> Source code in <code>src/prompt_refiner/tools/response_compressor.py</code> <pre><code>def __init__(\n    self,\n    drop_keys: Set[str] | None = None,\n    drop_null_fields: bool = True,\n    drop_empty_fields: bool = True,\n    max_depth: int = 8,\n    add_truncation_marker: bool = True,\n    truncation_suffix: str = \"\u2026 (truncated)\",\n):\n    \"\"\"Initialize ResponseCompressor with compression settings.\"\"\"\n    # Hardcoded limits for simplicity\n    self.max_string_chars = 512\n    self.max_list_items = 16\n    self.drop_keys = drop_keys or {\n        \"debug\",\n        \"trace\",\n        \"traces\",\n        \"stack\",\n        \"stacktrace\",\n        \"logs\",\n        \"logging\",\n    }\n    self.drop_null_fields = drop_null_fields\n    self.drop_empty_fields = drop_empty_fields\n    self.max_depth = max_depth\n    self.add_truncation_marker = add_truncation_marker\n    self.truncation_suffix = truncation_suffix\n</code></pre>"},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor--compress-api-response-before-sending-to-llm","title":"Compress API response before sending to LLM","text":"<p>compressor = ResponseCompressor() response = { ...     \"results\": [\"item1\", \"item2\"] * 100,  # 200 items ...     \"debug\": {\"trace\": \"...\"}, ...     \"data\": \"x\" * 1000 ... } compressed = compressor.process(response)</p>"},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor--result-results-limited-to-16-items-debug-removed-data-truncated-to-512-chars","title":"Result: results limited to 16 items, debug removed, data truncated to 512 chars","text":""},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor-functions","title":"Functions","text":""},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor.process","title":"process","text":"<pre><code>process(response)\n</code></pre> <p>Compress tool response data.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>JSON</code> <p>Tool response as dict</p> required <p>Returns:</p> Type Description <code>JSON</code> <p>Compressed response as dict</p> Example <p>response = { ...     \"results\": [{\"data\": \"x\" * 1000}] * 100, ...     \"debug\": {\"trace\": \"...\"} ... } compressor = ResponseCompressor() compressed = compressor.process(response)</p> Source code in <code>src/prompt_refiner/tools/response_compressor.py</code> <pre><code>def process(self, response: JSON) -&gt; JSON:\n    \"\"\"\n    Compress tool response data.\n\n    Args:\n        response: Tool response as dict\n\n    Returns:\n        Compressed response as dict\n\n    Example:\n        &gt;&gt;&gt; response = {\n        ...     \"results\": [{\"data\": \"x\" * 1000}] * 100,\n        ...     \"debug\": {\"trace\": \"...\"}\n        ... }\n        &gt;&gt;&gt; compressor = ResponseCompressor()\n        &gt;&gt;&gt; compressed = compressor.process(response)\n        &gt;&gt;&gt; # Result: debug removed, results truncated, data shortened\n    \"\"\"\n    # Compress the response\n    return self._compress_any(response, depth=0)\n</code></pre>"},{"location":"api-reference/tools/#prompt_refiner.tools.ResponseCompressor.process--result-debug-removed-results-truncated-data-shortened","title":"Result: debug removed, results truncated, data shortened","text":""},{"location":"api-reference/tools/#key-features_1","title":"Key Features","text":"<ul> <li>25.8% average reduction on 20 real-world API responses (range: 14-53%)</li> <li>Removes debug/trace/logs fields automatically</li> <li>Truncates long strings (&gt; 512 chars) and lists (&gt; 16 items)</li> <li>Preserves essential data structure</li> <li>52.7% reduction on verbose responses like Stripe Payment API</li> </ul>"},{"location":"api-reference/tools/#examples_1","title":"Examples","text":"<pre><code>from prompt_refiner import ResponseCompressor\n\n# Basic usage\napi_response = {\n    \"results\": [\n        {\"id\": 1, \"name\": \"Product A\", \"price\": 29.99},\n        {\"id\": 2, \"name\": \"Product B\", \"price\": 39.99},\n        # ... 100 more results\n    ],\n    \"debug_info\": {\n        \"query_time_ms\": 45,\n        \"cache_hit\": True,\n        \"server\": \"api-01\"\n    },\n    \"trace_id\": \"abc123...\",\n    \"logs\": [\"Started query\", \"Fetched from DB\", ...]\n}\n\ncompressor = ResponseCompressor()\ncompact = compressor.process(api_response)\n# Result: Essential data kept, debug/trace/logs removed, long lists truncated\n</code></pre> <pre><code># In agent workflow\nfrom prompt_refiner import SchemaCompressor, ResponseCompressor\nimport openai\nimport json\n\n# 1. Compress tool schema\ntool_schema = {...}\ncompressed_schema = SchemaCompressor().process(tool_schema)\n\n# 2. Call LLM with compressed schema\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for Python books\"}],\n    tools=[compressed_schema]\n)\n\n# 3. Execute tool\ntool_call = response.choices[0].message.tool_calls[0]\nfunction_args = json.loads(tool_call.function.arguments)\ntool_response = search_books(**function_args)  # Verbose response\n\n# 4. Compress response before sending to LLM\ncompact_response = ResponseCompressor().process(tool_response)\n\n# 5. Continue conversation with compressed response\nfinal_response = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Search for Python books\"},\n        response.choices[0].message,\n        {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"content\": json.dumps(compact_response)  # Compressed\n        }\n    ]\n)\n</code></pre>"},{"location":"api-reference/tools/#what-gets-compressed_1","title":"What Gets Compressed","text":"<p>Removed: - Debug fields (<code>debug_info</code>, <code>trace_id</code>, <code>_debug</code>) - Log fields (<code>logs</code>, <code>log</code>, <code>trace</code>, <code>_trace</code>) - Excessive metadata</p> <p>Truncated: - Long strings (&gt; 512 chars) - Long lists (&gt; 16 items) - Deep nesting (&gt; 10 levels)</p> <p>Preserved: - Essential data fields - JSON structure - Data types</p>"},{"location":"api-reference/tools/#configuration","title":"Configuration","text":"<p>ResponseCompressor uses sensible hardcoded limits:</p> <ul> <li>String limit: 512 characters</li> <li>List limit: 16 items</li> <li>Max depth: 10 levels</li> <li>Drop nulls: True (automatic)</li> <li>Drop empty containers: True (automatic)</li> </ul> <p>No Configuration Needed</p> <p>ResponseCompressor uses hardcoded sensible defaults that work well for most API responses. No configuration required.</p>"},{"location":"api-reference/tools/#cost-savings","title":"Cost Savings","text":"<p>Typical savings for different agent sizes using GPT-4 ($0.03/1k input tokens):</p> Agent Size Tools Calls/Day Monthly Savings Annual Savings Small 5 100 $44 $528 Medium 10 500 $541 $6,492 Large 20 1,000 $3,249 $38,988 Enterprise 50 5,000 $40,664 $487,968 <p>Based on 56.9% average schema reduction</p>"},{"location":"api-reference/tools/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/tools/#1-compress-schemas-once-reuse","title":"1. Compress Schemas Once, Reuse","text":"<pre><code># At application startup\ncompressor = SchemaCompressor()\nCOMPRESSED_TOOLS = [\n    compressor.process(search_schema),\n    compressor.process(create_schema),\n    compressor.process(update_schema)\n]\n\n# In agent loop - reuse compressed schemas\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=messages,\n    tools=COMPRESSED_TOOLS  # Reuse\n)\n</code></pre>"},{"location":"api-reference/tools/#2-always-compress-responses","title":"2. Always Compress Responses","text":"<pre><code># Always compress before sending to LLM\ntool_response = api.search(query)\ncompact = ResponseCompressor().process(tool_response)\nmessages.append({\"role\": \"tool\", \"content\": json.dumps(compact)})\n</code></pre>"},{"location":"api-reference/tools/#3-monitor-token-savings","title":"3. Monitor Token Savings","text":"<pre><code>import tiktoken\n\nencoder = tiktoken.encoding_for_model(\"gpt-4\")\n\noriginal_tokens = len(encoder.encode(json.dumps(original_schema)))\ncompressed_tokens = len(encoder.encode(json.dumps(compressed_schema)))\n\nprint(f\"Saved {original_tokens - compressed_tokens} tokens\")\nprint(f\"Reduction: {(1 - compressed_tokens/original_tokens)*100:.1f}%\")\n</code></pre>"},{"location":"api-reference/tools/#benchmark-results","title":"Benchmark Results","text":"<p>See comprehensive benchmark results for detailed performance on 20 real-world API schemas.</p>"},{"location":"api-reference/tools/#learn-more","title":"Learn More","text":"<ul> <li>Tools Module Guide</li> <li>Tools Examples (GitHub)</li> <li>Benchmark Results</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Practical examples for each module in Prompt Refiner.</p>"},{"location":"examples/#by-module","title":"By Module","text":""},{"location":"examples/#cleaner-examples","title":"Cleaner Examples","text":"<ul> <li>HTML Cleaning - Strip HTML tags and convert to Markdown</li> <li>JSON Cleaning - Strip nulls/empties from JSON for RAG APIs</li> <li>See more in: Cleaner Module</li> </ul>"},{"location":"examples/#compressor-examples","title":"Compressor Examples","text":"<ul> <li>Deduplication - Remove duplicate content from RAG results</li> <li>See more in: Compressor Module</li> </ul>"},{"location":"examples/#scrubber-examples","title":"Scrubber Examples","text":"<ul> <li>PII Redaction - Redact sensitive information</li> <li>See more in: Scrubber Module</li> </ul>"},{"location":"examples/#analyzer-examples","title":"Analyzer Examples","text":"<ul> <li>Token Analysis - Calculate token savings and ROI</li> <li>See more in: Analyzer Module</li> </ul>"},{"location":"examples/#packer-examples-advanced","title":"Packer Examples (Advanced)","text":"<ul> <li>Context Budget Management - RAG applications, chatbots, and conversation history</li> <li>See more in: Packer Module</li> </ul>"},{"location":"examples/#complete-examples","title":"Complete Examples","text":"<ul> <li>Complete Pipeline - Full optimization with all modules</li> </ul>"},{"location":"examples/#running-examples-locally","title":"Running Examples Locally","text":"<p>All examples are available in the <code>examples/</code> directory:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobHuang91/prompt-refiner.git\ncd prompt-refiner\n\n# Install dependencies\nmake install\n\n# Run an example\npython examples/packer/messages.py\n</code></pre>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Getting Started Guide</li> <li>API Reference</li> <li>Report Issues</li> </ul>"},{"location":"examples/complete-pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive example using all 5 modules together.</p>"},{"location":"examples/complete-pipeline/#full-optimization-pipeline","title":"Full Optimization Pipeline","text":"<pre><code>from prompt_refiner import (\n    # Cleaner\n    StripHTML, NormalizeWhitespace, FixUnicode,\n    # Compressor\n    Deduplicate, TruncateTokens,\n    # Scrubber\n    RedactPII,\n    # Analyzer\n    CountTokens\n)\n\n# Messy input with HTML, PII, duplicates\nmessy_input = \"\"\"\n&lt;div&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;Contact us at support@company.com or call 555-123-4567.&lt;/p&gt;\n    &lt;p&gt;We provide excellent service   with   lots   of   spaces.&lt;/p&gt;\n    &lt;p&gt;Our IP address is 192.168.1.1 for reference.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Initialize counter\ncounter = CountTokens(original_text=messy_input)\n\n# Build complete pipeline using pipe operator (recommended)\npipeline = (\n    # Clean dirty data\n    StripHTML()\n    | FixUnicode()\n    | NormalizeWhitespace()\n    # Compress\n    | Deduplicate(similarity_threshold=0.85)\n    | TruncateTokens(max_tokens=50, strategy=\"head\")\n    # Secure\n    | RedactPII(redact_types={\"email\", \"phone\", \"ip\"})\n    # Analyze\n    | counter\n)\n\n# Run pipeline\nresult = pipeline.run(messy_input)\n\nprint(\"Optimized result:\")\nprint(result)\nprint(\"\\nStatistics:\")\nprint(counter.format_stats())\n</code></pre> <p>Alternative: Fluent API</p> <p>You can also use <code>.pipe()</code> method chaining: <pre><code>from prompt_refiner import Refiner\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(FixUnicode())\n    .pipe(NormalizeWhitespace())\n    # ... continue with other operations\n)\n</code></pre></p>"},{"location":"examples/complete-pipeline/#related","title":"Related","text":"<ul> <li>Pipeline Basics</li> <li>All Modules Overview</li> </ul>"},{"location":"examples/deduplication/","title":"Deduplication Example","text":"<p>Remove duplicate content from RAG retrieval results.</p>"},{"location":"examples/deduplication/#scenario","title":"Scenario","text":"<p>Your RAG system retrieved multiple similar chunks that contain overlapping information.</p>"},{"location":"examples/deduplication/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import Deduplicate\n\n# RAG results with duplicates\nrag_results = \"\"\"\nPython is a high-level programming language.\n\nPython is a high level programming language.\n\nPython supports multiple programming paradigms.\n\"\"\"\n\npipeline = Deduplicate(similarity_threshold=0.85)\ndeduplicated = pipeline.run(rag_results)\n\nprint(deduplicated)\n# Output: Only unique paragraphs remain\n</code></pre>"},{"location":"examples/deduplication/#adjusting-sensitivity","title":"Adjusting Sensitivity","text":"<pre><code># More aggressive (70% similarity)\npipeline = Deduplicate(similarity_threshold=0.70)\n\n# Sentence-level deduplication\npipeline = Deduplicate(granularity=\"sentence\")\n</code></pre>"},{"location":"examples/deduplication/#performance-considerations","title":"Performance Considerations","text":"<p>When working with large RAG contexts, keep these performance tips in mind:</p>"},{"location":"examples/deduplication/#choosing-a-similarity-method","title":"Choosing a Similarity Method","text":"<pre><code># Fast: Jaccard (word-based) - recommended for most use cases\npipeline = Deduplicate(method=\"jaccard\")\n\n# Precise but slower: Levenshtein (character-based)\n# Only use when you need character-level accuracy\npipeline = Deduplicate(method=\"levenshtein\")\n</code></pre>"},{"location":"examples/deduplication/#scaling-with-input-size","title":"Scaling with Input Size","text":"<p>The deduplication algorithm compares each chunk against all previous chunks (O(n\u00b2)):</p> <ul> <li>10-50 chunks: Fast with either method (typical RAG use case)</li> <li>50-200 chunks: Use Jaccard for better performance</li> <li>200+ chunks: Use <code>granularity=\"paragraph\"</code> to reduce chunk count</li> </ul> <pre><code># For large documents: use paragraph granularity\npipeline = Deduplicate(\n    similarity_threshold=0.85,\n    method=\"jaccard\",\n    granularity=\"paragraph\"  # Fewer chunks = faster\n)\n</code></pre>"},{"location":"examples/deduplication/#full-example","title":"Full Example","text":"<p>See: <code>examples/compressor/deduplication.py</code></p>"},{"location":"examples/deduplication/#related","title":"Related","text":"<ul> <li>Deduplicate API Reference</li> <li>Compressor Module Guide</li> </ul>"},{"location":"examples/html-cleaning/","title":"HTML Cleaning Example","text":"<p>Clean HTML content from web scraping or user input.</p>"},{"location":"examples/html-cleaning/#scenario","title":"Scenario","text":"<p>You've scraped content from a website and need to clean it before sending to an LLM API.</p>"},{"location":"examples/html-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\nhtml_content = \"\"\"\n&lt;div class=\"article\"&gt;\n    &lt;h1&gt;Understanding &lt;strong&gt;LLMs&lt;/strong&gt;&lt;/h1&gt;\n    &lt;p&gt;Large Language Models are powerful &lt;em&gt;AI systems&lt;/em&gt;.&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n# Remove all HTML and normalize whitespace\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\ncleaned = pipeline.run(html_content)\nprint(cleaned)\n# Output: \"Understanding LLMs Large Language Models are powerful AI systems.\"\n</code></pre>"},{"location":"examples/html-cleaning/#converting-to-markdown","title":"Converting to Markdown","text":"<pre><code># Convert HTML to Markdown instead of removing\npipeline = (\n    StripHTML(to_markdown=True)\n    | NormalizeWhitespace()\n)\n\nmarkdown = pipeline.run(html_content)\nprint(markdown)\n# Output:\n# # Understanding **LLMs**\n#\n# Large Language Models are powerful *AI systems*.\n</code></pre>"},{"location":"examples/html-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/html_cleaning.py</code></p> <pre><code>python examples/cleaner/html_cleaning.py\n</code></pre>"},{"location":"examples/html-cleaning/#related","title":"Related","text":"<ul> <li>StripHTML API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/json-cleaning/","title":"JSON Cleaning Example","text":"<p>Clean and compress JSON from API responses before sending to LLM.</p>"},{"location":"examples/json-cleaning/#scenario","title":"Scenario","text":"<p>You're building a RAG application that fetches documents from an API. The API responses contain many null values and empty fields that waste tokens. You need to compress the JSON before including it in your LLM prompt.</p>"},{"location":"examples/json-cleaning/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import JsonCleaner\n\napi_response = \"\"\"\n{\n    \"documents\": [\n        {\n            \"id\": 1,\n            \"title\": \"Introduction to LLMs\",\n            \"content\": \"Large Language Models are powerful AI systems...\",\n            \"metadata\": {\n                \"author\": \"Alice\",\n                \"deprecated\": null,\n                \"tags\": []\n            }\n        }\n    ],\n    \"next_page\": null,\n    \"filters\": {}\n}\n\"\"\"\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ncompressed = cleaner.run(api_response)\nprint(compressed)\n# Output: {\"documents\":[{\"id\":1,\"title\":\"Introduction to LLMs\",\"content\":\"Large Language Models are powerful AI systems...\",\"metadata\":{\"author\":\"Alice\"}}]}\n</code></pre> <p>Token savings: 61% reduction (791 \u2192 316 characters)</p>"},{"location":"examples/json-cleaning/#only-minify-keep-all-data","title":"Only Minify (Keep All Data)","text":"<pre><code># Just remove whitespace, keep all data\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nminified = cleaner.run(api_response)\n# Output: {\"documents\":[...],\"next_page\":null,\"filters\":{}}\n</code></pre>"},{"location":"examples/json-cleaning/#rag-pipeline","title":"RAG Pipeline","text":"<pre><code>from prompt_refiner import JsonCleaner, TruncateTokens\n\n# Compress JSON and truncate if still too long\nrag_pipeline = (\n    JsonCleaner(strip_nulls=True, strip_empty=True)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n)\n\ncompressed = rag_pipeline.run(large_api_response)\n</code></pre>"},{"location":"examples/json-cleaning/#full-example","title":"Full Example","text":"<p>See the complete example: <code>examples/cleaner/json_cleaning.py</code></p> <pre><code>python examples/cleaner/json_cleaning.py\n</code></pre>"},{"location":"examples/json-cleaning/#related","title":"Related","text":"<ul> <li>JsonCleaner API Reference</li> <li>Cleaner Module Guide</li> </ul>"},{"location":"examples/packer/","title":"Packer Examples","text":"<p>Advanced examples for managing context budgets with MessagesPacker and TextPacker.</p> <p>Documentation Update in Progress</p> <p>These examples are being updated for v0.2.2 which removed the <code>model</code> and <code>max_tokens</code> parameters. Packers now include all items without token budget constraints - let LLM APIs handle final token limits. See API Reference for current v0.2.2 syntax.</p> <p>When to Use Packer</p> <p>Packer is ideal for:</p> <ul> <li>RAG Applications: Pack multiple retrieved documents within token budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"examples/packer/#example-1-basic-rag-with-messagespacker","title":"Example 1: Basic RAG with MessagesPacker","text":"<p>Pack RAG documents for chat APIs with priority-based selection:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_MEDIUM,\n)\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (must include)\npacker.add(\n    \"Answer based on the provided context only.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents with different priorities\npacker.add(\n    \"Product X costs $99 and includes 1-year warranty.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH  # Most relevant document\n)\n\npacker.add(\n    \"We offer free shipping on orders over $50.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM  # Less relevant document\n)\n\npacker.add(\n    \"Customer reviews rate Product X 4.5/5 stars.\",\n    role=\"system\",\n    priority=PRIORITY_MEDIUM\n)\n\n# User query (must include)\npacker.add(\n    \"What is the price of Product X?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into messages format\nmessages = packer.pack()\n\n# Use directly with OpenAI\n# response = client.chat.completions.create(\n#     model=\"gpt-4\",\n#     messages=messages\n# )\n\nprint(f\"Packed {len(messages)} messages\")\nfor msg in messages:\n    print(f\"{msg['role']}: {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-2-rag-with-dirty-html-jit-refinement","title":"Example 2: RAG with Dirty HTML (JIT Refinement)","text":"<p>Clean web-scraped RAG documents on-the-fly:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    StripHTML,\n    NormalizeWhitespace\n)\n\npacker = MessagesPacker(max_tokens=800)\n\n# System prompt\npacker.add(\n    \"You are a helpful product assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents from web scraping (with HTML)\ndocs = [\n    \"&lt;div class='product'&gt;&lt;h2&gt;Product Features&lt;/h2&gt;&lt;p&gt;  Waterproof   design  &lt;/p&gt;&lt;/div&gt;\",\n    \"&lt;html&gt;&lt;body&gt;   &lt;p&gt;Available in  &lt;b&gt;5 colors&lt;/b&gt;:  red, blue...&lt;/p&gt;  &lt;/body&gt;&lt;/html&gt;\",\n    \"&lt;article&gt;   Battery life:  &lt;strong&gt;  48 hours  &lt;/strong&gt;  continuous use  &lt;/article&gt;\"\n]\n\n# Clean each document before adding (JIT refinement)\nfor doc in docs:\n    packer.add(\n        doc,\n        role=\"system\",\n        priority=PRIORITY_HIGH,\n        refine_with=[StripHTML(), NormalizeWhitespace()]  # Clean on-the-fly!\n    )\n\n# User query\npacker.add(\n    \"What are the key features?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# All HTML is automatically cleaned before packing\nprint(\"Cleaned messages:\")\nfor msg in messages:\n    if msg['role'] == 'system' and 'Waterproof' in msg['content']:\n        print(f\"Before: {docs[0][:50]}...\")\n        print(f\"After:  {msg['content'][:50]}...\")\n</code></pre>"},{"location":"examples/packer/#example-3-chatbot-with-conversation-history","title":"Example 3: Chatbot with Conversation History","text":"<p>Manage conversation history with priorities - old messages can be dropped:</p> <pre><code>from prompt_refiner import (\n    MessagesPacker,\n    PRIORITY_SYSTEM,\n    PRIORITY_USER,\n    PRIORITY_HIGH,\n    PRIORITY_LOW,\n)\n\npacker = MessagesPacker(max_tokens=1000)\n\n# System prompt (highest priority)\npacker.add(\n    \"You are a helpful customer support agent.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG: Current relevant documentation\npacker.add(\n    \"Return policy: 30-day money-back guarantee for all products.\",\n    role=\"system\",\n    priority=PRIORITY_HIGH\n)\n\n# Old conversation history (can be dropped if budget is tight)\nold_conversation = [\n    {\"role\": \"user\", \"content\": \"What are your business hours?\"},\n    {\"role\": \"assistant\", \"content\": \"We're open 9 AM - 5 PM EST, Monday-Friday.\"},\n    {\"role\": \"user\", \"content\": \"Do you ship internationally?\"},\n    {\"role\": \"assistant\", \"content\": \"Yes, we ship to over 50 countries worldwide.\"}\n]\n\npacker.add_messages(old_conversation, priority=PRIORITY_LOW)\n\n# Current user query (highest priority)\npacker.add(\n    \"What is your return policy?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nmessages = packer.pack()\n\n# If budget is tight, old history is dropped, but system prompt + current query are kept\nprint(f\"Packed {len(messages)} messages (old history may be dropped)\")\n</code></pre>"},{"location":"examples/packer/#example-4-textpacker-for-base-models","title":"Example 4: TextPacker for Base Models","text":"<p>Use TextPacker with Llama or GPT-3 base models:</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\n# Use MARKDOWN format for better structure\npacker = TextPacker(\n    max_tokens=600,\n    text_format=TextFormat.MARKDOWN\n)\n\n# System instructions\npacker.add(\n    \"You are a QA assistant. Answer based on the context provided.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\n# RAG documents (no role = treated as context)\npacker.add(\n    \"Prompt-refiner is a Python library for optimizing LLM inputs.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"It reduces token usage by 4-15% through cleaning and compression.\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"The library has zero dependencies by default.\",\n    priority=PRIORITY_HIGH\n)\n\n# User query\npacker.add(\n    \"What is prompt-refiner?\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\n# Pack into formatted text\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# ### INSTRUCTIONS:\n# You are a QA assistant. Answer based on the context provided.\n#\n# ### CONTEXT:\n# - Prompt-refiner is a Python library for optimizing LLM inputs.\n# - It reduces token usage by 4-15% through cleaning and compression.\n# - The library has zero dependencies by default.\n#\n# ### INPUT:\n# What is prompt-refiner?\n\n# Use with completion API\n# response = client.completions.create(\n#     model=\"llama-2-70b\",\n#     prompt=prompt\n# )\n</code></pre>"},{"location":"examples/packer/#example-5-textpacker-with-xml-format","title":"Example 5: TextPacker with XML Format","text":"<p>Use XML format (Anthropic best practice for Claude base models):</p> <pre><code>from prompt_refiner import (\n    TextPacker,\n    TextFormat,\n    PRIORITY_SYSTEM,\n    PRIORITY_HIGH,\n    PRIORITY_USER,\n)\n\npacker = TextPacker(\n    max_tokens=500,\n    text_format=TextFormat.XML\n)\n\npacker.add(\n    \"You are a code review assistant.\",\n    role=\"system\",\n    priority=PRIORITY_SYSTEM\n)\n\npacker.add(\n    \"Code snippet: def hello(): return 'world'\",\n    priority=PRIORITY_HIGH\n)\n\npacker.add(\n    \"Please review this code for best practices.\",\n    role=\"user\",\n    priority=PRIORITY_USER\n)\n\nprompt = packer.pack()\n\nprint(prompt)\n# Output:\n# &lt;system&gt;\n# You are a code review assistant.\n# &lt;/system&gt;\n#\n# &lt;context&gt;\n# Code snippet: def hello(): return 'world'\n# &lt;/context&gt;\n#\n# &lt;user&gt;\n# Please review this code for best practices.\n# &lt;/user&gt;\n</code></pre>"},{"location":"examples/packer/#example-6-precise-mode-for-maximum-token-utilization","title":"Example 6: Precise Mode for Maximum Token Utilization","text":"<p>Use precise mode with tiktoken for 100% token budget utilization:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_USER\n\n# Install tiktoken: pip install llm-prompt-refiner[token]\n\n# Estimation mode (default): 10% safety buffer\npacker_estimate = MessagesPacker(max_tokens=1000)\nprint(f\"Estimation mode: {packer_estimate.effective_max_tokens} effective tokens\")\n# Output: Estimation mode: 900 effective tokens\n\n# Precise mode: 100% budget utilization (no safety buffer)\npacker_precise = MessagesPacker(max_tokens=1000, model=\"gpt-4\")\nprint(f\"Precise mode: {packer_precise.effective_max_tokens} effective tokens\")\n# Output: Precise mode: 997 effective tokens (1000 - 3 request overhead)\n\n# Use precise mode for production to maximize token capacity\npacker_precise.add(\"System prompt\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker_precise.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\nmessages = packer_precise.pack()\n</code></pre>"},{"location":"examples/packer/#example-7-inspection-and-debugging","title":"Example 7: Inspection and Debugging","text":"<p>Inspect items before packing to understand token distribution:</p> <pre><code>from prompt_refiner import MessagesPacker, PRIORITY_SYSTEM, PRIORITY_HIGH, PRIORITY_USER\n\npacker = MessagesPacker(max_tokens=500)\n\npacker.add(\"System prompt here\", role=\"system\", priority=PRIORITY_SYSTEM)\npacker.add(\"Document 1\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"Document 2\" * 50, role=\"system\", priority=PRIORITY_HIGH)\npacker.add(\"User query\", role=\"user\", priority=PRIORITY_USER)\n\n# Inspect items before packing\nitems = packer.get_items()\n\nprint(\"Items before packing:\")\nfor i, item in enumerate(items):\n    print(f\"{i+1}. Priority: {item['priority']}, Tokens: {item['tokens']}, Role: {item['role']}\")\n\n# Pack and see which items fit\nmessages = packer.pack()\nprint(f\"\\nPacked {len(messages)}/{len(items)} items\")\n</code></pre>"},{"location":"examples/packer/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Choose the Right Packer:</li> <li><code>MessagesPacker</code> for chat APIs (OpenAI, Anthropic)</li> <li> <p><code>TextPacker</code> for completion APIs (Llama Base, GPT-3)</p> </li> <li> <p>Set Priorities Correctly:</p> </li> <li><code>PRIORITY_SYSTEM</code> (0): System prompts, absolute must-have</li> <li><code>PRIORITY_USER</code> (10): User queries, critical</li> <li><code>PRIORITY_HIGH</code> (20): Core RAG documents</li> <li><code>PRIORITY_MEDIUM</code> (30): Supporting context</li> <li> <p><code>PRIORITY_LOW</code> (40): Old conversation history</p> </li> <li> <p>Use JIT Refinement:</p> </li> <li>Clean dirty documents with <code>refine_with</code> parameter</li> <li> <p>Chain multiple operations: <code>refine_with=[StripHTML(), NormalizeWhitespace()]</code></p> </li> <li> <p>Optimize for Production:</p> </li> <li>Use precise mode with <code>model</code> parameter for 100% token utilization</li> <li>Choose appropriate text format for base models (MARKDOWN recommended)</li> </ol>"},{"location":"examples/packer/#related-documentation","title":"Related Documentation","text":"<ul> <li>Packer Module Guide</li> <li>Packer API Reference</li> <li>Getting Started</li> </ul>"},{"location":"examples/pii-redaction/","title":"PII Redaction Example","text":"<p>Automatically redact sensitive information before sending to APIs.</p>"},{"location":"examples/pii-redaction/#scenario","title":"Scenario","text":"<p>User input contains personal information that should not be sent to external LLM APIs.</p>"},{"location":"examples/pii-redaction/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import RedactPII\n\nuser_input = \"\"\"\nPlease contact me at john.doe@example.com or call 555-123-4567.\nMy account number is EMP-12345.\n\"\"\"\n\npipeline = RedactPII()\nsecure = pipeline.run(user_input)\n\nprint(secure)\n# Output:\n# Please contact me at [EMAIL] or call [PHONE].\n# My account number is EMP-12345.\n</code></pre>"},{"location":"examples/pii-redaction/#custom-patterns","title":"Custom Patterns","text":"<pre><code>pipeline = RedactPII(\n    redact_types={\"email\", \"phone\"},\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n\nsecure = pipeline.run(user_input)\n# Now EMP-12345 is also redacted as [EMPLOYEE_ID]\n</code></pre>"},{"location":"examples/pii-redaction/#full-example","title":"Full Example","text":"<p>See: <code>examples/scrubber/pii_redaction.py</code></p>"},{"location":"examples/pii-redaction/#related","title":"Related","text":"<ul> <li>RedactPII API Reference</li> <li>Scrubber Module Guide</li> </ul>"},{"location":"examples/token-analysis/","title":"Token Analysis Example","text":"<p>Measure optimization impact and calculate cost savings.</p>"},{"location":"examples/token-analysis/#scenario","title":"Scenario","text":"<p>You want to demonstrate the value of prompt optimization.</p>"},{"location":"examples/token-analysis/#example-code","title":"Example Code","text":"<pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World   from   HTML  &lt;/p&gt;\"\n\n# Initialize counter with original text\ncounter = CountTokens(original_text=original_text)\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | counter\n)\n\nresult = pipeline.run(original_text)\n\n# Show statistics\nprint(counter.format_stats())\n# Output:\n# Original: 10 tokens\n# Cleaned: 4 tokens\n# Saved: 6 tokens (60.0%)\n</code></pre>"},{"location":"examples/token-analysis/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\n\n# GPT-4 pricing: $0.03 per 1K tokens\ncost_per_token = 0.03 / 1000\n\noriginal_cost = stats['original'] * cost_per_token\ncleaned_cost = stats['cleaned'] * cost_per_token\nsavings_per_request = original_cost - cleaned_cost\n\nprint(f\"Savings: ${savings_per_request:.4f} per request\")\n\n# Project annual savings\nrequests_per_day = 10000\nannual_savings = savings_per_request * requests_per_day * 365\nprint(f\"Annual savings: ${annual_savings:.2f}\")\n</code></pre>"},{"location":"examples/token-analysis/#full-example","title":"Full Example","text":"<p>See: <code>examples/analyzer/token_counting.py</code></p>"},{"location":"examples/token-analysis/#related","title":"Related","text":"<ul> <li>CountTokens API Reference</li> <li>Analyzer Module Guide</li> </ul>"},{"location":"modules/analyzer/","title":"Analyzer Module","text":"<p>Track optimization impact and demonstrate value with token counting and statistics.</p>"},{"location":"modules/analyzer/#counttokens-operation","title":"CountTokens Operation","text":"<p>Measure token usage before and after optimization.</p>"},{"location":"modules/analyzer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, CountTokens\n\noriginal_text = \"&lt;p&gt;Hello    World&lt;/p&gt;\"\ncounter = CountTokens(original_text=original_text)\n\nrefiner = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(counter)\n)\n\nresult = refiner.run(original_text)\nprint(counter.format_stats())\n# Original: 6 tokens\n# Cleaned: 2 tokens\n# Saved: 4 tokens (66.7%)\n</code></pre>"},{"location":"modules/analyzer/#calculate-cost-savings","title":"Calculate Cost Savings","text":"<pre><code>stats = counter.get_stats()\ncost_per_token = 0.03 / 1000  # GPT-4 pricing\nsavings = stats['saved'] * cost_per_token\nprint(f\"Savings: ${savings:.4f} per request\")\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/cleaner/","title":"Cleaner Module","text":"<p>The Cleaner module provides operations for cleaning dirty data from various sources.</p>"},{"location":"modules/cleaner/#overview","title":"Overview","text":"<p>When working with real-world text data, you often encounter:</p> <ul> <li>HTML tags from web scraping</li> <li>Excessive whitespace and formatting issues</li> <li>Problematic Unicode characters</li> <li>JSON with null values and empty containers</li> </ul> <p>The Cleaner module addresses these issues efficiently.</p>"},{"location":"modules/cleaner/#operations","title":"Operations","text":""},{"location":"modules/cleaner/#striphtml","title":"StripHTML","text":"<p>Remove HTML tags or convert them to Markdown.</p> <p>Use cases:</p> <ul> <li>Web scraping</li> <li>Email content processing</li> <li>User-generated HTML content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import StripHTML\n\n# Remove all HTML\ncleaner = StripHTML()\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello World!\"\n\n# Convert to Markdown\ncleaner = StripHTML(to_markdown=True)\nresult = cleaner.run(\"&lt;p&gt;Hello &lt;b&gt;World&lt;/b&gt;!&lt;/p&gt;\")\n# Output: \"Hello **World**!\\n\\n\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#normalizewhitespace","title":"NormalizeWhitespace","text":"<p>Collapse excessive whitespace, tabs, and newlines.</p> <p>Use cases:</p> <ul> <li>Text from PDFs</li> <li>User input normalization</li> <li>Copy-pasted content</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import NormalizeWhitespace\n\ncleaner = NormalizeWhitespace()\nresult = cleaner.run(\"Hello    World  \\t\\n  Foo\")\n# Output: \"Hello World Foo\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#fixunicode","title":"FixUnicode","text":"<p>Remove problematic Unicode characters.</p> <p>Use cases:</p> <ul> <li>Zero-width spaces from copy-paste</li> <li>Control characters</li> <li>Invisible characters causing issues</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import FixUnicode\n\ncleaner = FixUnicode()\nresult = cleaner.run(\"Hello\\u200bWorld\")\n# Output: \"HelloWorld\"\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#jsoncleaner","title":"JsonCleaner","text":"<p>Clean and minify JSON by removing null values and empty containers.</p> <p>Use cases:</p> <ul> <li>RAG API responses with null/empty fields</li> <li>Compressing JSON context before LLM input</li> <li>Normalizing inconsistent API data</li> <li>Token optimization for JSON-heavy prompts</li> </ul> <p>Example:</p> <pre><code>from prompt_refiner import JsonCleaner\n\n# Strip nulls and empty containers\ncleaner = JsonCleaner(strip_nulls=True, strip_empty=True)\ndirty_json = \"\"\"\n{\n    \"name\": \"Alice\",\n    \"age\": null,\n    \"address\": {},\n    \"tags\": []\n}\n\"\"\"\nresult = cleaner.run(dirty_json)\n# Output: {\"name\":\"Alice\"}\n\n# Only minify (keep all data)\ncleaner = JsonCleaner(strip_nulls=False, strip_empty=False)\nresult = cleaner.run(dirty_json)\n# Output: {\"name\":\"Alice\",\"age\":null,\"address\":{},\"tags\":[]}\n</code></pre> <p>Token savings: 50-60% reduction in typical RAG API responses!</p> <p>Full API Reference \u2192</p>"},{"location":"modules/cleaner/#common-patterns","title":"Common Patterns","text":""},{"location":"modules/cleaner/#web-content-pipeline","title":"Web Content Pipeline","text":"<pre><code>from prompt_refiner import StripHTML, FixUnicode, NormalizeWhitespace\n\nweb_cleaner = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#text-normalization","title":"Text Normalization","text":"<pre><code>from prompt_refiner import FixUnicode, NormalizeWhitespace\n\nnormalizer = (\n    FixUnicode()\n    | NormalizeWhitespace()\n)\n</code></pre>"},{"location":"modules/cleaner/#rag-context-compression","title":"RAG Context Compression","text":"<pre><code>from prompt_refiner import JsonCleaner, TruncateTokens\n\nrag_compressor = (\n    JsonCleaner(strip_nulls=True, strip_empty=True)\n    | TruncateTokens(max_tokens=500, strategy=\"head\")\n)\n</code></pre>"},{"location":"modules/cleaner/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples</li> <li>Full API Reference</li> <li>Explore Other Modules</li> </ul>"},{"location":"modules/compressor/","title":"Compressor Module","text":"<p>Reduce text size while preserving meaning through smart truncation and deduplication.</p>"},{"location":"modules/compressor/#operations","title":"Operations","text":""},{"location":"modules/compressor/#truncatetokens","title":"TruncateTokens","text":"<p>Smart text truncation respecting sentence boundaries.</p> <pre><code>from prompt_refiner import TruncateTokens\n\n# Keep first 100 tokens\ntruncator = TruncateTokens(max_tokens=100, strategy=\"head\")\n\n# Keep last 100 tokens (for conversation history)\ntruncator = TruncateTokens(max_tokens=100, strategy=\"tail\")\n\n# Keep beginning and end, remove middle\ntruncator = TruncateTokens(max_tokens=100, strategy=\"middle_out\")\n</code></pre> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#deduplicate","title":"Deduplicate","text":"<p>Remove duplicate or similar content chunks.</p> <pre><code>from prompt_refiner import Deduplicate\n\n# Remove paragraphs with 85% similarity\ndeduper = Deduplicate(similarity_threshold=0.85)\n\n# Sentence-level deduplication\ndeduper = Deduplicate(granularity=\"sentence\")\n</code></pre> <p>Performance Considerations:</p> <ul> <li>Method Choice: Use <code>jaccard</code> (default) for most cases - it's fast and works well with typical prompts. Only use <code>levenshtein</code> when you need character-level precision.</li> <li>Complexity: Deduplication uses O(n\u00b2) comparisons where n is the number of chunks. For 50 chunks, this is ~1,225 comparisons.</li> <li>Large Inputs: For 200+ chunks, use <code>granularity=\"paragraph\"</code> to reduce chunk count and speed up processing.</li> <li>Jaccard: O(m) per comparison - fast even with long chunks</li> <li>Levenshtein: O(m\u2081 \u00d7 m\u2082) per comparison - can be slow with chunks over 1000 characters</li> </ul> <p>Full API Reference \u2192</p>"},{"location":"modules/compressor/#common-use-cases","title":"Common Use Cases","text":""},{"location":"modules/compressor/#rag-context-optimization","title":"RAG Context Optimization","text":"<pre><code>from prompt_refiner import Deduplicate, TruncateTokens\n\nrag_optimizer = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n</code></pre> <p>View Examples</p>"},{"location":"modules/overview/","title":"Modules Overview","text":"<p>Prompt Refiner is organized into 5 core modules plus measurement utilities.</p>"},{"location":"modules/overview/#the-5-core-modules","title":"The 5 Core Modules","text":""},{"location":"modules/overview/#1-cleaner-clean-dirty-data","title":"1. Cleaner - Clean Dirty Data","text":"<p>The Cleaner module removes unwanted artifacts from your text.</p> <p>Operations:</p> <ul> <li>StripHTML - Remove or convert HTML tags</li> <li>NormalizeWhitespace - Collapse excessive whitespace</li> <li>FixUnicode - Remove problematic Unicode characters</li> <li>JsonCleaner - Strip nulls/empties from JSON, minify</li> </ul> <p>When to use:</p> <ul> <li>Processing web-scraped content</li> <li>Cleaning user-generated text</li> <li>Compressing JSON from RAG APIs</li> <li>Normalizing text from various sources</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#2-compressor-reduce-size","title":"2. Compressor - Reduce Size","text":"<p>The Compressor module reduces token count while preserving meaning.</p> <p>Operations:</p> <ul> <li>TruncateTokens - Smart text truncation with sentence boundaries</li> <li>Deduplicate - Remove similar or duplicate content</li> </ul> <p>When to use:</p> <ul> <li>Fitting content within context windows</li> <li>Optimizing RAG retrieval results</li> <li>Reducing API costs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#3-scrubber-security-privacy","title":"3. Scrubber - Security &amp; Privacy","text":"<p>The Scrubber module protects sensitive information.</p> <p>Operations:</p> <ul> <li>RedactPII - Automatically redact personally identifiable information</li> </ul> <p>When to use:</p> <ul> <li>Before sending data to external APIs</li> <li>Compliance with privacy regulations</li> <li>Protecting user data in logs</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#4-packer-context-budget-management","title":"4. Packer - Context Budget Management","text":"<p>The Packer module manages context budgets with intelligent priority-based item selection.</p> <p>Operations:</p> <ul> <li>MessagesPacker - Pack items for chat completion APIs</li> <li>TextPacker - Pack items for text completion APIs</li> </ul> <p>When to use:</p> <ul> <li>RAG applications with multiple documents</li> <li>Chatbots with conversation history</li> <li>Managing context windows with size limits</li> <li>Combining system prompts, user input, and documents</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#5-strategy-preset-strategies","title":"5. Strategy - Preset Strategies","text":"<p>The Strategy module provides benchmark-tested preset strategies for quick setup.</p> <p>Strategies:</p> <ul> <li>MinimalStrategy - 4.3% reduction, 98.7% quality</li> <li>StandardStrategy - 4.8% reduction, 98.4% quality</li> <li>AggressiveStrategy - 15% reduction, 96.4% quality</li> </ul> <p>When to use:</p> <ul> <li>Quick setup without manual configuration</li> <li>Benchmark-tested optimization presets</li> <li>Extending with additional custom operations</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#measurement-utilities","title":"Measurement Utilities","text":""},{"location":"modules/overview/#analyzer-measure-impact","title":"Analyzer - Measure Impact","text":"<p>The Analyzer module measures optimization impact but does not transform prompts. Use it to track token savings and demonstrate ROI.</p> <p>Operations:</p> <ul> <li>TokenTracker - Measure token savings and calculate ROI</li> <li>Token Counters - Built-in functions for counting tokens</li> </ul> <p>When to use:</p> <ul> <li>Demonstrating cost savings to stakeholders</li> <li>A/B testing optimization strategies</li> <li>Monitoring optimization impact over time</li> <li>Calculating ROI for prompt optimization</li> </ul> <p>Learn more \u2192</p>"},{"location":"modules/overview/#combining-modules","title":"Combining Modules","text":"<p>The real power comes from combining modules:</p>"},{"location":"modules/overview/#pipeline-example","title":"Pipeline Example","text":"<pre><code>from prompt_refiner import (\n    TokenTracker,              # Analyzer\n    StripHTML,                 # Cleaner\n    NormalizeWhitespace,       # Cleaner\n    TruncateTokens,            # Compressor\n    RedactPII,                 # Scrubber\n    character_based_counter,   # Token counter\n)\n\noriginal_text = \"Your text here...\"\n\n# Build pipeline\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n    | RedactPII()\n)\n\n# Track optimization with TokenTracker\nwith TokenTracker(pipeline, character_based_counter) as tracker:\n    result = tracker.process(original_text)\n\n# Show token savings\nstats = tracker.stats\nprint(f\"Saved {stats['saved_tokens']} tokens ({stats['saving_percent']})\")\n</code></pre>"},{"location":"modules/overview/#packer-example","title":"Packer Example","text":"<pre><code>from prompt_refiner import (\n    MessagesPacker,\n    ROLE_SYSTEM,\n    ROLE_QUERY,\n    ROLE_CONTEXT,\n    StripHTML,\n)\n\n# Manage RAG context for chat APIs with automatic priorities\npacker = MessagesPacker(\n    system=\"You are a helpful assistant.\",\n    query=\"What is prompt-refiner?\",\n)\n\n# Add retrieved documents with automatic cleaning\nfor doc in retrieved_docs:\n    packer.add(\n        doc.content,\n        role=ROLE_CONTEXT,  # Auto-assigned PRIORITY_HIGH\n        refine_with=StripHTML(),\n    )\n\nmessages = packer.pack()  # Returns List[Dict] directly\n</code></pre>"},{"location":"modules/overview/#module-relationships","title":"Module Relationships","text":"<pre><code>graph LR\n    A[Raw Input] --&gt; B[Cleaner]\n    B --&gt; C[Compressor]\n    C --&gt; D[Scrubber]\n    D --&gt; E[Optimized Output]\n    E -.-&gt; F[Analyzer&lt;br/&gt;Measurement Only]\n\n    G[Multiple Items] --&gt; H[Packer]\n    H --&gt; I[Packed Context]\n</code></pre> <p>Note: Analyzer (dotted line) measures but doesn't transform the output.</p>"},{"location":"modules/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Clean before compressing, compress before redacting</li> <li>Use Packer for RAG: When managing multiple documents with priorities</li> <li>Test your pipeline: Different inputs may need different operations</li> <li>Measure impact: Use TokenTracker to track token savings and demonstrate ROI</li> <li>Start simple: Begin with one module and add more as needed</li> </ol>"},{"location":"modules/packer/","title":"Packer Module","text":"<p>Intelligently manage context budgets with smart priority-based packing for RAG applications and chatbots.</p>"},{"location":"modules/packer/#overview-v013","title":"Overview (v0.1.3+)","text":"<p>The Packer module provides two specialized packers following the Single Responsibility Principle:</p> <ul> <li><code>MessagesPacker</code>: For chat completion APIs (OpenAI, Anthropic). Returns <code>List[Dict]</code></li> <li><code>TextPacker</code>: For text completion APIs (Llama Base, GPT-3). Returns <code>str</code></li> </ul> <p>Key Features: - Smart priority-based selection (auto-prioritizes: system &gt; query &gt; context &gt; history) - Semantic roles for clear intent (ROLE_SYSTEM, ROLE_QUERY, ROLE_CONTEXT, ROLE_USER, ROLE_ASSISTANT) - JIT refinement with <code>refine_with</code> parameter - Automatic format overhead calculation</p>"},{"location":"modules/packer/#messagespacker","title":"MessagesPacker","text":"<p>Pack items into chat message format for chat completion APIs.</p>"},{"location":"modules/packer/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import MessagesPacker\n\n# Create packer with token budget\npacker = MessagesPacker(max_tokens=1000)\n\n# Add items with semantic roles (auto-prioritized)\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\"  # Auto: highest priority\n)\n\npacker.add(\n    \"Product documentation: Feature A, B, C...\",\n    role=\"context\"  # Auto: high priority\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=\"query\"  # Auto: critical priority\n)\n\n# Pack into messages format\nmessages = packer.pack()  # Returns List[Dict[str, str]]\n\n# Use directly with chat APIs\n# response = client.chat.completions.create(messages=messages)\n</code></pre>"},{"location":"modules/packer/#rag-conversation-history-example","title":"RAG + Conversation History Example","text":"<pre><code>from prompt_refiner import MessagesPacker, StripHTML\n\npacker = MessagesPacker(max_tokens=500)\n\n# System prompt (auto: highest priority)\npacker.add(\n    \"Answer based on the provided context.\",\n    role=\"system\"\n)\n\n# RAG documents with JIT cleaning (auto: high priority)\npacker.add(\n    \"&lt;p&gt;Prompt-refiner is a library...&lt;/p&gt;\",\n    role=\"context\",\n    refine_with=StripHTML()\n)\n\n# Old conversation history (auto: low priority, can be dropped)\nold_messages = [\n    {\"role\": \"user\", \"content\": \"What is this library?\"},\n    {\"role\": \"assistant\", \"content\": \"It's a tool for optimizing prompts.\"}\n]\npacker.add_messages(old_messages)\n\n# Current query (auto: critical priority)\npacker.add(\n    \"How does it reduce costs?\",\n    role=\"query\"\n)\n\n# Pack into messages\nmessages = packer.pack()  # List[Dict[str, str]]\n</code></pre>"},{"location":"modules/packer/#textpacker","title":"TextPacker","text":"<p>Pack items into formatted text for text completion APIs (base models).</p>"},{"location":"modules/packer/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat\n\n# Create packer with MARKDOWN format\npacker = TextPacker(\n    max_tokens=1000,\n    text_format=TextFormat.MARKDOWN\n)\n\n# Add items with semantic roles (auto-prioritized)\npacker.add(\n    \"You are a helpful assistant.\",\n    role=\"system\"  # Auto: highest priority\n)\n\npacker.add(\n    \"Product documentation...\",\n    role=\"context\"  # Auto: high priority\n)\n\npacker.add(\n    \"What are the key features?\",\n    role=\"query\"  # Auto: critical priority\n)\n\n# Pack into formatted text\nprompt = packer.pack()  # Returns str\n\n# Use with completion APIs\n# response = client.completions.create(prompt=prompt)\n</code></pre>"},{"location":"modules/packer/#text-formats","title":"Text Formats","text":"<p>RAW Format (default): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.RAW)\n# Output: Simple concatenation with separators\n</code></pre></p> <p>MARKDOWN Format (recommended for base models): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\n# Output:\n# ### INSTRUCTIONS:\n# System prompt\n#\n# ### CONTEXT:\n# - Document 1\n# - Document 2\n#\n# ### CONVERSATION:\n# User: Hello\n# Assistant: Hi\n#\n# ### INPUT:\n# Final query\n</code></pre></p> <p>XML Format (Anthropic best practice): <pre><code>packer = TextPacker(max_tokens=1000, text_format=TextFormat.XML)\n# Output: &lt;role&gt;content&lt;/role&gt; tags\n</code></pre></p>"},{"location":"modules/packer/#rag-example-with-grouped-sections","title":"RAG Example with Grouped Sections","text":"<pre><code>from prompt_refiner import TextPacker, TextFormat, StripHTML\n\npacker = TextPacker(max_tokens=500, text_format=TextFormat.MARKDOWN)\n\n# System prompt (auto: highest priority)\npacker.add(\n    \"Answer based on context.\",\n    role=\"system\"\n)\n\n# RAG documents (auto: high priority)\npacker.add(\n    \"&lt;p&gt;Document 1...&lt;/p&gt;\",\n    role=\"context\",\n    refine_with=StripHTML()\n)\n\npacker.add(\n    \"Document 2...\",\n    role=\"context\"\n)\n\n# User query (auto: critical priority)\npacker.add(\n    \"What is the answer?\",\n    role=\"query\"\n)\n\nprompt = packer.pack()  # str\n</code></pre>"},{"location":"modules/packer/#semantic-roles-priorities","title":"Semantic Roles &amp; Priorities","text":"<p>Semantic Roles (Recommended): <pre><code>from prompt_refiner import (\n    ROLE_SYSTEM,      # \"system\" - System instructions (auto: PRIORITY_SYSTEM = 0)\n    ROLE_QUERY,       # \"query\" - Current user question (auto: PRIORITY_QUERY = 10)\n    ROLE_CONTEXT,     # \"context\" - RAG documents (auto: PRIORITY_HIGH = 20)\n    ROLE_USER,        # \"user\" - User messages in history (auto: PRIORITY_LOW = 40)\n    ROLE_ASSISTANT,   # \"assistant\" - Assistant messages in history (auto: PRIORITY_LOW = 40)\n)\n</code></pre></p> <p>Priority Constants (Optional): <pre><code>from prompt_refiner import (\n    PRIORITY_SYSTEM,   # 0 - Absolute must-have (system prompts)\n    PRIORITY_QUERY,    # 10 - Current user query (critical for response)\n    PRIORITY_HIGH,     # 20 - Important context (core RAG docs)\n    PRIORITY_MEDIUM,   # 30 - Normal priority (general RAG docs)\n    PRIORITY_LOW,      # 40 - Optional content (old history)\n)\n</code></pre></p> <p>Use Semantic Roles</p> <p>Semantic roles auto-infer priorities, making code clearer. You usually don't need to specify priority manually!</p>"},{"location":"modules/packer/#common-features","title":"Common Features","text":""},{"location":"modules/packer/#jit-refinement","title":"JIT Refinement","text":"<p>Apply operations before adding items:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\npacker.add(\n    \"&lt;div&gt;  Messy   HTML  &lt;/div&gt;\",\n    role=\"context\",\n    refine_with=[StripHTML(), NormalizeWhitespace()]\n)\n</code></pre>"},{"location":"modules/packer/#method-chaining","title":"Method Chaining","text":"<pre><code>from prompt_refiner import MessagesPacker\n\nmessages = (\n    MessagesPacker(max_tokens=500)\n    .add(\"System prompt\", role=\"system\")\n    .add(\"User query\", role=\"query\")\n    .pack()\n)\n</code></pre>"},{"location":"modules/packer/#inspection","title":"Inspection","text":"<pre><code>from prompt_refiner import MessagesPacker\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"Item 1\", role=\"system\")\npacker.add(\"Item 2\", role=\"query\")\n\nitems = packer.get_items()\nfor item in items:\n    print(f\"Priority: {item['priority']}, Tokens: {item['tokens']}\")\n</code></pre>"},{"location":"modules/packer/#reset","title":"Reset","text":"<pre><code>from prompt_refiner import MessagesPacker\n\npacker = MessagesPacker(max_tokens=1000)\npacker.add(\"First batch\", role=\"context\")\nmessages1 = packer.pack()\n\n# Clear and reuse\npacker.reset()\npacker.add(\"Second batch\", role=\"context\")\nmessages2 = packer.pack()\n</code></pre>"},{"location":"modules/packer/#how-it-works","title":"How It Works","text":"<ol> <li>Add items with priorities, roles, and optional JIT refinement</li> <li>Sort by priority (lower number = higher priority)</li> <li>Greedy packing - select items that fit within budget</li> <li>Restore insertion order for natural reading flow</li> <li>Format output:</li> <li>MessagesPacker: Returns <code>List[Dict[str, str]]</code></li> <li>TextPacker: Returns <code>str</code> (formatted based on text_format)</li> </ol>"},{"location":"modules/packer/#token-overhead-optimization","title":"Token Overhead Optimization","text":""},{"location":"modules/packer/#messagespacker_1","title":"MessagesPacker","text":"<ul> <li>Pre-calculates ChatML format overhead (~4 tokens per message)</li> <li>100% token budget utilization in precise mode</li> </ul>"},{"location":"modules/packer/#textpacker-markdown","title":"TextPacker (MARKDOWN)","text":"<ul> <li>\"Entrance fee\" strategy: Pre-reserves 30 tokens for section headers</li> <li>Marginal costs: Only counts bullet points and newlines per item</li> <li>Result: Fits more documents compared to per-item header calculation</li> </ul>"},{"location":"modules/packer/#use-cases","title":"Use Cases","text":"<ul> <li>RAG Applications: Pack retrieved documents into context budget</li> <li>Chatbots: Manage conversation history with priorities</li> <li>Context Window Management: Fit critical information within model limits</li> <li>Multi-source Data: Combine system prompts, user input, and documents</li> </ul>"},{"location":"modules/packer/#new-in-v013","title":"New in v0.1.3","text":"<p>The Packer module now provides two specialized packers:</p> <pre><code>from prompt_refiner import MessagesPacker, TextPacker\n\n# For chat APIs (OpenAI, Anthropic)\nmessages_packer = MessagesPacker(max_tokens=1000)\nmessages = messages_packer.pack()  # List[Dict[str, str]]\n\n# For completion APIs (Llama Base, GPT-3)\ntext_packer = TextPacker(max_tokens=1000, text_format=TextFormat.MARKDOWN)\ntext = text_packer.pack()  # str\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/scrubber/","title":"Scrubber Module","text":"<p>Protect sensitive information with automatic PII redaction.</p>"},{"location":"modules/scrubber/#redactpii-operation","title":"RedactPII Operation","text":"<p>Automatically redact personally identifiable information using regex patterns.</p>"},{"location":"modules/scrubber/#supported-pii-types","title":"Supported PII Types","text":"<ul> <li><code>email</code> - Email addresses</li> <li><code>phone</code> - Phone numbers</li> <li><code>ip</code> - IP addresses</li> <li><code>credit_card</code> - Credit card numbers</li> <li><code>ssn</code> - Social Security Numbers</li> <li><code>url</code> - URLs</li> </ul>"},{"location":"modules/scrubber/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import RedactPII\n\n# Redact all PII types\nredactor = RedactPII()\nresult = redactor.process(\"Contact john@example.com or 555-123-4567\")\n# Output: \"Contact [EMAIL] or [PHONE]\"\n\n# Redact specific types\nredactor = RedactPII(redact_types={\"email\", \"phone\"})\n</code></pre>"},{"location":"modules/scrubber/#custom-patterns","title":"Custom Patterns","text":"<pre><code>redactor = RedactPII(\n    custom_patterns={\"employee_id\": r\"EMP-\\d{5}\"}\n)\n</code></pre> <p>Full API Reference \u2192 View Examples</p>"},{"location":"modules/tools/","title":"Tools Module","text":"<p>The Tools module optimizes AI agent function calling by compressing tool schemas and responses. This is one of the most impactful optimizations in Prompt Refiner, achieving 57% average token reduction with 100% lossless compression.</p> <p>Benchmark Results</p> <p>Tested on 20 real-world API schemas (Stripe, Salesforce, HubSpot, Slack, OpenAI, Anthropic), SchemaCompressor achieves:</p> <ul> <li>56.9% average reduction across all schemas</li> <li>70%+ reduction on enterprise APIs</li> <li>100% lossless - all protocol fields preserved</li> <li>100% callable (20/20 validated) - all compressed schemas work correctly with OpenAI function calling</li> <li>A medium agent (10 tools, 500 calls/day) saves $541/month on GPT-4</li> </ul> <p>View benchmark results \u2192</p>"},{"location":"modules/tools/#overview","title":"Overview","text":"<p>Function calling is a major source of token consumption in AI agent systems:</p> <ul> <li>Tool schemas with verbose descriptions consume thousands of tokens</li> <li>API responses often include debug info, traces, and excessive data</li> <li>Multiple tools multiply the cost (10 tools = 10x the schema tokens)</li> </ul> <p>The Tools module solves this with two components:</p> <ol> <li>SchemaCompressor - Compress function/tool schemas (OpenAI, Anthropic format)</li> <li>ResponseCompressor - Compress verbose API/tool responses</li> </ol>"},{"location":"modules/tools/#schemacompressor","title":"SchemaCompressor","text":"<p>Compresses tool schemas while preserving 100% of the protocol specification.</p>"},{"location":"modules/tools/#basic-usage","title":"Basic Usage","text":"<pre><code>from prompt_refiner import SchemaCompressor\n\n# Your tool schema (OpenAI or Anthropic format)\ntool_schema = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search_products\",\n        \"description\": \"Search for products in the e-commerce catalog...\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search query with keywords...\"\n                },\n                # ... more parameters\n            },\n            \"required\": [\"query\"]\n        }\n    }\n}\n\n# Compress the schema\ncompressor = SchemaCompressor()\ncompressed_schema = compressor.process(tool_schema)\n\n# Use compressed schema with OpenAI/Anthropic\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=[compressed_schema]  # Compressed but functionally identical\n)\n</code></pre>"},{"location":"modules/tools/#what-gets-compressed","title":"What Gets Compressed","text":"<p>SchemaCompressor optimizes documentation fields while preserving all protocol fields:</p> <p>\u2705 Optimized (Documentation): - <code>description</code> fields (main source of verbosity) - Redundant explanations and examples - Marketing language and filler words - Overly detailed parameter descriptions</p> <p>\u274c Never Modified (Protocol): - Function <code>name</code> - Parameter <code>names</code> - Parameter <code>type</code> (string, number, boolean, etc.) - <code>required</code> fields list - <code>enum</code> values - <code>default</code> values - JSON structure</p>"},{"location":"modules/tools/#integration-with-pydantic","title":"Integration with Pydantic","text":"<p>Works seamlessly with Pydantic function tools:</p> <pre><code>from pydantic import BaseModel, Field\nfrom openai.pydantic_function_tool import pydantic_function_tool\nfrom prompt_refiner import SchemaCompressor\n\nclass SearchBooksInput(BaseModel):\n    \"\"\"Search for books in the library database.\"\"\"\n\n    query: str = Field(\n        description=\"The search query string containing keywords...\"\n    )\n    category: str | None = Field(\n        default=None,\n        description=\"Filter by book category like Fiction, Science...\"\n    )\n    max_results: int = Field(\n        default=10,\n        description=\"Maximum number of results to return...\"\n    )\n\n# Generate and compress schema\ntool_schema = pydantic_function_tool(SearchBooksInput, name=\"search_books\")\ncompressed = SchemaCompressor().process(tool_schema)\n\n# 30-60% token reduction typical for Pydantic schemas\n</code></pre>"},{"location":"modules/tools/#performance-by-schema-type","title":"Performance by Schema Type","text":"<p>Token reduction varies by schema verbosity:</p> Schema Type Avg Reduction Example Very Verbose (Enterprise APIs) 67.4% HubSpot Contact (73.2%), Salesforce Account (72.1%) Complex (Rich APIs) 61.7% Slack (70.8%), Stripe (66.7%), E-commerce (46.0%) Medium (Standard APIs) 13.1% Weather API (20.1%), GitHub (6.1%) Simple (Minimal APIs) 0.0% Calculator (already minimal) <p>Best Candidates for Compression</p> <p>Enterprise and complex APIs with extensive documentation see 60-70%+ reduction. Simple APIs with minimal docs see little benefit (already optimized).</p>"},{"location":"modules/tools/#batch-compression","title":"Batch Compression","text":"<p>Compress multiple tool schemas at once:</p> <pre><code>from prompt_refiner import SchemaCompressor\n\ntools = [\n    search_tool_schema,\n    create_tool_schema,\n    update_tool_schema,\n    delete_tool_schema\n]\n\ncompressor = SchemaCompressor()\ncompressed_tools = [compressor.process(tool) for tool in tools]\n\n# Use all compressed tools\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=compressed_tools  # All compressed\n)\n</code></pre>"},{"location":"modules/tools/#responsecompressor","title":"ResponseCompressor","text":"<p>Compresses verbose API/tool responses before sending back to the LLM.</p>"},{"location":"modules/tools/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prompt_refiner import ResponseCompressor\n\n# Verbose API response\napi_response = {\n    \"results\": [\n        {\"id\": 1, \"name\": \"Product A\", \"price\": 29.99},\n        {\"id\": 2, \"name\": \"Product B\", \"price\": 39.99},\n        # ... 100 more results\n    ],\n    \"debug_info\": {\n        \"query_time_ms\": 45,\n        \"cache_hit\": True,\n        \"server\": \"api-01\"\n    },\n    \"trace_id\": \"abc123...\",\n    \"logs\": [\"Started query\", \"Fetched from DB\", ...],\n    \"metadata\": {...}\n}\n\n# Compress the response\ncompressor = ResponseCompressor()\ncompact_response = compressor.process(api_response)\n\n# Compact response sent back to LLM\n# - Keeps essential data (results, relevant fields)\n# - Removes debug/trace/logs\n# - Truncates long lists and strings\n# - 30-70% token reduction typical\n</code></pre>"},{"location":"modules/tools/#what-gets-compressed_1","title":"What Gets Compressed","text":"<p>Removed: - Debug fields (<code>debug_info</code>, <code>trace_id</code>, <code>_debug</code>) - Log fields (<code>logs</code>, <code>log</code>, <code>trace</code>, <code>_trace</code>) - Excessive metadata</p> <p>Truncated: - Long strings (&gt; 512 chars) - Long lists (&gt; 16 items) - Deep nesting (&gt; 10 levels)</p> <p>Preserved: - Essential data fields - JSON structure - Data types</p>"},{"location":"modules/tools/#configuration","title":"Configuration","text":"<p>ResponseCompressor uses sensible hardcoded limits (no configuration needed):</p> <ul> <li>String limit: 512 characters</li> <li>List limit: 16 items</li> <li>Max depth: 10 levels</li> <li>Drop nulls: Optional (default: True)</li> <li>Drop empty containers: Optional (default: True)</li> </ul> <pre><code># Default behavior (recommended)\ncompressor = ResponseCompressor()\n\n# Keep nulls and empty containers if needed\ncompressor = ResponseCompressor()\n# Currently no customization - uses hardcoded sensible defaults\n</code></pre>"},{"location":"modules/tools/#integration-in-agent-workflow","title":"Integration in Agent Workflow","text":"<p>Typical AI agent flow with compression:</p> <pre><code>from prompt_refiner import SchemaCompressor, ResponseCompressor\nimport openai\n\n# 1. Compress tool schemas (one-time, reuse compressed schemas)\ntool_schema = {...}\ncompressed_schema = SchemaCompressor().process(tool_schema)\n\n# 2. Call LLM with compressed schema\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for Python books\"}],\n    tools=[compressed_schema]\n)\n\n# 3. Execute tool\ntool_call = response.choices[0].message.tool_calls[0]\nfunction_args = json.loads(tool_call.function.arguments)\ntool_response = search_books(**function_args)  # Verbose response\n\n# 4. Compress tool response before sending back to LLM\ncompact_response = ResponseCompressor().process(tool_response)\n\n# 5. Continue conversation with compressed response\nfinal_response = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Search for Python books\"},\n        response.choices[0].message,\n        {\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"content\": json.dumps(compact_response)  # Compressed\n        }\n    ]\n)\n</code></pre>"},{"location":"modules/tools/#cost-savings","title":"Cost Savings","text":"<p>Real-world cost savings for different agent sizes:</p> Agent Size Tools Calls/Day Monthly Savings (GPT-4) Annual Savings Small 5 100 $44 $528 Medium 10 500 $541 $6,492 Large 20 1,000 $3,249 $38,988 Enterprise 50 5,000 $40,664 $487,968 <p>Assumes 56.9% average schema reduction, GPT-4 pricing ($0.03/1k input tokens)</p> <p>Medium Agent Breakdown</p> <p>Setup: 10 tools, 500 calls/day, GPT-4</p> <p>Before compression: - 10 tools \u00d7 800 tokens/tool = 8,000 tokens per call - 500 calls/day \u00d7 30 days = 15,000 calls/month - 15,000 \u00d7 8,000 = 120M tokens/month - Cost: 120M / 1000 \u00d7 $0.03 = $3,600/month</p> <p>After compression (56.9% reduction): - 10 tools \u00d7 345 tokens/tool = 3,450 tokens per call - 15,000 \u00d7 3,450 = 51.75M tokens/month - Cost: 51.75M / 1000 \u00d7 $0.03 = $1,553/month</p> <p>Monthly savings: $2,047 \ud83c\udf89</p>"},{"location":"modules/tools/#best-practices","title":"Best Practices","text":""},{"location":"modules/tools/#1-compress-schemas-once-reuse","title":"1. Compress Schemas Once, Reuse","text":"<p>Tool schemas don't change often - compress once and reuse:</p> <pre><code># At application startup\ncompressor = SchemaCompressor()\nCOMPRESSED_TOOLS = [\n    compressor.process(search_schema),\n    compressor.process(create_schema),\n    compressor.process(update_schema)\n]\n\n# In your agent loop - use pre-compressed schemas\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=messages,\n    tools=COMPRESSED_TOOLS  # Reuse compressed schemas\n)\n</code></pre>"},{"location":"modules/tools/#2-always-compress-responses","title":"2. Always Compress Responses","text":"<p>API responses are often verbose - always compress before sending to LLM:</p> <pre><code># BAD: Send raw verbose response\ntool_response = api.search(query)\nmessages.append({\"role\": \"tool\", \"content\": json.dumps(tool_response)})\n\n# GOOD: Compress before sending\ntool_response = api.search(query)\ncompact = ResponseCompressor().process(tool_response)\nmessages.append({\"role\": \"tool\", \"content\": json.dumps(compact)})\n</code></pre>"},{"location":"modules/tools/#3-monitor-token-savings","title":"3. Monitor Token Savings","text":"<p>Track actual savings to validate optimization:</p> <pre><code>import tiktoken\n\nencoder = tiktoken.encoding_for_model(\"gpt-4\")\n\n# Before\noriginal_tokens = len(encoder.encode(json.dumps(original_schema)))\n\n# After\ncompressed_tokens = len(encoder.encode(json.dumps(compressed_schema)))\n\nprint(f\"Saved {original_tokens - compressed_tokens} tokens ({(1 - compressed_tokens/original_tokens)*100:.1f}% reduction)\")\n</code></pre>"},{"location":"modules/tools/#4-test-with-real-schemas","title":"4. Test with Real Schemas","text":"<p>Test compression on your actual tool schemas to measure impact:</p> <pre><code>from prompt_refiner import SchemaCompressor\nimport json\n\n# Load your schema\nwith open(\"my_tool_schema.json\") as f:\n    schema = json.load(f)\n\n# Compress and compare\ncompressed = SchemaCompressor().process(schema)\n\nprint(\"Original:\", json.dumps(schema, indent=2))\nprint(\"\\nCompressed:\", json.dumps(compressed, indent=2))\nprint(f\"\\nSize reduction: {len(json.dumps(schema))} \u2192 {len(json.dumps(compressed))} chars\")\n</code></pre>"},{"location":"modules/tools/#limitations","title":"Limitations","text":""},{"location":"modules/tools/#schemacompressor_1","title":"SchemaCompressor","text":"<ul> <li>Only works with OpenAI/Anthropic function calling format</li> <li>Minimal benefit on already-concise schemas (&lt; 200 tokens)</li> <li>English-language descriptions assumed (may not optimize other languages well)</li> </ul>"},{"location":"modules/tools/#responsecompressor_1","title":"ResponseCompressor","text":"<ul> <li>Hardcoded limits (512 char strings, 16 item lists) - cannot customize</li> <li>May truncate important data if not configured for your use case</li> <li>Binary data (images, files) not supported</li> </ul>"},{"location":"modules/tools/#examples","title":"Examples","text":""},{"location":"modules/tools/#complete-agent-example","title":"Complete Agent Example","text":"<p>See <code>examples/tools/</code> for complete working examples with OpenAI function calling.</p>"},{"location":"modules/tools/#benchmark","title":"Benchmark","text":"<p>See <code>benchmark/function_calling/</code> for comprehensive benchmark on 20 real-world API schemas.</p>"},{"location":"modules/tools/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, see:</p> <ul> <li>SchemaCompressor API Reference</li> <li>ResponseCompressor API Reference</li> </ul>"},{"location":"modules/tools/#learn-more","title":"Learn More","text":"<ul> <li>Function Calling Benchmark Results</li> <li>Tools Module Examples (GitHub)</li> <li>SchemaCompressor Implementation</li> <li>ResponseCompressor Implementation</li> </ul>"},{"location":"user-guide/custom-operations/","title":"Custom Operations","text":"<p>Create your own operations to extend Prompt Refiner.</p>"},{"location":"user-guide/custom-operations/#creating-a-custom-operation","title":"Creating a Custom Operation","text":"<p>All operations inherit from the <code>Operation</code> base class and implement the <code>process</code> method:</p> <pre><code>from prompt_refiner import Operation\n\nclass RemoveEmojis(Operation):\n    \"\"\"Remove emoji characters from text.\"\"\"\n\n    def process(self, text: str) -&gt; str:\n        import re\n        # Simple emoji removal pattern\n        emoji_pattern = re.compile(\n            \"[\"\n            \"\\U0001F600-\\U0001F64F\"  # emoticons\n            \"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n            \"]+\", flags=re.UNICODE\n        )\n        return emoji_pattern.sub(\"\", text)\n</code></pre>"},{"location":"user-guide/custom-operations/#using-your-custom-operation","title":"Using Your Custom Operation","text":"<p>Use it like any built-in operation:</p> <pre><code>from prompt_refiner import Refiner, NormalizeWhitespace\n\npipeline = (\n    Refiner()\n    .pipe(RemoveEmojis())\n    .pipe(NormalizeWhitespace())\n)\n\nresult = pipeline.run(\"Hello \ud83d\ude00 World \ud83c\udf0d!\")\n# Output: \"Hello World !\"\n</code></pre>"},{"location":"user-guide/custom-operations/#more-examples","title":"More Examples","text":""},{"location":"user-guide/custom-operations/#remove-urls","title":"Remove URLs","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveURLs(Operation):\n    def process(self, text: str) -&gt; str:\n        url_pattern = r'https?://\\S+|www\\.\\S+'\n        return re.sub(url_pattern, '[URL]', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#lowercase-text","title":"Lowercase Text","text":"<pre><code>from prompt_refiner import Operation\n\nclass Lowercase(Operation):\n    def process(self, text: str) -&gt; str:\n        return text.lower()\n</code></pre>"},{"location":"user-guide/custom-operations/#remove-numbers","title":"Remove Numbers","text":"<pre><code>import re\nfrom prompt_refiner import Operation\n\nclass RemoveNumbers(Operation):\n    def process(self, text: str) -&gt; str:\n        return re.sub(r'\\d+', '', text)\n</code></pre>"},{"location":"user-guide/custom-operations/#guidelines","title":"Guidelines","text":"<ol> <li>Single responsibility - Each operation should do one thing well</li> <li>Immutable - Don't modify the input, return a new string</li> <li>Deterministic - Same input should always produce same output</li> <li>Document - Add docstrings explaining what it does</li> </ol>"},{"location":"user-guide/custom-operations/#contributing","title":"Contributing","text":"<p>Have a useful operation? Consider contributing it to Prompt Refiner!</p> <p>See contributing guide \u2192</p>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Learn how to use Prompt Refiner effectively to optimize your LLM inputs.</p>"},{"location":"user-guide/overview/#what-is-prompt-refiner","title":"What is Prompt Refiner?","text":"<p>Prompt Refiner is a library for cleaning and optimizing text before sending it to LLM APIs. It helps you:</p> <ul> <li>Save money by reducing token usage</li> <li>Improve quality by cleaning and normalizing text</li> <li>Enhance security by redacting PII</li> <li>Track value by measuring optimization impact</li> </ul>"},{"location":"user-guide/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/overview/#operations","title":"Operations","text":"<p>An Operation is a single transformation that processes text:</p> <pre><code>from prompt_refiner import StripHTML\n\noperation = StripHTML()\nresult = operation.process(\"&lt;p&gt;Hello&lt;/p&gt;\")\n# Output: \"Hello\"\n</code></pre> <p>All operations implement the same interface: <code>process(text: str) -&gt; str</code></p>"},{"location":"user-guide/overview/#pipelines","title":"Pipelines","text":"<p>A Pipeline chains multiple operations together:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace\n\n# Using the pipe operator (recommended)\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n)\n\nresult = pipeline.run(\"&lt;p&gt;Hello    World&lt;/p&gt;\")\n# Output: \"Hello World\"\n</code></pre> <p>Alternatively, use the fluent API: <pre><code>from prompt_refiner import Refiner\n\npipeline = Refiner().pipe(StripHTML()).pipe(NormalizeWhitespace())\n</code></pre></p>"},{"location":"user-guide/overview/#the-4-modules","title":"The 4 Modules","text":"<ul> <li>Cleaner - Clean dirty data</li> <li>Compressor - Reduce size</li> <li>Scrubber - Security &amp; privacy</li> <li>Analyzer - Track metrics</li> </ul>"},{"location":"user-guide/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about pipelines</li> <li>Create custom operations</li> <li>Browse examples</li> </ul>"},{"location":"user-guide/pipelines/","title":"Pipeline Basics","text":"<p>Learn how to build effective pipelines with Prompt Refiner.</p>"},{"location":"user-guide/pipelines/#two-ways-to-build-pipelines","title":"Two Ways to Build Pipelines","text":"<p>Prompt Refiner supports two syntax options for building pipelines:</p>"},{"location":"user-guide/pipelines/#pipe-operator-recommended","title":"Pipe Operator (Recommended)","text":"<p>The pipe operator (<code>|</code>) provides a clean, Pythonic syntax similar to LangChain:</p> <pre><code>from prompt_refiner import StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    StripHTML()\n    | NormalizeWhitespace()\n    | TruncateTokens(max_tokens=1000)\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More concise - no need to import or instantiate <code>Refiner()</code> - Familiar to LangChain, LangGraph, and modern Python framework users - Cleaner visual appearance</p>"},{"location":"user-guide/pipelines/#fluent-api","title":"Fluent API","text":"<p>The fluent API uses method chaining with <code>.pipe()</code>:</p> <pre><code>from prompt_refiner import Refiner, StripHTML, NormalizeWhitespace, TruncateTokens\n\npipeline = (\n    Refiner()\n    .pipe(StripHTML())\n    .pipe(NormalizeWhitespace())\n    .pipe(TruncateTokens(max_tokens=1000))\n)\n\nresult = pipeline.run(input_text)\n</code></pre> <p>Why use this: - More explicit - clear that you're creating a Refiner pipeline - Traditional method chaining pattern</p> <p>Choose One Style</p> <p>Pick one syntax style per project and use it consistently. Both work identically under the hood. Don't mix styles in the same pipeline.</p>"},{"location":"user-guide/pipelines/#the-pipeline-pattern","title":"The Pipeline Pattern","text":"<p>A pipeline chains operations in sequence:</p> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre> <p>All operations process the text in order, with each operation's output becoming the next operation's input.</p>"},{"location":"user-guide/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<ol> <li>Text enters the pipeline</li> <li>Each operation processes it in order</li> <li>Output of one operation becomes input of the next</li> <li>Final result is returned</li> </ol> <pre><code>input \u2192 Operation1 \u2192 Operation2 \u2192 Operation3 \u2192 output\n</code></pre>"},{"location":"user-guide/pipelines/#order-matters","title":"Order Matters","text":"<p>Operations run in the order you add them:</p> <pre><code># \u2705 Correct: Clean HTML first, then normalize\npipeline = StripHTML() | NormalizeWhitespace()\n\n# \u274c Wrong order - normalizes first, HTML remains\npipeline = NormalizeWhitespace() | StripHTML()\n</code></pre>"},{"location":"user-guide/pipelines/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/pipelines/#1-clean-before-compressing","title":"1. Clean Before Compressing","text":"<pre><code>pipeline = (\n    StripHTML()                  # Clean first\n    | NormalizeWhitespace()\n    | TruncateTokens()           # Then compress\n)\n</code></pre>"},{"location":"user-guide/pipelines/#2-compress-before-redacting","title":"2. Compress Before Redacting","text":"<pre><code>pipeline = (\n    TruncateTokens()  # Compress first\n    | RedactPII()     # Then redact\n)\n</code></pre>"},{"location":"user-guide/pipelines/#3-analyze-last","title":"3. Analyze Last","text":"<pre><code>counter = CountTokens(original_text=text)\npipeline = (\n    StripHTML()\n    | TruncateTokens()\n    | counter  # Analyze at the end\n)\n</code></pre>"},{"location":"user-guide/pipelines/#multiple-pipelines","title":"Multiple Pipelines","text":"<p>Create different pipelines for different use cases:</p> <pre><code># Pipeline for web content\nweb_pipeline = (\n    StripHTML(to_markdown=True)\n    | FixUnicode()\n    | NormalizeWhitespace()\n)\n\n# Pipeline for RAG\nrag_pipeline = (\n    Deduplicate()\n    | TruncateTokens(max_tokens=2000)\n)\n\n# Pipeline for secure processing\nsecure_pipeline = RedactPII()\n\n# Use them\ncleaned_web = web_pipeline.run(html_content)\noptimized_rag = rag_pipeline.run(rag_context)\nsafe_text = secure_pipeline.run(user_input)\n</code></pre> <p>Learn about custom operations \u2192</p>"}]}